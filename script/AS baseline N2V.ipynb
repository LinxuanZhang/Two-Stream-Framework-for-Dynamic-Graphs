{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16929,"status":"ok","timestamp":1692200660896,"user":{"displayName":"Quandary zhang","userId":"14333747180371872055"},"user_tz":-60},"id":"I7IVQVEDWEWB","outputId":"59274c47-cd83-496b-c2e8-4f8ff45d736f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/mlds\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/mlds"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ElDsah6AWVwk","executionInfo":{"status":"ok","timestamp":1692200670065,"user_tz":-60,"elapsed":9171,"user":{"displayName":"Quandary zhang","userId":"14333747180371872055"}}},"outputs":[],"source":["import data_auto_sys as aus\n","import tasker_link_prediction as t_lp\n","from splitter import splitter\n","from models import Baseline_node2vec\n","from trainer import Trainer\n","import yaml\n","import os"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1298,"status":"ok","timestamp":1692200671361,"user":{"displayName":"Quandary zhang","userId":"14333747180371872055"},"user_tz":-60},"id":"doo1WcuyWZAC","outputId":"819c3c48-d268-41e3-86a2-2711d14bece6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'adam_config': {'adam_beta_1': 0.9,\n","  'adam_beta_2': 0.999,\n","  'adam_epsilon': 1e-07,\n","  'adam_learning_rate': 0.01},\n"," 'model_path': 'models/AS_N2V_baseline/',\n"," 'classifier_hidden_size': 20,\n"," 'ffn_fusion_size': 10,\n"," 'ffn_hidden_size': 10,\n"," 'gcn_fusion_size': 20,\n"," 'spatial_hidden_size': 20,\n"," 'spatial_input_dim': 128,\n"," 'temporal_hidden_size': 20,\n"," 'temporal_input_dim': 200,\n"," 'num_epochs': 1000,\n"," 'patience': 10,\n"," 'major_threshold': None,\n"," 'train_proportion': 0.7,\n"," 'dev_proportion': 0.1,\n"," 'data_path': 'data/AS-733/',\n"," 'prep_data_path': 'prep_data/AS733_neg/'}"]},"metadata":{},"execution_count":3}],"source":["prep = False\n","\n","with open('config/config_AS_baseline.yaml', 'r') as file:\n","    config = yaml.safe_load(file)\n","\n","model_path = config['model_path']\n","\n","if not os.path.exists(model_path):\n","    os.mkdir(model_path)\n","    os.mkdir(model_path + \"best_checkpoints/\")\n","    os.mkdir(model_path + \"latest_checkpoints/\")\n","\n","with open(model_path + 'config.yaml', 'w') as file:\n","    yaml.dump(config, file)\n","\n","config"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29751,"status":"ok","timestamp":1692200701111,"user":{"displayName":"Quandary zhang","userId":"14333747180371872055"},"user_tz":-60},"id":"unCQbLC8Wb0S","outputId":"7e2f92f4-0ee2-413e-c8e2-bcf8f1ee4250"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset splits sizes:  train 60 dev 10 test 20\n"]}],"source":["data = aus.Autonomous_System_Dataset(config['data_path'])\n","tasker = t_lp.Link_Pred_Tasker(data, path=config['prep_data_path'], prep=prep,\n","                               embs_dim=config['spatial_input_dim'], temp_dim=int(config['temporal_input_dim']/10),\n","                               major_threshold=config['major_threshold'], smart_neg_sampling=True)\n","\n","splitter_as = splitter(tasker, train_proportion = config['train_proportion'], dev_proportion = config['dev_proportion'])\n","\n","model= Baseline_node2vec(config['spatial_input_dim'])\n","\n","trainer = Trainer(model=model, splitter=splitter_as, model_path=model_path, adam_config=config['adam_config'], patience=config['patience'])"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wuhZRFQVognD","executionInfo":{"status":"ok","timestamp":1692208237576,"user_tz":-60,"elapsed":7536467,"user":{"displayName":"Quandary zhang","userId":"14333747180371872055"}},"outputId":"bab86691-305d-4bb5-b318-2a2a4381b460"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Step 1, Loss: 0.07661040127277374\n","Epoch 1, Step 2, Loss: 0.021808285266160965\n","Epoch 1, Step 3, Loss: 0.018378430977463722\n","Epoch 1, Step 4, Loss: 0.020400533452630043\n","Epoch 1, Step 5, Loss: 0.023163912817835808\n","Epoch 1, Step 6, Loss: 0.021660037338733673\n","Epoch 1, Step 7, Loss: 0.02187223546206951\n","Epoch 1, Step 8, Loss: 0.031005164608359337\n","Epoch 1, Step 9, Loss: 0.030089542269706726\n","Epoch 1, Step 10, Loss: 0.028898024931550026\n","Epoch 1, Step 11, Loss: 0.024003230035305023\n","Epoch 1, Step 12, Loss: 0.024607956409454346\n","Epoch 1, Step 13, Loss: 0.022434009239077568\n","Epoch 1, Step 14, Loss: 0.023440618067979813\n","Epoch 1, Step 15, Loss: 0.020527714863419533\n","Epoch 1, Step 16, Loss: 0.020121080800890923\n","Epoch 1, Step 17, Loss: 0.01828143186867237\n","Epoch 1, Step 18, Loss: 0.01830928772687912\n","Epoch 1, Step 19, Loss: 0.01859920471906662\n","Epoch 1, Step 20, Loss: 0.017412032932043076\n","Epoch 1, Step 21, Loss: 0.017159925773739815\n","Epoch 1, Step 22, Loss: 0.016236837953329086\n","Epoch 1, Step 23, Loss: 0.018532654270529747\n","Epoch 1, Step 24, Loss: 0.017438210546970367\n","Epoch 1, Step 25, Loss: 0.015999503433704376\n","Epoch 1, Step 26, Loss: 0.017125137150287628\n","Epoch 1, Step 27, Loss: 0.015996579080820084\n","Epoch 1, Step 28, Loss: 0.01604635640978813\n","Epoch 1, Step 29, Loss: 0.014841190539300442\n","Epoch 1, Step 30, Loss: 0.01478701364248991\n","Epoch 1, Step 31, Loss: 0.014261070638895035\n","Epoch 1, Step 32, Loss: 0.015283225104212761\n","Epoch 1, Step 33, Loss: 0.014527150429785252\n","Epoch 1, Step 34, Loss: 0.015213227830827236\n","Epoch 1, Step 35, Loss: 0.01495792530477047\n","Epoch 1, Step 36, Loss: 0.015517501160502434\n","Epoch 1, Step 37, Loss: 0.014975398778915405\n","Epoch 1, Step 38, Loss: 0.014155183918774128\n","Epoch 1, Step 39, Loss: 0.014510524459183216\n","Epoch 1, Step 40, Loss: 0.01537302602082491\n","Epoch 1, Step 41, Loss: 0.014371750876307487\n","Epoch 1, Step 42, Loss: 0.014277291484177113\n","Epoch 1, Step 43, Loss: 0.013763105496764183\n","Epoch 1, Step 44, Loss: 0.01383964717388153\n","Epoch 1, Step 45, Loss: 0.013544280081987381\n","Epoch 1, Step 46, Loss: 0.013905012980103493\n","Epoch 1, Step 47, Loss: 0.013404367491602898\n","Epoch 1, Step 48, Loss: 0.012827783823013306\n","Epoch 1, Step 49, Loss: 0.013348681852221489\n","Epoch 1, Step 50, Loss: 0.013506202958524227\n","Epoch 1, Step 51, Loss: 0.013789297081530094\n","Epoch 1, Step 52, Loss: 0.012837817892432213\n","Epoch 1, Step 53, Loss: 0.013198438100516796\n","Epoch 1, Step 54, Loss: 0.01371765322983265\n","Epoch 1, Step 55, Loss: 0.013995629735291004\n","Epoch 1, Step 56, Loss: 0.013443004339933395\n","Epoch 1, Step 57, Loss: 0.013263463042676449\n","Epoch 1, Step 58, Loss: 0.012338473461568356\n","Epoch 1, Step 59, Loss: 0.012482785619795322\n","Epoch 1, Step 60, Loss: 0.012217683717608452\n","Train Metric MRRs: 0.1050158892721214\n","Train Metric MAPs: 0.08214054300312382\n","Validation Metric MRRs: 0.15497022172148725\n","Validation Metric MAPs: 0.14193543841194803\n","Epoch 2, Step 1, Loss: 0.013427936471998692\n","Epoch 2, Step 2, Loss: 0.012790554203093052\n","Epoch 2, Step 3, Loss: 0.012837774120271206\n","Epoch 2, Step 4, Loss: 0.014626996591687202\n","Epoch 2, Step 5, Loss: 0.012900413945317268\n","Epoch 2, Step 6, Loss: 0.012304462492465973\n","Epoch 2, Step 7, Loss: 0.011914352886378765\n","Epoch 2, Step 8, Loss: 0.013269847258925438\n","Epoch 2, Step 9, Loss: 0.012600063346326351\n","Epoch 2, Step 10, Loss: 0.011806564405560493\n","Epoch 2, Step 11, Loss: 0.012500906363129616\n","Epoch 2, Step 12, Loss: 0.013269316405057907\n","Epoch 2, Step 13, Loss: 0.011913677677512169\n","Epoch 2, Step 14, Loss: 0.013414205051958561\n","Epoch 2, Step 15, Loss: 0.011882157996296883\n","Epoch 2, Step 16, Loss: 0.012715342454612255\n","Epoch 2, Step 17, Loss: 0.01199860218912363\n","Epoch 2, Step 18, Loss: 0.012113439850509167\n","Epoch 2, Step 19, Loss: 0.012126333080232143\n","Epoch 2, Step 20, Loss: 0.011781926266849041\n","Epoch 2, Step 21, Loss: 0.0118415467441082\n","Epoch 2, Step 22, Loss: 0.011984588578343391\n","Epoch 2, Step 23, Loss: 0.011682567186653614\n","Epoch 2, Step 24, Loss: 0.012772133573889732\n","Epoch 2, Step 25, Loss: 0.011711879633367062\n","Epoch 2, Step 26, Loss: 0.011516383849084377\n","Epoch 2, Step 27, Loss: 0.011369366198778152\n","Epoch 2, Step 28, Loss: 0.011021359823644161\n","Epoch 2, Step 29, Loss: 0.011604415252804756\n","Epoch 2, Step 30, Loss: 0.010756170377135277\n","Epoch 2, Step 31, Loss: 0.011716723442077637\n","Epoch 2, Step 32, Loss: 0.011583934538066387\n","Epoch 2, Step 33, Loss: 0.011926832608878613\n","Epoch 2, Step 34, Loss: 0.01135847344994545\n","Epoch 2, Step 35, Loss: 0.010959484614431858\n","Epoch 2, Step 36, Loss: 0.010711350478231907\n","Epoch 2, Step 37, Loss: 0.01129772886633873\n","Epoch 2, Step 38, Loss: 0.010388839058578014\n","Epoch 2, Step 39, Loss: 0.010393702425062656\n","Epoch 2, Step 40, Loss: 0.011087272316217422\n","Epoch 2, Step 41, Loss: 0.010390276089310646\n","Epoch 2, Step 42, Loss: 0.010663657449185848\n","Epoch 2, Step 43, Loss: 0.010479691438376904\n","Epoch 2, Step 44, Loss: 0.010253273881971836\n","Epoch 2, Step 45, Loss: 0.010730382055044174\n","Epoch 2, Step 46, Loss: 0.01105229277163744\n","Epoch 2, Step 47, Loss: 0.010663419030606747\n","Epoch 2, Step 48, Loss: 0.010238596238195896\n","Epoch 2, Step 49, Loss: 0.009719505906105042\n","Epoch 2, Step 50, Loss: 0.010151132941246033\n","Epoch 2, Step 51, Loss: 0.00984297413378954\n","Epoch 2, Step 52, Loss: 0.009227960370481014\n","Epoch 2, Step 53, Loss: 0.009714944288134575\n","Epoch 2, Step 54, Loss: 0.009366404265165329\n","Epoch 2, Step 55, Loss: 0.009907148778438568\n","Epoch 2, Step 56, Loss: 0.009782187640666962\n","Epoch 2, Step 57, Loss: 0.009958157315850258\n","Epoch 2, Step 58, Loss: 0.009344398975372314\n","Epoch 2, Step 59, Loss: 0.010219245217740536\n","Epoch 2, Step 60, Loss: 0.009615875780582428\n","Train Metric MRRs: 0.2205597742580865\n","Train Metric MAPs: 0.17448170086290704\n","Validation Metric MRRs: 0.2767782516401482\n","Validation Metric MAPs: 0.21333496857580317\n","Epoch 3, Step 1, Loss: 0.009636842645704746\n","Epoch 3, Step 2, Loss: 0.009646166115999222\n","Epoch 3, Step 3, Loss: 0.009934542700648308\n","Epoch 3, Step 4, Loss: 0.009317423216998577\n","Epoch 3, Step 5, Loss: 0.008991576731204987\n","Epoch 3, Step 6, Loss: 0.010426187887787819\n","Epoch 3, Step 7, Loss: 0.01045913901180029\n","Epoch 3, Step 8, Loss: 0.010438429191708565\n","Epoch 3, Step 9, Loss: 0.008980835787951946\n","Epoch 3, Step 10, Loss: 0.009301786310970783\n","Epoch 3, Step 11, Loss: 0.010831046849489212\n","Epoch 3, Step 12, Loss: 0.009935464709997177\n","Epoch 3, Step 13, Loss: 0.009662117809057236\n","Epoch 3, Step 14, Loss: 0.01052246056497097\n","Epoch 3, Step 15, Loss: 0.01010499894618988\n","Epoch 3, Step 16, Loss: 0.010169236920773983\n","Epoch 3, Step 17, Loss: 0.009913046844303608\n","Epoch 3, Step 18, Loss: 0.009537232108414173\n","Epoch 3, Step 19, Loss: 0.009348421357572079\n","Epoch 3, Step 20, Loss: 0.009633438661694527\n","Epoch 3, Step 21, Loss: 0.009224279783666134\n","Epoch 3, Step 22, Loss: 0.008767029270529747\n","Epoch 3, Step 23, Loss: 0.008573627099394798\n","Epoch 3, Step 24, Loss: 0.00846987497061491\n","Epoch 3, Step 25, Loss: 0.009963608346879482\n","Epoch 3, Step 26, Loss: 0.009062571451067924\n","Epoch 3, Step 27, Loss: 0.008341912180185318\n","Epoch 3, Step 28, Loss: 0.008437861688435078\n","Epoch 3, Step 29, Loss: 0.0083374148234725\n","Epoch 3, Step 30, Loss: 0.008527555502951145\n","Epoch 3, Step 31, Loss: 0.008097879588603973\n","Epoch 3, Step 32, Loss: 0.008644578978419304\n","Epoch 3, Step 33, Loss: 0.0077226608991622925\n","Epoch 3, Step 34, Loss: 0.008172251284122467\n","Epoch 3, Step 35, Loss: 0.00846425537019968\n","Epoch 3, Step 36, Loss: 0.008306960575282574\n","Epoch 3, Step 37, Loss: 0.007726577576249838\n","Epoch 3, Step 38, Loss: 0.008231496438384056\n","Epoch 3, Step 39, Loss: 0.007644343189895153\n","Epoch 3, Step 40, Loss: 0.007796997204422951\n","Epoch 3, Step 41, Loss: 0.007421627640724182\n","Epoch 3, Step 42, Loss: 0.007693820167332888\n","Epoch 3, Step 43, Loss: 0.007823043502867222\n","Epoch 3, Step 44, Loss: 0.007349720224738121\n","Epoch 3, Step 45, Loss: 0.0072973682545125484\n","Epoch 3, Step 46, Loss: 0.007567156571894884\n","Epoch 3, Step 47, Loss: 0.007455772254616022\n","Epoch 3, Step 48, Loss: 0.006698894780129194\n","Epoch 3, Step 49, Loss: 0.006329786032438278\n","Epoch 3, Step 50, Loss: 0.007068476174026728\n","Epoch 3, Step 51, Loss: 0.006556295789778233\n","Epoch 3, Step 52, Loss: 0.00619125971570611\n","Epoch 3, Step 53, Loss: 0.006466053891927004\n","Epoch 3, Step 54, Loss: 0.00660194456577301\n","Epoch 3, Step 55, Loss: 0.006907945033162832\n","Epoch 3, Step 56, Loss: 0.006652746349573135\n","Epoch 3, Step 57, Loss: 0.00658169062808156\n","Epoch 3, Step 58, Loss: 0.0062583438120782375\n","Epoch 3, Step 59, Loss: 0.007144291419535875\n","Epoch 3, Step 60, Loss: 0.006693069823086262\n","Train Metric MRRs: 0.3361054592814583\n","Train Metric MAPs: 0.24306907932598187\n","Validation Metric MRRs: 0.38849735080057174\n","Validation Metric MAPs: 0.24579988537560835\n","Epoch 4, Step 1, Loss: 0.0072381217032670975\n","Epoch 4, Step 2, Loss: 0.006002254784107208\n","Epoch 4, Step 3, Loss: 0.006323860026896\n","Epoch 4, Step 4, Loss: 0.006558588705956936\n","Epoch 4, Step 5, Loss: 0.006056239362806082\n","Epoch 4, Step 6, Loss: 0.006031414028257132\n","Epoch 4, Step 7, Loss: 0.006316265556961298\n","Epoch 4, Step 8, Loss: 0.006812438368797302\n","Epoch 4, Step 9, Loss: 0.006151043344289064\n","Epoch 4, Step 10, Loss: 0.006702321581542492\n","Epoch 4, Step 11, Loss: 0.0060946885496377945\n","Epoch 4, Step 12, Loss: 0.006331756245344877\n","Epoch 4, Step 13, Loss: 0.005911900661885738\n","Epoch 4, Step 14, Loss: 0.006507663056254387\n","Epoch 4, Step 15, Loss: 0.005884557496756315\n","Epoch 4, Step 16, Loss: 0.006567183416336775\n","Epoch 4, Step 17, Loss: 0.006056644022464752\n","Epoch 4, Step 18, Loss: 0.006247582379728556\n","Epoch 4, Step 19, Loss: 0.005973170977085829\n","Epoch 4, Step 20, Loss: 0.006177731789648533\n","Epoch 4, Step 21, Loss: 0.005422116722911596\n","Epoch 4, Step 22, Loss: 0.0064675104804337025\n","Epoch 4, Step 23, Loss: 0.005570532288402319\n","Epoch 4, Step 24, Loss: 0.005473255179822445\n","Epoch 4, Step 25, Loss: 0.007339072413742542\n","Epoch 4, Step 26, Loss: 0.005894524976611137\n","Epoch 4, Step 27, Loss: 0.005664989817887545\n","Epoch 4, Step 28, Loss: 0.006781945005059242\n","Epoch 4, Step 29, Loss: 0.005216110963374376\n","Epoch 4, Step 30, Loss: 0.005699696019291878\n","Epoch 4, Step 31, Loss: 0.006560525856912136\n","Epoch 4, Step 32, Loss: 0.006209189537912607\n","Epoch 4, Step 33, Loss: 0.006011615507304668\n","Epoch 4, Step 34, Loss: 0.006859568879008293\n","Epoch 4, Step 35, Loss: 0.005740827415138483\n","Epoch 4, Step 36, Loss: 0.009346175007522106\n","Epoch 4, Step 37, Loss: 0.006359957158565521\n","Epoch 4, Step 38, Loss: 0.007881255820393562\n","Epoch 4, Step 39, Loss: 0.005674481391906738\n","Epoch 4, Step 40, Loss: 0.006811206229031086\n","Epoch 4, Step 41, Loss: 0.006948591209948063\n","Epoch 4, Step 42, Loss: 0.0065812719985842705\n","Epoch 4, Step 43, Loss: 0.007285451050847769\n","Epoch 4, Step 44, Loss: 0.006118090823292732\n","Epoch 4, Step 45, Loss: 0.008573976345360279\n","Epoch 4, Step 46, Loss: 0.007009307388216257\n","Epoch 4, Step 47, Loss: 0.008656330406665802\n","Epoch 4, Step 48, Loss: 0.012355707585811615\n","Epoch 4, Step 49, Loss: 0.005773593205958605\n","Epoch 4, Step 50, Loss: 0.006641674321144819\n","Epoch 4, Step 51, Loss: 0.006447107531130314\n","Epoch 4, Step 52, Loss: 0.007053975947201252\n","Epoch 4, Step 53, Loss: 0.00775937270373106\n","Epoch 4, Step 54, Loss: 0.008030906319618225\n","Epoch 4, Step 55, Loss: 0.006553858518600464\n","Epoch 4, Step 56, Loss: 0.006565961521118879\n","Epoch 4, Step 57, Loss: 0.0076642935164272785\n","Epoch 4, Step 58, Loss: 0.007599398493766785\n","Epoch 4, Step 59, Loss: 0.007161962799727917\n","Epoch 4, Step 60, Loss: 0.007474236655980349\n","Train Metric MRRs: 0.4325559904288291\n","Train Metric MAPs: 0.29834287061424164\n","Validation Metric MRRs: 0.3650209561159133\n","Validation Metric MAPs: 0.28649000085586984\n","Epoch 5, Step 1, Loss: 0.007391358260065317\n","Epoch 5, Step 2, Loss: 0.005596239119768143\n","Epoch 5, Step 3, Loss: 0.006987340748310089\n","Epoch 5, Step 4, Loss: 0.0072818477638065815\n","Epoch 5, Step 5, Loss: 0.006172292400151491\n","Epoch 5, Step 6, Loss: 0.009068306535482407\n","Epoch 5, Step 7, Loss: 0.00765782268717885\n","Epoch 5, Step 8, Loss: 0.007174100261181593\n","Epoch 5, Step 9, Loss: 0.008078619837760925\n","Epoch 5, Step 10, Loss: 0.007437400985509157\n","Epoch 5, Step 11, Loss: 0.006890186574310064\n","Epoch 5, Step 12, Loss: 0.006200761068612337\n","Epoch 5, Step 13, Loss: 0.007502463646233082\n","Epoch 5, Step 14, Loss: 0.006641822401434183\n","Epoch 5, Step 15, Loss: 0.006657267455011606\n","Epoch 5, Step 16, Loss: 0.010921969078481197\n","Epoch 5, Step 17, Loss: 0.006458908785134554\n","Epoch 5, Step 18, Loss: 0.00649466784670949\n","Epoch 5, Step 19, Loss: 0.009139454923570156\n","Epoch 5, Step 20, Loss: 0.0074570197612047195\n","Epoch 5, Step 21, Loss: 0.006276176776736975\n","Epoch 5, Step 22, Loss: 0.006675129756331444\n","Epoch 5, Step 23, Loss: 0.007038011681288481\n","Epoch 5, Step 24, Loss: 0.007977855391800404\n","Epoch 5, Step 25, Loss: 0.0070134541019797325\n","Epoch 5, Step 26, Loss: 0.0059888423420488834\n","Epoch 5, Step 27, Loss: 0.007046917919069529\n","Epoch 5, Step 28, Loss: 0.006097853649407625\n","Epoch 5, Step 29, Loss: 0.007208615075796843\n","Epoch 5, Step 30, Loss: 0.006585810333490372\n","Epoch 5, Step 31, Loss: 0.005094466265290976\n","Epoch 5, Step 32, Loss: 0.005980632267892361\n","Epoch 5, Step 33, Loss: 0.006190084852278233\n","Epoch 5, Step 34, Loss: 0.00812368094921112\n","Epoch 5, Step 35, Loss: 0.007517391350120306\n","Epoch 5, Step 36, Loss: 0.006841049529612064\n","Epoch 5, Step 37, Loss: 0.0054950956255197525\n","Epoch 5, Step 38, Loss: 0.005183077417314053\n","Epoch 5, Step 39, Loss: 0.006714265327900648\n","Epoch 5, Step 40, Loss: 0.006512862630188465\n","Epoch 5, Step 41, Loss: 0.006170246284455061\n","Epoch 5, Step 42, Loss: 0.0054178545251488686\n","Epoch 5, Step 43, Loss: 0.005822551902383566\n","Epoch 5, Step 44, Loss: 0.005266109015792608\n","Epoch 5, Step 45, Loss: 0.006086899898946285\n","Epoch 5, Step 46, Loss: 0.005932984873652458\n","Epoch 5, Step 47, Loss: 0.009887711144983768\n","Epoch 5, Step 48, Loss: 0.006576007232069969\n","Epoch 5, Step 49, Loss: 0.005051111802458763\n","Epoch 5, Step 50, Loss: 0.00694040022790432\n","Epoch 5, Step 51, Loss: 0.01529496069997549\n","Epoch 5, Step 52, Loss: 0.006516126450151205\n","Epoch 5, Step 53, Loss: 0.005215349607169628\n","Epoch 5, Step 54, Loss: 0.00597670441493392\n","Epoch 5, Step 55, Loss: 0.008305099792778492\n","Epoch 5, Step 56, Loss: 0.011918649077415466\n","Epoch 5, Step 57, Loss: 0.01320856437087059\n","Epoch 5, Step 58, Loss: 0.008577131666243076\n","Epoch 5, Step 59, Loss: 0.006043526344001293\n","Epoch 5, Step 60, Loss: 0.005373699124902487\n","Train Metric MRRs: 0.44445245685601414\n","Train Metric MAPs: 0.3143433124413451\n","Validation Metric MRRs: 0.419318989780475\n","Validation Metric MAPs: 0.30737080073941897\n","Epoch 6, Step 1, Loss: 0.006908009760081768\n","Epoch 6, Step 2, Loss: 0.009394383057951927\n","Epoch 6, Step 3, Loss: 0.009522508829832077\n","Epoch 6, Step 4, Loss: 0.008319536224007607\n","Epoch 6, Step 5, Loss: 0.007607446052134037\n","Epoch 6, Step 6, Loss: 0.009947318583726883\n","Epoch 6, Step 7, Loss: 0.006319709587842226\n","Epoch 6, Step 8, Loss: 0.014605531468987465\n","Epoch 6, Step 9, Loss: 0.010091137140989304\n","Epoch 6, Step 10, Loss: 0.006753155961632729\n","Epoch 6, Step 11, Loss: 0.006162751000374556\n","Epoch 6, Step 12, Loss: 0.007276090793311596\n","Epoch 6, Step 13, Loss: 0.006582677364349365\n","Epoch 6, Step 14, Loss: 0.006792256608605385\n","Epoch 6, Step 15, Loss: 0.005000262521207333\n","Epoch 6, Step 16, Loss: 0.007438702508807182\n","Epoch 6, Step 17, Loss: 0.007806600071489811\n","Epoch 6, Step 18, Loss: 0.007581793703138828\n","Epoch 6, Step 19, Loss: 0.007865739054977894\n","Epoch 6, Step 20, Loss: 0.005666621495038271\n","Epoch 6, Step 21, Loss: 0.006235180422663689\n","Epoch 6, Step 22, Loss: 0.00685245031490922\n","Epoch 6, Step 23, Loss: 0.0059345923364162445\n","Epoch 6, Step 24, Loss: 0.006691412068903446\n","Epoch 6, Step 25, Loss: 0.005944169592112303\n","Epoch 6, Step 26, Loss: 0.005122392904013395\n","Epoch 6, Step 27, Loss: 0.0055537717416882515\n","Epoch 6, Step 28, Loss: 0.005438597407191992\n","Epoch 6, Step 29, Loss: 0.004686518106609583\n","Epoch 6, Step 30, Loss: 0.004595616832375526\n","Epoch 6, Step 31, Loss: 0.004973438568413258\n","Epoch 6, Step 32, Loss: 0.005465212278068066\n","Epoch 6, Step 33, Loss: 0.005019834265112877\n","Epoch 6, Step 34, Loss: 0.005941742565482855\n","Epoch 6, Step 35, Loss: 0.005361167713999748\n","Epoch 6, Step 36, Loss: 0.0048392838798463345\n","Epoch 6, Step 37, Loss: 0.006076883524656296\n","Epoch 6, Step 38, Loss: 0.00544188404455781\n","Epoch 6, Step 39, Loss: 0.005351079162210226\n","Epoch 6, Step 40, Loss: 0.004809620790183544\n","Epoch 6, Step 41, Loss: 0.005055826157331467\n","Epoch 6, Step 42, Loss: 0.005604132078588009\n","Epoch 6, Step 43, Loss: 0.004954164382070303\n","Epoch 6, Step 44, Loss: 0.0046703326515853405\n","Epoch 6, Step 45, Loss: 0.004594435915350914\n","Epoch 6, Step 46, Loss: 0.005711170844733715\n","Epoch 6, Step 47, Loss: 0.004646431654691696\n","Epoch 6, Step 48, Loss: 0.005727606825530529\n","Epoch 6, Step 49, Loss: 0.004726500250399113\n","Epoch 6, Step 50, Loss: 0.0044664498418569565\n","Epoch 6, Step 51, Loss: 0.0049246130511164665\n","Epoch 6, Step 52, Loss: 0.004698710981756449\n","Epoch 6, Step 53, Loss: 0.005910180974751711\n","Epoch 6, Step 54, Loss: 0.005728515330702066\n","Epoch 6, Step 55, Loss: 0.005244752857834101\n","Epoch 6, Step 56, Loss: 0.004157520364969969\n","Epoch 6, Step 57, Loss: 0.004650054965168238\n","Epoch 6, Step 58, Loss: 0.005274605471640825\n","Epoch 6, Step 59, Loss: 0.005323763005435467\n","Epoch 6, Step 60, Loss: 0.005069116596132517\n","Train Metric MRRs: 0.486509046768736\n","Train Metric MAPs: 0.3389264227400446\n","Validation Metric MRRs: 0.5198673832563243\n","Validation Metric MAPs: 0.3564224704191215\n","Epoch 7, Step 1, Loss: 0.00780612975358963\n","Epoch 7, Step 2, Loss: 0.005503261461853981\n","Epoch 7, Step 3, Loss: 0.005026576574891806\n","Epoch 7, Step 4, Loss: 0.004330734722316265\n","Epoch 7, Step 5, Loss: 0.005663513205945492\n","Epoch 7, Step 6, Loss: 0.006135276053100824\n","Epoch 7, Step 7, Loss: 0.007518770173192024\n","Epoch 7, Step 8, Loss: 0.006925459951162338\n","Epoch 7, Step 9, Loss: 0.004207675810903311\n","Epoch 7, Step 10, Loss: 0.004754065535962582\n","Epoch 7, Step 11, Loss: 0.0059202597476542\n","Epoch 7, Step 12, Loss: 0.007418700493872166\n","Epoch 7, Step 13, Loss: 0.006622618064284325\n","Epoch 7, Step 14, Loss: 0.005417109001427889\n","Epoch 7, Step 15, Loss: 0.004711109213531017\n","Epoch 7, Step 16, Loss: 0.005987089592963457\n","Epoch 7, Step 17, Loss: 0.004864940419793129\n","Epoch 7, Step 18, Loss: 0.006101740524172783\n","Epoch 7, Step 19, Loss: 0.005021949764341116\n","Epoch 7, Step 20, Loss: 0.005989093333482742\n","Epoch 7, Step 21, Loss: 0.006258999463170767\n","Epoch 7, Step 22, Loss: 0.005120659247040749\n","Epoch 7, Step 23, Loss: 0.004198305308818817\n","Epoch 7, Step 24, Loss: 0.0046728914603590965\n","Epoch 7, Step 25, Loss: 0.00488682696595788\n","Epoch 7, Step 26, Loss: 0.005636266432702541\n","Epoch 7, Step 27, Loss: 0.00641902768984437\n","Epoch 7, Step 28, Loss: 0.0049012769013643265\n","Epoch 7, Step 29, Loss: 0.004959784913808107\n","Epoch 7, Step 30, Loss: 0.004069582559168339\n","Epoch 7, Step 31, Loss: 0.004948452580720186\n","Epoch 7, Step 32, Loss: 0.0049281250685453415\n","Epoch 7, Step 33, Loss: 0.0047705331817269325\n","Epoch 7, Step 34, Loss: 0.006154078058898449\n","Epoch 7, Step 35, Loss: 0.004404634237289429\n","Epoch 7, Step 36, Loss: 0.004821660928428173\n","Epoch 7, Step 37, Loss: 0.004087534733116627\n","Epoch 7, Step 38, Loss: 0.0045914496295154095\n","Epoch 7, Step 39, Loss: 0.005088967736810446\n","Epoch 7, Step 40, Loss: 0.005448358133435249\n","Epoch 7, Step 41, Loss: 0.0041007474064826965\n","Epoch 7, Step 42, Loss: 0.004621223080903292\n","Epoch 7, Step 43, Loss: 0.00438374700024724\n","Epoch 7, Step 44, Loss: 0.004220621194690466\n","Epoch 7, Step 45, Loss: 0.004482113290578127\n","Epoch 7, Step 46, Loss: 0.004519006237387657\n","Epoch 7, Step 47, Loss: 0.003976861014962196\n","Epoch 7, Step 48, Loss: 0.004275973420590162\n","Epoch 7, Step 49, Loss: 0.0036298560444265604\n","Epoch 7, Step 50, Loss: 0.004901430103927851\n","Epoch 7, Step 51, Loss: 0.004235307686030865\n","Epoch 7, Step 52, Loss: 0.0040047261863946915\n","Epoch 7, Step 53, Loss: 0.004128493834286928\n","Epoch 7, Step 54, Loss: 0.00457102432847023\n","Epoch 7, Step 55, Loss: 0.0045510586351156235\n","Epoch 7, Step 56, Loss: 0.003789931768551469\n","Epoch 7, Step 57, Loss: 0.004172204993665218\n","Epoch 7, Step 58, Loss: 0.0038465154357254505\n","Epoch 7, Step 59, Loss: 0.0037281725089997053\n","Epoch 7, Step 60, Loss: 0.003923314157873392\n","Train Metric MRRs: 0.5394059796048123\n","Train Metric MAPs: 0.4028403407882069\n","Validation Metric MRRs: 0.524352499662601\n","Validation Metric MAPs: 0.35015926493410504\n","Epoch 8, Step 1, Loss: 0.004543013405054808\n","Epoch 8, Step 2, Loss: 0.0036073457449674606\n","Epoch 8, Step 3, Loss: 0.003976658917963505\n","Epoch 8, Step 4, Loss: 0.003929973114281893\n","Epoch 8, Step 5, Loss: 0.0036423197016119957\n","Epoch 8, Step 6, Loss: 0.004754580557346344\n","Epoch 8, Step 7, Loss: 0.004154249094426632\n","Epoch 8, Step 8, Loss: 0.005094359163194895\n","Epoch 8, Step 9, Loss: 0.00420360779389739\n","Epoch 8, Step 10, Loss: 0.004530701786279678\n","Epoch 8, Step 11, Loss: 0.0036165518686175346\n","Epoch 8, Step 12, Loss: 0.0037634368054568768\n","Epoch 8, Step 13, Loss: 0.003679125104099512\n","Epoch 8, Step 14, Loss: 0.005059889983385801\n","Epoch 8, Step 15, Loss: 0.004640454426407814\n","Epoch 8, Step 16, Loss: 0.0048019192181527615\n","Epoch 8, Step 17, Loss: 0.004811209160834551\n","Epoch 8, Step 18, Loss: 0.0042894636280834675\n","Epoch 8, Step 19, Loss: 0.0031110020354390144\n","Epoch 8, Step 20, Loss: 0.0038160153198987246\n","Epoch 8, Step 21, Loss: 0.003868337022140622\n","Epoch 8, Step 22, Loss: 0.004077949095517397\n","Epoch 8, Step 23, Loss: 0.0041426499374210835\n","Epoch 8, Step 24, Loss: 0.004752930253744125\n","Epoch 8, Step 25, Loss: 0.004409928806126118\n","Epoch 8, Step 26, Loss: 0.0038873585872352123\n","Epoch 8, Step 27, Loss: 0.00437081977725029\n","Epoch 8, Step 28, Loss: 0.003822095924988389\n","Epoch 8, Step 29, Loss: 0.004306764341890812\n","Epoch 8, Step 30, Loss: 0.004593372344970703\n","Epoch 8, Step 31, Loss: 0.0036639936733990908\n","Epoch 8, Step 32, Loss: 0.0037435083650052547\n","Epoch 8, Step 33, Loss: 0.0038278610445559025\n","Epoch 8, Step 34, Loss: 0.003632647218182683\n","Epoch 8, Step 35, Loss: 0.004796357825398445\n","Epoch 8, Step 36, Loss: 0.00350360875017941\n","Epoch 8, Step 37, Loss: 0.004638677462935448\n","Epoch 8, Step 38, Loss: 0.003955747466534376\n","Epoch 8, Step 39, Loss: 0.003980363253504038\n","Epoch 8, Step 40, Loss: 0.00412735203281045\n","Epoch 8, Step 41, Loss: 0.004216089844703674\n","Epoch 8, Step 42, Loss: 0.003922494128346443\n","Epoch 8, Step 43, Loss: 0.003741708816960454\n","Epoch 8, Step 44, Loss: 0.003254338400438428\n","Epoch 8, Step 45, Loss: 0.004169418476521969\n","Epoch 8, Step 46, Loss: 0.004611227661371231\n","Epoch 8, Step 47, Loss: 0.003164625493809581\n","Epoch 8, Step 48, Loss: 0.0033130203373730183\n","Epoch 8, Step 49, Loss: 0.003466387279331684\n","Epoch 8, Step 50, Loss: 0.0035265041515231133\n","Epoch 8, Step 51, Loss: 0.003945119213312864\n","Epoch 8, Step 52, Loss: 0.003538452787324786\n","Epoch 8, Step 53, Loss: 0.003951719496399164\n","Epoch 8, Step 54, Loss: 0.004360399674624205\n","Epoch 8, Step 55, Loss: 0.004272079095244408\n","Epoch 8, Step 56, Loss: 0.00379925686866045\n","Epoch 8, Step 57, Loss: 0.0034006228670477867\n","Epoch 8, Step 58, Loss: 0.003126133233308792\n","Epoch 8, Step 59, Loss: 0.0035073182079941034\n","Epoch 8, Step 60, Loss: 0.0037927869707345963\n","Train Metric MRRs: 0.5837223327186727\n","Train Metric MAPs: 0.44657907738553215\n","Validation Metric MRRs: 0.5619988749004899\n","Validation Metric MAPs: 0.37082050009758627\n","Epoch 9, Step 1, Loss: 0.003640977665781975\n","Epoch 9, Step 2, Loss: 0.0029246031772345304\n","Epoch 9, Step 3, Loss: 0.003269120818004012\n","Epoch 9, Step 4, Loss: 0.0033848551101982594\n","Epoch 9, Step 5, Loss: 0.00341394473798573\n","Epoch 9, Step 6, Loss: 0.0038913364987820387\n","Epoch 9, Step 7, Loss: 0.004215598572045565\n","Epoch 9, Step 8, Loss: 0.0033272739965468645\n","Epoch 9, Step 9, Loss: 0.0029691255185753107\n","Epoch 9, Step 10, Loss: 0.003422236768528819\n","Epoch 9, Step 11, Loss: 0.003250580281019211\n","Epoch 9, Step 12, Loss: 0.004358953796327114\n","Epoch 9, Step 13, Loss: 0.003731529926881194\n","Epoch 9, Step 14, Loss: 0.003155375365167856\n","Epoch 9, Step 15, Loss: 0.003256662981584668\n","Epoch 9, Step 16, Loss: 0.0038721729069948196\n","Epoch 9, Step 17, Loss: 0.0037356435786932707\n","Epoch 9, Step 18, Loss: 0.004936144687235355\n","Epoch 9, Step 19, Loss: 0.003435145365074277\n","Epoch 9, Step 20, Loss: 0.0032431294675916433\n","Epoch 9, Step 21, Loss: 0.004227743484079838\n","Epoch 9, Step 22, Loss: 0.003393331775441766\n","Epoch 9, Step 23, Loss: 0.0032738635782152414\n","Epoch 9, Step 24, Loss: 0.004077995661646128\n","Epoch 9, Step 25, Loss: 0.004166341386735439\n","Epoch 9, Step 26, Loss: 0.004495191853493452\n","Epoch 9, Step 27, Loss: 0.00394635321572423\n","Epoch 9, Step 28, Loss: 0.0035577844828367233\n","Epoch 9, Step 29, Loss: 0.004466638434678316\n","Epoch 9, Step 30, Loss: 0.003983111120760441\n","Epoch 9, Step 31, Loss: 0.0038604114670306444\n","Epoch 9, Step 32, Loss: 0.0037723451387137175\n","Epoch 9, Step 33, Loss: 0.004311942961066961\n","Epoch 9, Step 34, Loss: 0.004722621291875839\n","Epoch 9, Step 35, Loss: 0.003399551846086979\n","Epoch 9, Step 36, Loss: 0.003439993364736438\n","Epoch 9, Step 37, Loss: 0.00330341630615294\n","Epoch 9, Step 38, Loss: 0.004413394723087549\n","Epoch 9, Step 39, Loss: 0.004237605258822441\n","Epoch 9, Step 40, Loss: 0.003138297703117132\n","Epoch 9, Step 41, Loss: 0.0036881323903799057\n","Epoch 9, Step 42, Loss: 0.004432958085089922\n","Epoch 9, Step 43, Loss: 0.003839332377538085\n","Epoch 9, Step 44, Loss: 0.003903335426002741\n","Epoch 9, Step 45, Loss: 0.003975349012762308\n","Epoch 9, Step 46, Loss: 0.0035659708082675934\n","Epoch 9, Step 47, Loss: 0.003415436018258333\n","Epoch 9, Step 48, Loss: 0.0031351363286376\n","Epoch 9, Step 49, Loss: 0.003797018900513649\n","Epoch 9, Step 50, Loss: 0.0041266740299761295\n","Epoch 9, Step 51, Loss: 0.003809332149103284\n","Epoch 9, Step 52, Loss: 0.003106558695435524\n","Epoch 9, Step 53, Loss: 0.003094738582149148\n","Epoch 9, Step 54, Loss: 0.00356810400262475\n","Epoch 9, Step 55, Loss: 0.0038912480231374502\n","Epoch 9, Step 56, Loss: 0.00421911058947444\n","Epoch 9, Step 57, Loss: 0.0040968963876366615\n","Epoch 9, Step 58, Loss: 0.0031284273136407137\n","Epoch 9, Step 59, Loss: 0.002897365018725395\n","Epoch 9, Step 60, Loss: 0.0036328204441815615\n","Train Metric MRRs: 0.6094009279889878\n","Train Metric MAPs: 0.46854202909604464\n","Validation Metric MRRs: 0.583289330864489\n","Validation Metric MAPs: 0.42456157146935236\n","Epoch 10, Step 1, Loss: 0.0030636072624474764\n","Epoch 10, Step 2, Loss: 0.003026941791176796\n","Epoch 10, Step 3, Loss: 0.003418096574023366\n","Epoch 10, Step 4, Loss: 0.003042304189875722\n","Epoch 10, Step 5, Loss: 0.003302796510979533\n","Epoch 10, Step 6, Loss: 0.0036099208518862724\n","Epoch 10, Step 7, Loss: 0.0034629444126039743\n","Epoch 10, Step 8, Loss: 0.003211565315723419\n","Epoch 10, Step 9, Loss: 0.0029597345273941755\n","Epoch 10, Step 10, Loss: 0.0035754956770688295\n","Epoch 10, Step 11, Loss: 0.002968638902530074\n","Epoch 10, Step 12, Loss: 0.0031429429072886705\n","Epoch 10, Step 13, Loss: 0.0029439511708915234\n","Epoch 10, Step 14, Loss: 0.0033824793063104153\n","Epoch 10, Step 15, Loss: 0.0029656928963959217\n","Epoch 10, Step 16, Loss: 0.0037665681447833776\n","Epoch 10, Step 17, Loss: 0.002974938368424773\n","Epoch 10, Step 18, Loss: 0.003712109522894025\n","Epoch 10, Step 19, Loss: 0.003642893163487315\n","Epoch 10, Step 20, Loss: 0.0032173250801861286\n","Epoch 10, Step 21, Loss: 0.0032327508088201284\n","Epoch 10, Step 22, Loss: 0.0028016483411192894\n","Epoch 10, Step 23, Loss: 0.0028967149555683136\n","Epoch 10, Step 24, Loss: 0.0029234751127660275\n","Epoch 10, Step 25, Loss: 0.0029147842433303595\n","Epoch 10, Step 26, Loss: 0.0031175382900983095\n","Epoch 10, Step 27, Loss: 0.0035856200847774744\n","Epoch 10, Step 28, Loss: 0.0029853549785912037\n","Epoch 10, Step 29, Loss: 0.003237761091440916\n","Epoch 10, Step 30, Loss: 0.0033532448578625917\n","Epoch 10, Step 31, Loss: 0.002720803255215287\n","Epoch 10, Step 32, Loss: 0.0028043913189321756\n","Epoch 10, Step 33, Loss: 0.0034883685875684023\n","Epoch 10, Step 34, Loss: 0.00326916272751987\n","Epoch 10, Step 35, Loss: 0.0043563744984567165\n","Epoch 10, Step 36, Loss: 0.0030696834437549114\n","Epoch 10, Step 37, Loss: 0.0036864348221570253\n","Epoch 10, Step 38, Loss: 0.0028265200089663267\n","Epoch 10, Step 39, Loss: 0.0034596771001815796\n","Epoch 10, Step 40, Loss: 0.003364223288372159\n","Epoch 10, Step 41, Loss: 0.0033214306458830833\n","Epoch 10, Step 42, Loss: 0.0033113767858594656\n","Epoch 10, Step 43, Loss: 0.0038042678497731686\n","Epoch 10, Step 44, Loss: 0.0033932337537407875\n","Epoch 10, Step 45, Loss: 0.004014934413135052\n","Epoch 10, Step 46, Loss: 0.004054798278957605\n","Epoch 10, Step 47, Loss: 0.003903079079464078\n","Epoch 10, Step 48, Loss: 0.003996771760284901\n","Epoch 10, Step 49, Loss: 0.0030857648234814405\n","Epoch 10, Step 50, Loss: 0.0029565319418907166\n","Epoch 10, Step 51, Loss: 0.0033804383128881454\n","Epoch 10, Step 52, Loss: 0.0047691152431070805\n","Epoch 10, Step 53, Loss: 0.003778255544602871\n","Epoch 10, Step 54, Loss: 0.004295066464692354\n","Epoch 10, Step 55, Loss: 0.003188515082001686\n","Epoch 10, Step 56, Loss: 0.0029090915340930223\n","Epoch 10, Step 57, Loss: 0.003872825764119625\n","Epoch 10, Step 58, Loss: 0.003781938459724188\n","Epoch 10, Step 59, Loss: 0.003589274827390909\n","Epoch 10, Step 60, Loss: 0.0035132525954395533\n","Train Metric MRRs: 0.6284981741313149\n","Train Metric MAPs: 0.49021564661495015\n","Validation Metric MRRs: 0.5758030762581907\n","Validation Metric MAPs: 0.43554572927809193\n","Epoch 11, Step 1, Loss: 0.004254900850355625\n","Epoch 11, Step 2, Loss: 0.0032689308281987906\n","Epoch 11, Step 3, Loss: 0.0035402141511440277\n","Epoch 11, Step 4, Loss: 0.0032206273172050714\n","Epoch 11, Step 5, Loss: 0.00412968173623085\n","Epoch 11, Step 6, Loss: 0.005072302650660276\n","Epoch 11, Step 7, Loss: 0.004457727074623108\n","Epoch 11, Step 8, Loss: 0.003121701069176197\n","Epoch 11, Step 9, Loss: 0.003579887794330716\n","Epoch 11, Step 10, Loss: 0.003966107498854399\n","Epoch 11, Step 11, Loss: 0.004114903509616852\n","Epoch 11, Step 12, Loss: 0.003769692499190569\n","Epoch 11, Step 13, Loss: 0.0033210664987564087\n","Epoch 11, Step 14, Loss: 0.0032768091186881065\n","Epoch 11, Step 15, Loss: 0.002880405867472291\n","Epoch 11, Step 16, Loss: 0.003866408485919237\n","Epoch 11, Step 17, Loss: 0.003781504463404417\n","Epoch 11, Step 18, Loss: 0.00457913288846612\n","Epoch 11, Step 19, Loss: 0.003960070200264454\n","Epoch 11, Step 20, Loss: 0.0035910161677747965\n","Epoch 11, Step 21, Loss: 0.0029029250144958496\n","Epoch 11, Step 22, Loss: 0.0032150777988135815\n","Epoch 11, Step 23, Loss: 0.003096749307587743\n","Epoch 11, Step 24, Loss: 0.003246901324018836\n","Epoch 11, Step 25, Loss: 0.0034622640814632177\n","Epoch 11, Step 26, Loss: 0.0036889316979795694\n","Epoch 11, Step 27, Loss: 0.0026375045999884605\n","Epoch 11, Step 28, Loss: 0.0029491025488823652\n","Epoch 11, Step 29, Loss: 0.0031516626477241516\n","Epoch 11, Step 30, Loss: 0.0035021479707211256\n","Epoch 11, Step 31, Loss: 0.0034752427600324154\n","Epoch 11, Step 32, Loss: 0.0036723639350384474\n","Epoch 11, Step 33, Loss: 0.0031955165322870016\n","Epoch 11, Step 34, Loss: 0.002921449253335595\n","Epoch 11, Step 35, Loss: 0.0039802114479243755\n","Epoch 11, Step 36, Loss: 0.002935124794021249\n","Epoch 11, Step 37, Loss: 0.003991324454545975\n","Epoch 11, Step 38, Loss: 0.0036016919184476137\n","Epoch 11, Step 39, Loss: 0.0030422366689890623\n","Epoch 11, Step 40, Loss: 0.003349108388647437\n","Epoch 11, Step 41, Loss: 0.0037646398413926363\n","Epoch 11, Step 42, Loss: 0.0031691023614257574\n","Epoch 11, Step 43, Loss: 0.003779110498726368\n","Epoch 11, Step 44, Loss: 0.003358687274158001\n","Epoch 11, Step 45, Loss: 0.0029631752986460924\n","Epoch 11, Step 46, Loss: 0.003274892922490835\n","Epoch 11, Step 47, Loss: 0.0032296073623001575\n","Epoch 11, Step 48, Loss: 0.004272928461432457\n","Epoch 11, Step 49, Loss: 0.0036457572132349014\n","Epoch 11, Step 50, Loss: 0.0041390107944607735\n","Epoch 11, Step 51, Loss: 0.0028982118237763643\n","Epoch 11, Step 52, Loss: 0.003223966108635068\n","Epoch 11, Step 53, Loss: 0.0030906228348612785\n","Epoch 11, Step 54, Loss: 0.003688509576022625\n","Epoch 11, Step 55, Loss: 0.005194765981286764\n","Epoch 11, Step 56, Loss: 0.004802622366696596\n","Epoch 11, Step 57, Loss: 0.0032765078358352184\n","Epoch 11, Step 58, Loss: 0.0034193950705230236\n","Epoch 11, Step 59, Loss: 0.003294514026492834\n","Epoch 11, Step 60, Loss: 0.004638735670596361\n","Train Metric MRRs: 0.6242673658649209\n","Train Metric MAPs: 0.4960112306634196\n","Validation Metric MRRs: 0.5648397528292072\n","Validation Metric MAPs: 0.40760455360969505\n","Epoch 12, Step 1, Loss: 0.005871906876564026\n","Epoch 12, Step 2, Loss: 0.004337839316576719\n","Epoch 12, Step 3, Loss: 0.00376935675740242\n","Epoch 12, Step 4, Loss: 0.003443682100623846\n","Epoch 12, Step 5, Loss: 0.0030936752445995808\n","Epoch 12, Step 6, Loss: 0.0036973650567233562\n","Epoch 12, Step 7, Loss: 0.003539836034178734\n","Epoch 12, Step 8, Loss: 0.007421680726110935\n","Epoch 12, Step 9, Loss: 0.004125896841287613\n","Epoch 12, Step 10, Loss: 0.004283145070075989\n","Epoch 12, Step 11, Loss: 0.003783096559345722\n","Epoch 12, Step 12, Loss: 0.004290824290364981\n","Epoch 12, Step 13, Loss: 0.004213284235447645\n","Epoch 12, Step 14, Loss: 0.004911127034574747\n","Epoch 12, Step 15, Loss: 0.005053545814007521\n","Epoch 12, Step 16, Loss: 0.003955195192247629\n","Epoch 12, Step 17, Loss: 0.005181594751775265\n","Epoch 12, Step 18, Loss: 0.005061175674200058\n","Epoch 12, Step 19, Loss: 0.0035083505790680647\n","Epoch 12, Step 20, Loss: 0.004599784035235643\n","Epoch 12, Step 21, Loss: 0.00464736670255661\n","Epoch 12, Step 22, Loss: 0.0038936494383960962\n","Epoch 12, Step 23, Loss: 0.0034006163477897644\n","Epoch 12, Step 24, Loss: 0.003528754925355315\n","Epoch 12, Step 25, Loss: 0.0036776673514395952\n","Epoch 12, Step 26, Loss: 0.003940827213227749\n","Epoch 12, Step 27, Loss: 0.004554527346044779\n","Epoch 12, Step 28, Loss: 0.0035646255128085613\n","Epoch 12, Step 29, Loss: 0.0032163355499505997\n","Epoch 12, Step 30, Loss: 0.0032402786891907454\n","Epoch 12, Step 31, Loss: 0.003476717509329319\n","Epoch 12, Step 32, Loss: 0.0035068884026259184\n","Epoch 12, Step 33, Loss: 0.003685314441099763\n","Epoch 12, Step 34, Loss: 0.0033208667300641537\n","Epoch 12, Step 35, Loss: 0.003717470448464155\n","Epoch 12, Step 36, Loss: 0.00374214886687696\n","Epoch 12, Step 37, Loss: 0.0033689301926642656\n","Epoch 12, Step 38, Loss: 0.00405043363571167\n","Epoch 12, Step 39, Loss: 0.0038857730105519295\n","Epoch 12, Step 40, Loss: 0.0034113700967282057\n","Epoch 12, Step 41, Loss: 0.0037200546357780695\n","Epoch 12, Step 42, Loss: 0.002699911594390869\n","Epoch 12, Step 43, Loss: 0.0038688797503709793\n","Epoch 12, Step 44, Loss: 0.0035517574287950993\n","Epoch 12, Step 45, Loss: 0.003843232523649931\n","Epoch 12, Step 46, Loss: 0.0033378463704138994\n","Epoch 12, Step 47, Loss: 0.003211043309420347\n","Epoch 12, Step 48, Loss: 0.0034661972895264626\n","Epoch 12, Step 49, Loss: 0.004206940997391939\n","Epoch 12, Step 50, Loss: 0.003883018856868148\n","Epoch 12, Step 51, Loss: 0.0035922315437346697\n","Epoch 12, Step 52, Loss: 0.0035036320332437754\n","Epoch 12, Step 53, Loss: 0.0028650311287492514\n","Epoch 12, Step 54, Loss: 0.003662816248834133\n","Epoch 12, Step 55, Loss: 0.0035250235814601183\n","Epoch 12, Step 56, Loss: 0.003306244034320116\n","Epoch 12, Step 57, Loss: 0.004328708630055189\n","Epoch 12, Step 58, Loss: 0.006024331785738468\n","Epoch 12, Step 59, Loss: 0.0046478561125695705\n","Epoch 12, Step 60, Loss: 0.003116043983027339\n","Train Metric MRRs: 0.6134723678921189\n","Train Metric MAPs: 0.49046552485784944\n","Validation Metric MRRs: 0.5748470271977404\n","Validation Metric MAPs: 0.37735151156484403\n","Epoch 13, Step 1, Loss: 0.0036059925332665443\n","Epoch 13, Step 2, Loss: 0.003394938074052334\n","Epoch 13, Step 3, Loss: 0.003850761102512479\n","Epoch 13, Step 4, Loss: 0.004633727017790079\n","Epoch 13, Step 5, Loss: 0.0036041252315044403\n","Epoch 13, Step 6, Loss: 0.007577012293040752\n","Epoch 13, Step 7, Loss: 0.0047162980772554874\n","Epoch 13, Step 8, Loss: 0.003988648299127817\n","Epoch 13, Step 9, Loss: 0.0046788654290139675\n","Epoch 13, Step 10, Loss: 0.004931178409606218\n","Epoch 13, Step 11, Loss: 0.00466048251837492\n","Epoch 13, Step 12, Loss: 0.0055486042983829975\n","Epoch 13, Step 13, Loss: 0.003953374456614256\n","Epoch 13, Step 14, Loss: 0.004117666278034449\n","Epoch 13, Step 15, Loss: 0.003935026004910469\n","Epoch 13, Step 16, Loss: 0.004727758467197418\n","Epoch 13, Step 17, Loss: 0.005590787623077631\n","Epoch 13, Step 18, Loss: 0.007569605018943548\n","Epoch 13, Step 19, Loss: 0.003134043887257576\n","Epoch 13, Step 20, Loss: 0.003371814964339137\n","Epoch 13, Step 21, Loss: 0.00392636563628912\n","Epoch 13, Step 22, Loss: 0.004832742735743523\n","Epoch 13, Step 23, Loss: 0.0034105475060641766\n","Epoch 13, Step 24, Loss: 0.005330563522875309\n","Epoch 13, Step 25, Loss: 0.003896643640473485\n","Epoch 13, Step 26, Loss: 0.0036042691208422184\n","Epoch 13, Step 27, Loss: 0.002993169939145446\n","Epoch 13, Step 28, Loss: 0.0036859503015875816\n","Epoch 13, Step 29, Loss: 0.004487073514610529\n","Epoch 13, Step 30, Loss: 0.0036922679282724857\n","Epoch 13, Step 31, Loss: 0.003175708930939436\n","Epoch 13, Step 32, Loss: 0.0038108830340206623\n","Epoch 13, Step 33, Loss: 0.003267766674980521\n","Epoch 13, Step 34, Loss: 0.004550223704427481\n","Epoch 13, Step 35, Loss: 0.0034394613467156887\n","Epoch 13, Step 36, Loss: 0.003623268101364374\n","Epoch 13, Step 37, Loss: 0.003695588791742921\n","Epoch 13, Step 38, Loss: 0.003608613507822156\n","Epoch 13, Step 39, Loss: 0.0028969033155590296\n","Epoch 13, Step 40, Loss: 0.0029390151612460613\n","Epoch 13, Step 41, Loss: 0.003789512673392892\n","Epoch 13, Step 42, Loss: 0.0033301585353910923\n","Epoch 13, Step 43, Loss: 0.003994158934801817\n","Epoch 13, Step 44, Loss: 0.0031549246050417423\n","Epoch 13, Step 45, Loss: 0.00336999143473804\n","Epoch 13, Step 46, Loss: 0.003002001205459237\n","Epoch 13, Step 47, Loss: 0.003072817111387849\n","Epoch 13, Step 48, Loss: 0.004872053861618042\n","Epoch 13, Step 49, Loss: 0.0030736722983419895\n","Epoch 13, Step 50, Loss: 0.0029926584102213383\n","Epoch 13, Step 51, Loss: 0.002760597737506032\n","Epoch 13, Step 52, Loss: 0.004208538215607405\n","Epoch 13, Step 53, Loss: 0.0036986421328037977\n","Epoch 13, Step 54, Loss: 0.003956775180995464\n","Epoch 13, Step 55, Loss: 0.004633641801774502\n","Epoch 13, Step 56, Loss: 0.003486296394839883\n","Epoch 13, Step 57, Loss: 0.0031548866536468267\n","Epoch 13, Step 58, Loss: 0.0032684756442904472\n","Epoch 13, Step 59, Loss: 0.005221507046371698\n","Epoch 13, Step 60, Loss: 0.007976782508194447\n","Train Metric MRRs: 0.6089108824587869\n","Train Metric MAPs: 0.4841979303618323\n","Validation Metric MRRs: 0.5576661861431349\n","Validation Metric MAPs: 0.4110394959875937\n","Epoch 14, Step 1, Loss: 0.004546172916889191\n","Epoch 14, Step 2, Loss: 0.003415339393541217\n","Epoch 14, Step 3, Loss: 0.003792047733440995\n","Epoch 14, Step 4, Loss: 0.004060365725308657\n","Epoch 14, Step 5, Loss: 0.0043397159315645695\n","Epoch 14, Step 6, Loss: 0.00843977089971304\n","Epoch 14, Step 7, Loss: 0.006695499178022146\n","Epoch 14, Step 8, Loss: 0.005454471334815025\n","Epoch 14, Step 9, Loss: 0.0034115854650735855\n","Epoch 14, Step 10, Loss: 0.004528476390987635\n","Epoch 14, Step 11, Loss: 0.006576376035809517\n","Epoch 14, Step 12, Loss: 0.01361573301255703\n","Epoch 14, Step 13, Loss: 0.007967236451804638\n","Epoch 14, Step 14, Loss: 0.003938168752938509\n","Epoch 14, Step 15, Loss: 0.004488606937229633\n","Epoch 14, Step 16, Loss: 0.005110240075737238\n","Epoch 14, Step 17, Loss: 0.008192146196961403\n","Epoch 14, Step 18, Loss: 0.010824527591466904\n","Epoch 14, Step 19, Loss: 0.005157036241143942\n","Epoch 14, Step 20, Loss: 0.00691103283315897\n","Epoch 14, Step 21, Loss: 0.009431361220777035\n","Epoch 14, Step 22, Loss: 0.01234052237123251\n","Epoch 14, Step 23, Loss: 0.007151944097131491\n","Epoch 14, Step 24, Loss: 0.004810328595340252\n","Epoch 14, Step 25, Loss: 0.005947582423686981\n","Epoch 14, Step 26, Loss: 0.01017206534743309\n","Epoch 14, Step 27, Loss: 0.007949350401759148\n","Epoch 14, Step 28, Loss: 0.005113242659717798\n","Epoch 14, Step 29, Loss: 0.009616165421903133\n","Epoch 14, Step 30, Loss: 0.004283314570784569\n","Epoch 14, Step 31, Loss: 0.004155587870627642\n","Epoch 14, Step 32, Loss: 0.004635821562260389\n","Epoch 14, Step 33, Loss: 0.005419720895588398\n","Epoch 14, Step 34, Loss: 0.009145380929112434\n","Epoch 14, Step 35, Loss: 0.0050231036730110645\n","Epoch 14, Step 36, Loss: 0.005966183263808489\n","Epoch 14, Step 37, Loss: 0.00528431311249733\n","Epoch 14, Step 38, Loss: 0.005252316128462553\n","Epoch 14, Step 39, Loss: 0.0054815844632685184\n","Epoch 14, Step 40, Loss: 0.00517750158905983\n","Epoch 14, Step 41, Loss: 0.00429677776992321\n","Epoch 14, Step 42, Loss: 0.0061766356229782104\n","Epoch 14, Step 43, Loss: 0.00507778488099575\n","Epoch 14, Step 44, Loss: 0.004608097951859236\n","Epoch 14, Step 45, Loss: 0.004992479924112558\n","Epoch 14, Step 46, Loss: 0.005268034990876913\n","Epoch 14, Step 47, Loss: 0.004942386876791716\n","Epoch 14, Step 48, Loss: 0.004203738644719124\n","Epoch 14, Step 49, Loss: 0.004417360294610262\n","Epoch 14, Step 50, Loss: 0.005608359817415476\n","Epoch 14, Step 51, Loss: 0.004573006648570299\n","Epoch 14, Step 52, Loss: 0.005353962071239948\n","Epoch 14, Step 53, Loss: 0.004999644123017788\n","Epoch 14, Step 54, Loss: 0.004253562074154615\n","Epoch 14, Step 55, Loss: 0.005743149667978287\n","Epoch 14, Step 56, Loss: 0.004914452787488699\n","Epoch 14, Step 57, Loss: 0.0045880284160375595\n","Epoch 14, Step 58, Loss: 0.00450384896248579\n","Epoch 14, Step 59, Loss: 0.0031957642640918493\n","Epoch 14, Step 60, Loss: 0.005225760862231255\n","Train Metric MRRs: 0.5464757475306736\n","Train Metric MAPs: 0.40046546177198694\n","Validation Metric MRRs: 0.5251667860006224\n","Validation Metric MAPs: 0.3904586162277388\n","Epoch 15, Step 1, Loss: 0.005554277449846268\n","Epoch 15, Step 2, Loss: 0.005517593119293451\n","Epoch 15, Step 3, Loss: 0.003767952788621187\n","Epoch 15, Step 4, Loss: 0.003655884647741914\n","Epoch 15, Step 5, Loss: 0.0042898571118712425\n","Epoch 15, Step 6, Loss: 0.0038447733968496323\n","Epoch 15, Step 7, Loss: 0.0035967917647212744\n","Epoch 15, Step 8, Loss: 0.008746995590627193\n","Epoch 15, Step 9, Loss: 0.005985501687973738\n","Epoch 15, Step 10, Loss: 0.004840954206883907\n","Epoch 15, Step 11, Loss: 0.0038193142972886562\n","Epoch 15, Step 12, Loss: 0.003775976365432143\n","Epoch 15, Step 13, Loss: 0.003519999096170068\n","Epoch 15, Step 14, Loss: 0.006927484646439552\n","Epoch 15, Step 15, Loss: 0.00578869367018342\n","Epoch 15, Step 16, Loss: 0.005636895075440407\n","Epoch 15, Step 17, Loss: 0.008777973242104053\n","Epoch 15, Step 18, Loss: 0.0050039454363286495\n","Epoch 15, Step 19, Loss: 0.0053919842466712\n","Epoch 15, Step 20, Loss: 0.009757108055055141\n","Epoch 15, Step 21, Loss: 0.006988862529397011\n","Epoch 15, Step 22, Loss: 0.00664412509649992\n","Epoch 15, Step 23, Loss: 0.006112093571573496\n","Epoch 15, Step 24, Loss: 0.00802195817232132\n","Epoch 15, Step 25, Loss: 0.007040910888463259\n","Epoch 15, Step 26, Loss: 0.004682162776589394\n","Epoch 15, Step 27, Loss: 0.0042897663079202175\n","Epoch 15, Step 28, Loss: 0.005625061225146055\n","Epoch 15, Step 29, Loss: 0.004978327080607414\n","Epoch 15, Step 30, Loss: 0.010530037805438042\n","Epoch 15, Step 31, Loss: 0.006993675604462624\n","Epoch 15, Step 32, Loss: 0.008824891410768032\n","Epoch 15, Step 33, Loss: 0.005713731050491333\n","Epoch 15, Step 34, Loss: 0.005215376149863005\n","Epoch 15, Step 35, Loss: 0.006972858216613531\n","Epoch 15, Step 36, Loss: 0.005098322406411171\n","Epoch 15, Step 37, Loss: 0.007882053032517433\n","Epoch 15, Step 38, Loss: 0.006703188642859459\n","Epoch 15, Step 39, Loss: 0.007262192666530609\n","Epoch 15, Step 40, Loss: 0.006742074154317379\n","Epoch 15, Step 41, Loss: 0.006384409498423338\n","Epoch 15, Step 42, Loss: 0.005667404271662235\n","Epoch 15, Step 43, Loss: 0.004198126494884491\n","Epoch 15, Step 44, Loss: 0.004215994384139776\n","Epoch 15, Step 45, Loss: 0.004001638386398554\n","Epoch 15, Step 46, Loss: 0.004485802724957466\n","Epoch 15, Step 47, Loss: 0.004727305378764868\n","Epoch 15, Step 48, Loss: 0.004205650184303522\n","Epoch 15, Step 49, Loss: 0.003567150793969631\n","Epoch 15, Step 50, Loss: 0.0035943312104791403\n","Epoch 15, Step 51, Loss: 0.004205165430903435\n","Epoch 15, Step 52, Loss: 0.005645724944770336\n","Epoch 15, Step 53, Loss: 0.0055135805159807205\n","Epoch 15, Step 54, Loss: 0.0035718518774956465\n","Epoch 15, Step 55, Loss: 0.003959973342716694\n","Epoch 15, Step 56, Loss: 0.003228073241189122\n","Epoch 15, Step 57, Loss: 0.003850540379062295\n","Epoch 15, Step 58, Loss: 0.004470867570489645\n","Epoch 15, Step 59, Loss: 0.003506173612549901\n","Epoch 15, Step 60, Loss: 0.003376943292096257\n","Train Metric MRRs: 0.559108872660932\n","Train Metric MAPs: 0.38619075423973087\n","Validation Metric MRRs: 0.560639416407409\n","Validation Metric MAPs: 0.39924055424546223\n","Epoch 16, Step 1, Loss: 0.0035089750308543444\n","Epoch 16, Step 2, Loss: 0.0035046017728745937\n","Epoch 16, Step 3, Loss: 0.0037981104105710983\n","Epoch 16, Step 4, Loss: 0.004810969345271587\n","Epoch 16, Step 5, Loss: 0.0034872950054705143\n","Epoch 16, Step 6, Loss: 0.0034111954737454653\n","Epoch 16, Step 7, Loss: 0.003914761357009411\n","Epoch 16, Step 8, Loss: 0.003991713281720877\n","Epoch 16, Step 9, Loss: 0.0038770793471485376\n","Epoch 16, Step 10, Loss: 0.005734934005886316\n","Epoch 16, Step 11, Loss: 0.003550020745024085\n","Epoch 16, Step 12, Loss: 0.003762442385777831\n","Epoch 16, Step 13, Loss: 0.003481267485767603\n","Epoch 16, Step 14, Loss: 0.0030760951340198517\n","Epoch 16, Step 15, Loss: 0.0031309991609305143\n","Epoch 16, Step 16, Loss: 0.004686217289417982\n","Epoch 16, Step 17, Loss: 0.004625629633665085\n","Epoch 16, Step 18, Loss: 0.006726848892867565\n","Epoch 16, Step 19, Loss: 0.0036587188951671124\n","Epoch 16, Step 20, Loss: 0.003403792390599847\n","Epoch 16, Step 21, Loss: 0.0034583681263029575\n","Epoch 16, Step 22, Loss: 0.0037128895055502653\n","Epoch 16, Step 23, Loss: 0.005083020776510239\n","Epoch 16, Step 24, Loss: 0.005510995630174875\n","Epoch 16, Step 25, Loss: 0.00435599172487855\n","Epoch 16, Step 26, Loss: 0.004800043534487486\n","Epoch 16, Step 27, Loss: 0.004207185003906488\n","Epoch 16, Step 28, Loss: 0.004463092889636755\n","Epoch 16, Step 29, Loss: 0.005432517733424902\n","Epoch 16, Step 30, Loss: 0.003410900244489312\n","Epoch 16, Step 31, Loss: 0.004059940110892057\n","Epoch 16, Step 32, Loss: 0.005245363339781761\n","Epoch 16, Step 33, Loss: 0.006986952852457762\n","Epoch 16, Step 34, Loss: 0.006484351586550474\n","Epoch 16, Step 35, Loss: 0.005663190968334675\n","Epoch 16, Step 36, Loss: 0.005878100171685219\n","Epoch 16, Step 37, Loss: 0.0049333833158016205\n","Epoch 16, Step 38, Loss: 0.004784430842846632\n","Epoch 16, Step 39, Loss: 0.004998472519218922\n","Epoch 16, Step 40, Loss: 0.005180465057492256\n","Epoch 16, Step 41, Loss: 0.0038619909901171923\n","Epoch 16, Step 42, Loss: 0.007146209478378296\n","Epoch 16, Step 43, Loss: 0.004813261330127716\n","Epoch 16, Step 44, Loss: 0.005380925722420216\n","Epoch 16, Step 45, Loss: 0.004101733211427927\n","Epoch 16, Step 46, Loss: 0.004689333960413933\n","Epoch 16, Step 47, Loss: 0.004018463660031557\n","Epoch 16, Step 48, Loss: 0.00489479536190629\n","Epoch 16, Step 49, Loss: 0.0040809684433043\n","Epoch 16, Step 50, Loss: 0.0043998523615300655\n","Epoch 16, Step 51, Loss: 0.0047856164164841175\n","Epoch 16, Step 52, Loss: 0.005396448541432619\n","Epoch 16, Step 53, Loss: 0.004365873523056507\n","Epoch 16, Step 54, Loss: 0.004950999282300472\n","Epoch 16, Step 55, Loss: 0.003490992123261094\n","Epoch 16, Step 56, Loss: 0.002975859446451068\n","Epoch 16, Step 57, Loss: 0.003926904406398535\n","Epoch 16, Step 58, Loss: 0.003573266789317131\n","Epoch 16, Step 59, Loss: 0.004087448585778475\n","Epoch 16, Step 60, Loss: 0.0031499159522354603\n","Train Metric MRRs: 0.5799567013065124\n","Train Metric MAPs: 0.4118788383818806\n","Validation Metric MRRs: 0.5733774867765093\n","Validation Metric MAPs: 0.35914762779222986\n","Epoch 17, Step 1, Loss: 0.003844284685328603\n","Epoch 17, Step 2, Loss: 0.0033057965338230133\n","Epoch 17, Step 3, Loss: 0.0033484436571598053\n","Epoch 17, Step 4, Loss: 0.0034917527809739113\n","Epoch 17, Step 5, Loss: 0.003691965015605092\n","Epoch 17, Step 6, Loss: 0.003652719547972083\n","Epoch 17, Step 7, Loss: 0.0042096455581486225\n","Epoch 17, Step 8, Loss: 0.0034099691547453403\n","Epoch 17, Step 9, Loss: 0.0030897771939635277\n","Epoch 17, Step 10, Loss: 0.0034921085461974144\n","Epoch 17, Step 11, Loss: 0.002924926346167922\n","Epoch 17, Step 12, Loss: 0.0033644591458141804\n","Epoch 17, Step 13, Loss: 0.0028468226082623005\n","Epoch 17, Step 14, Loss: 0.003273320384323597\n","Epoch 17, Step 15, Loss: 0.003148601623252034\n","Epoch 17, Step 16, Loss: 0.003510352224111557\n","Epoch 17, Step 17, Loss: 0.0029591196216642857\n","Epoch 17, Step 18, Loss: 0.0036361704114824533\n","Epoch 17, Step 19, Loss: 0.0032107550650835037\n","Epoch 17, Step 20, Loss: 0.0030622684862464666\n","Epoch 17, Step 21, Loss: 0.00333341583609581\n","Epoch 17, Step 22, Loss: 0.00316714309155941\n","Epoch 17, Step 23, Loss: 0.0032215071842074394\n","Epoch 17, Step 24, Loss: 0.003235867712646723\n","Epoch 17, Step 25, Loss: 0.003413097932934761\n","Epoch 17, Step 26, Loss: 0.003932117950171232\n","Epoch 17, Step 27, Loss: 0.005049240309745073\n","Epoch 17, Step 28, Loss: 0.003229205496609211\n","Epoch 17, Step 29, Loss: 0.003757962491363287\n","Epoch 17, Step 30, Loss: 0.003133373102173209\n","Epoch 17, Step 31, Loss: 0.002767771715298295\n","Epoch 17, Step 32, Loss: 0.0030004670843482018\n","Epoch 17, Step 33, Loss: 0.004709142260253429\n","Epoch 17, Step 34, Loss: 0.004522978328168392\n","Epoch 17, Step 35, Loss: 0.004202021285891533\n","Epoch 17, Step 36, Loss: 0.003887276165187359\n","Epoch 17, Step 37, Loss: 0.00465602008625865\n","Epoch 17, Step 38, Loss: 0.003801136976107955\n","Epoch 17, Step 39, Loss: 0.004228958860039711\n","Epoch 17, Step 40, Loss: 0.004394727759063244\n","Epoch 17, Step 41, Loss: 0.0036281710490584373\n","Epoch 17, Step 42, Loss: 0.004311871714890003\n","Epoch 17, Step 43, Loss: 0.0032564769499003887\n","Epoch 17, Step 44, Loss: 0.004101228900253773\n","Epoch 17, Step 45, Loss: 0.004154527094215155\n","Epoch 17, Step 46, Loss: 0.005799828562885523\n","Epoch 17, Step 47, Loss: 0.00298116821795702\n","Epoch 17, Step 48, Loss: 0.0036153278779238462\n","Epoch 17, Step 49, Loss: 0.003266129642724991\n","Epoch 17, Step 50, Loss: 0.0035202479921281338\n","Epoch 17, Step 51, Loss: 0.004180759657174349\n","Epoch 17, Step 52, Loss: 0.004538623616099358\n","Epoch 17, Step 53, Loss: 0.004366596695035696\n","Epoch 17, Step 54, Loss: 0.0043807863257825375\n","Epoch 17, Step 55, Loss: 0.0038517084904015064\n","Epoch 17, Step 56, Loss: 0.0032857556361705065\n","Epoch 17, Step 57, Loss: 0.0029079962987452745\n","Epoch 17, Step 58, Loss: 0.0031825443729758263\n","Epoch 17, Step 59, Loss: 0.003207551082596183\n","Epoch 17, Step 60, Loss: 0.003040529787540436\n","Train Metric MRRs: 0.6117931913412583\n","Train Metric MAPs: 0.44225662628293266\n","Validation Metric MRRs: 0.5642921047308399\n","Validation Metric MAPs: 0.3332379186852695\n","Epoch 18, Step 1, Loss: 0.0039467960596084595\n","Epoch 18, Step 2, Loss: 0.0032505164854228497\n","Epoch 18, Step 3, Loss: 0.003736109472811222\n","Epoch 18, Step 4, Loss: 0.0027783415280282497\n","Epoch 18, Step 5, Loss: 0.0029271089006215334\n","Epoch 18, Step 6, Loss: 0.0026412575971335173\n","Epoch 18, Step 7, Loss: 0.003151037497445941\n","Epoch 18, Step 8, Loss: 0.0028651943430304527\n","Epoch 18, Step 9, Loss: 0.0030244828667491674\n","Epoch 18, Step 10, Loss: 0.003042935160920024\n","Epoch 18, Step 11, Loss: 0.0028195171616971493\n","Epoch 18, Step 12, Loss: 0.002802670933306217\n","Epoch 18, Step 13, Loss: 0.0026361907366663218\n","Epoch 18, Step 14, Loss: 0.0027215511072427034\n","Epoch 18, Step 15, Loss: 0.0028600534424185753\n","Epoch 18, Step 16, Loss: 0.00323591660708189\n","Epoch 18, Step 17, Loss: 0.0027149366214871407\n","Epoch 18, Step 18, Loss: 0.0033727444242686033\n","Epoch 18, Step 19, Loss: 0.002707680920138955\n","Epoch 18, Step 20, Loss: 0.002913493663072586\n","Epoch 18, Step 21, Loss: 0.0030321376398205757\n","Epoch 18, Step 22, Loss: 0.0029546497389674187\n","Epoch 18, Step 23, Loss: 0.002614497672766447\n","Epoch 18, Step 24, Loss: 0.0028179006185382605\n","Epoch 18, Step 25, Loss: 0.0029990694019943476\n","Epoch 18, Step 26, Loss: 0.00311110308393836\n","Epoch 18, Step 27, Loss: 0.004072817973792553\n","Epoch 18, Step 28, Loss: 0.0031882161274552345\n","Epoch 18, Step 29, Loss: 0.003753610886633396\n","Epoch 18, Step 30, Loss: 0.003195882774889469\n","Epoch 18, Step 31, Loss: 0.002278878353536129\n","Epoch 18, Step 32, Loss: 0.00260940077714622\n","Epoch 18, Step 33, Loss: 0.0034158858470618725\n","Epoch 18, Step 34, Loss: 0.0037523177452385426\n","Epoch 18, Step 35, Loss: 0.004208169877529144\n","Epoch 18, Step 36, Loss: 0.0033293156884610653\n","Epoch 18, Step 37, Loss: 0.004392580129206181\n","Epoch 18, Step 38, Loss: 0.003491856623440981\n","Epoch 18, Step 39, Loss: 0.0034606819972395897\n","Epoch 18, Step 40, Loss: 0.0037270192988216877\n","Epoch 18, Step 41, Loss: 0.0034563669469207525\n","Epoch 18, Step 42, Loss: 0.0034283765126019716\n","Epoch 18, Step 43, Loss: 0.0029709157533943653\n","Epoch 18, Step 44, Loss: 0.0034466383513063192\n","Epoch 18, Step 45, Loss: 0.0037852791137993336\n","Epoch 18, Step 46, Loss: 0.004742853809148073\n","Epoch 18, Step 47, Loss: 0.0028459117747843266\n","Epoch 18, Step 48, Loss: 0.0034043160267174244\n","Epoch 18, Step 49, Loss: 0.003549048211425543\n","Epoch 18, Step 50, Loss: 0.003323018318042159\n","Epoch 18, Step 51, Loss: 0.0033391518518328667\n","Epoch 18, Step 52, Loss: 0.003540105652064085\n","Epoch 18, Step 53, Loss: 0.0037527906242758036\n","Epoch 18, Step 54, Loss: 0.004209356382489204\n","Epoch 18, Step 55, Loss: 0.004144306294620037\n","Epoch 18, Step 56, Loss: 0.0037818357814103365\n","Epoch 18, Step 57, Loss: 0.003253266680985689\n","Epoch 18, Step 58, Loss: 0.003499804064631462\n","Epoch 18, Step 59, Loss: 0.00274235219694674\n","Epoch 18, Step 60, Loss: 0.002733231522142887\n","Train Metric MRRs: 0.6374272236434251\n","Train Metric MAPs: 0.46970096614291384\n","Validation Metric MRRs: 0.5713956136570747\n","Validation Metric MAPs: 0.3377312530640486\n","Epoch 19, Step 1, Loss: 0.003546948079019785\n","Epoch 19, Step 2, Loss: 0.0029958095401525497\n","Epoch 19, Step 3, Loss: 0.003881145967170596\n","Epoch 19, Step 4, Loss: 0.0030185307841748\n","Epoch 19, Step 5, Loss: 0.003232155926525593\n","Epoch 19, Step 6, Loss: 0.0027787501458078623\n","Epoch 19, Step 7, Loss: 0.0028976767789572477\n","Epoch 19, Step 8, Loss: 0.002612276468425989\n","Epoch 19, Step 9, Loss: 0.0028630043379962444\n","Epoch 19, Step 10, Loss: 0.0029929704032838345\n","Epoch 19, Step 11, Loss: 0.003020109375938773\n","Epoch 19, Step 12, Loss: 0.0029408270493149757\n","Epoch 19, Step 13, Loss: 0.002749064238741994\n","Epoch 19, Step 14, Loss: 0.002498962916433811\n","Epoch 19, Step 15, Loss: 0.0026738785672932863\n","Epoch 19, Step 16, Loss: 0.003102295333519578\n","Epoch 19, Step 17, Loss: 0.0028586876578629017\n","Epoch 19, Step 18, Loss: 0.0033882507123053074\n","Epoch 19, Step 19, Loss: 0.0027705340180546045\n","Epoch 19, Step 20, Loss: 0.002719447948038578\n","Epoch 19, Step 21, Loss: 0.0028384022880345583\n","Epoch 19, Step 22, Loss: 0.0026596966199576855\n","Epoch 19, Step 23, Loss: 0.0023836151231080294\n","Epoch 19, Step 24, Loss: 0.002736822934821248\n","Epoch 19, Step 25, Loss: 0.0027077023405581713\n","Epoch 19, Step 26, Loss: 0.002595300553366542\n","Epoch 19, Step 27, Loss: 0.0030790662858635187\n","Epoch 19, Step 28, Loss: 0.0029296977445483208\n","Epoch 19, Step 29, Loss: 0.003523565363138914\n","Epoch 19, Step 30, Loss: 0.0030502413865178823\n","Epoch 19, Step 31, Loss: 0.002198412548750639\n","Epoch 19, Step 32, Loss: 0.0023627334740012884\n","Epoch 19, Step 33, Loss: 0.0024834114592522383\n","Epoch 19, Step 34, Loss: 0.0025690849870443344\n","Epoch 19, Step 35, Loss: 0.003738557919859886\n","Epoch 19, Step 36, Loss: 0.002892897929996252\n","Epoch 19, Step 37, Loss: 0.0036848566960543394\n","Epoch 19, Step 38, Loss: 0.0033335883636027575\n","Epoch 19, Step 39, Loss: 0.0027928610797971487\n","Epoch 19, Step 40, Loss: 0.0029644917231053114\n","Epoch 19, Step 41, Loss: 0.00316085503436625\n","Epoch 19, Step 42, Loss: 0.00291610648855567\n","Epoch 19, Step 43, Loss: 0.0028672772459685802\n","Epoch 19, Step 44, Loss: 0.0031834100373089314\n","Epoch 19, Step 45, Loss: 0.00374909327365458\n","Epoch 19, Step 46, Loss: 0.00365327182225883\n","Epoch 19, Step 47, Loss: 0.00255793915130198\n","Epoch 19, Step 48, Loss: 0.0027750213630497456\n","Epoch 19, Step 49, Loss: 0.0030030133202672005\n","Epoch 19, Step 50, Loss: 0.003534681163728237\n","Epoch 19, Step 51, Loss: 0.003319259500131011\n","Epoch 19, Step 52, Loss: 0.0027842081617563963\n","Epoch 19, Step 53, Loss: 0.003230487694963813\n","Epoch 19, Step 54, Loss: 0.003530808025971055\n","Epoch 19, Step 55, Loss: 0.0036088447086513042\n","Epoch 19, Step 56, Loss: 0.0035776703152805567\n","Epoch 19, Step 57, Loss: 0.0034796870313584805\n","Epoch 19, Step 58, Loss: 0.0037615278270095587\n","Epoch 19, Step 59, Loss: 0.003016869304701686\n","Epoch 19, Step 60, Loss: 0.0028000089805573225\n","Train Metric MRRs: 0.6540863255620281\n","Train Metric MAPs: 0.48664862054537\n","Validation Metric MRRs: 0.5970122737258886\n","Validation Metric MAPs: 0.4045650161204452\n","Epoch 20, Step 1, Loss: 0.0030845727305859327\n","Epoch 20, Step 2, Loss: 0.0025369194336235523\n","Epoch 20, Step 3, Loss: 0.003119521774351597\n","Epoch 20, Step 4, Loss: 0.0030108140781521797\n","Epoch 20, Step 5, Loss: 0.0038864202797412872\n","Epoch 20, Step 6, Loss: 0.004118343349546194\n","Epoch 20, Step 7, Loss: 0.003073197789490223\n","Epoch 20, Step 8, Loss: 0.002446387894451618\n","Epoch 20, Step 9, Loss: 0.0027006492018699646\n","Epoch 20, Step 10, Loss: 0.003308061510324478\n","Epoch 20, Step 11, Loss: 0.0034033218398690224\n","Epoch 20, Step 12, Loss: 0.003476843936368823\n","Epoch 20, Step 13, Loss: 0.003280795644968748\n","Epoch 20, Step 14, Loss: 0.002785028889775276\n","Epoch 20, Step 15, Loss: 0.002694404684007168\n","Epoch 20, Step 16, Loss: 0.003335875691846013\n","Epoch 20, Step 17, Loss: 0.002782856347039342\n","Epoch 20, Step 18, Loss: 0.0032878699712455273\n","Epoch 20, Step 19, Loss: 0.00285600614733994\n","Epoch 20, Step 20, Loss: 0.002816565567627549\n","Epoch 20, Step 21, Loss: 0.0033977634739130735\n","Epoch 20, Step 22, Loss: 0.0027982648462057114\n","Epoch 20, Step 23, Loss: 0.0026278900913894176\n","Epoch 20, Step 24, Loss: 0.002801199210807681\n","Epoch 20, Step 25, Loss: 0.0026124021969735622\n","Epoch 20, Step 26, Loss: 0.002496939618140459\n","Epoch 20, Step 27, Loss: 0.002974942559376359\n","Epoch 20, Step 28, Loss: 0.0032523428089916706\n","Epoch 20, Step 29, Loss: 0.0038077100180089474\n","Epoch 20, Step 30, Loss: 0.0032122954726219177\n","Epoch 20, Step 31, Loss: 0.0024709536228328943\n","Epoch 20, Step 32, Loss: 0.0024863409344106913\n","Epoch 20, Step 33, Loss: 0.0024067535996437073\n","Epoch 20, Step 34, Loss: 0.002261185087263584\n","Epoch 20, Step 35, Loss: 0.002981145866215229\n","Epoch 20, Step 36, Loss: 0.0028466046787798405\n","Epoch 20, Step 37, Loss: 0.004065342713147402\n","Epoch 20, Step 38, Loss: 0.0036768701393157244\n","Epoch 20, Step 39, Loss: 0.0026643823366612196\n","Epoch 20, Step 40, Loss: 0.002634350210428238\n","Epoch 20, Step 41, Loss: 0.002811862388625741\n","Epoch 20, Step 42, Loss: 0.0025728163309395313\n","Epoch 20, Step 43, Loss: 0.002725874772295356\n","Epoch 20, Step 44, Loss: 0.0034308466129004955\n","Epoch 20, Step 45, Loss: 0.0036862276028841734\n","Epoch 20, Step 46, Loss: 0.0035636425018310547\n","Epoch 20, Step 47, Loss: 0.0027130788657814264\n","Epoch 20, Step 48, Loss: 0.0027331020683050156\n","Epoch 20, Step 49, Loss: 0.0024863719008862972\n","Epoch 20, Step 50, Loss: 0.002800431102514267\n","Epoch 20, Step 51, Loss: 0.0031686045695096254\n","Epoch 20, Step 52, Loss: 0.0034494984429329634\n","Epoch 20, Step 53, Loss: 0.0038157927338033915\n","Epoch 20, Step 54, Loss: 0.003098792629316449\n","Epoch 20, Step 55, Loss: 0.003395666368305683\n","Epoch 20, Step 56, Loss: 0.002687795553356409\n","Epoch 20, Step 57, Loss: 0.0031115019228309393\n","Epoch 20, Step 58, Loss: 0.0034539555199444294\n","Epoch 20, Step 59, Loss: 0.0034901383332908154\n","Epoch 20, Step 60, Loss: 0.0032290772069245577\n","Train Metric MRRs: 0.6593867307658423\n","Train Metric MAPs: 0.5096471301081893\n","Validation Metric MRRs: 0.5950106103562197\n","Validation Metric MAPs: 0.45475809065566564\n","Epoch 21, Step 1, Loss: 0.003540755482390523\n","Epoch 21, Step 2, Loss: 0.0029035520274192095\n","Epoch 21, Step 3, Loss: 0.0030383034609258175\n","Epoch 21, Step 4, Loss: 0.0025331072974950075\n","Epoch 21, Step 5, Loss: 0.0030409726314246655\n","Epoch 21, Step 6, Loss: 0.003612291067838669\n","Epoch 21, Step 7, Loss: 0.003470940515398979\n","Epoch 21, Step 8, Loss: 0.0032949650194495916\n","Epoch 21, Step 9, Loss: 0.002725925762206316\n","Epoch 21, Step 10, Loss: 0.0027606775984168053\n","Epoch 21, Step 11, Loss: 0.0026578912511467934\n","Epoch 21, Step 12, Loss: 0.0030412806663662195\n","Epoch 21, Step 13, Loss: 0.00294832163490355\n","Epoch 21, Step 14, Loss: 0.0031244431156665087\n","Epoch 21, Step 15, Loss: 0.003042543074116111\n","Epoch 21, Step 16, Loss: 0.003965682815760374\n","Epoch 21, Step 17, Loss: 0.0029092151671648026\n","Epoch 21, Step 18, Loss: 0.003003486664965749\n","Epoch 21, Step 19, Loss: 0.0024747385177761316\n","Epoch 21, Step 20, Loss: 0.0026348368264734745\n","Epoch 21, Step 21, Loss: 0.0028614355251193047\n","Epoch 21, Step 22, Loss: 0.002878794213756919\n","Epoch 21, Step 23, Loss: 0.0038825981318950653\n","Epoch 21, Step 24, Loss: 0.0036073741503059864\n","Epoch 21, Step 25, Loss: 0.0028168130666017532\n","Epoch 21, Step 26, Loss: 0.0022002721671015024\n","Epoch 21, Step 27, Loss: 0.002665073610842228\n","Epoch 21, Step 28, Loss: 0.002782935742288828\n","Epoch 21, Step 29, Loss: 0.003559169825166464\n","Epoch 21, Step 30, Loss: 0.00369562697596848\n","Epoch 21, Step 31, Loss: 0.003275256371125579\n","Epoch 21, Step 32, Loss: 0.003141415072605014\n","Epoch 21, Step 33, Loss: 0.00278270966373384\n","Epoch 21, Step 34, Loss: 0.002388397231698036\n","Epoch 21, Step 35, Loss: 0.0022762948647141457\n","Epoch 21, Step 36, Loss: 0.0024117131251841784\n","Epoch 21, Step 37, Loss: 0.0034144988749176264\n","Epoch 21, Step 38, Loss: 0.004310743883252144\n","Epoch 21, Step 39, Loss: 0.003491584211587906\n","Epoch 21, Step 40, Loss: 0.00337406387552619\n","Epoch 21, Step 41, Loss: 0.002966544358059764\n","Epoch 21, Step 42, Loss: 0.0027053297962993383\n","Epoch 21, Step 43, Loss: 0.0026286724023520947\n","Epoch 21, Step 44, Loss: 0.0034675069618970156\n","Epoch 21, Step 45, Loss: 0.003988254815340042\n","Epoch 21, Step 46, Loss: 0.003915613517165184\n","Epoch 21, Step 47, Loss: 0.0038168057799339294\n","Epoch 21, Step 48, Loss: 0.003921817988157272\n","Epoch 21, Step 49, Loss: 0.003024682402610779\n","Epoch 21, Step 50, Loss: 0.0026296109426766634\n","Epoch 21, Step 51, Loss: 0.0025283668655902147\n","Epoch 21, Step 52, Loss: 0.0029385623056441545\n","Epoch 21, Step 53, Loss: 0.0041473135352134705\n","Epoch 21, Step 54, Loss: 0.0035505276173353195\n","Epoch 21, Step 55, Loss: 0.004687933716922998\n","Epoch 21, Step 56, Loss: 0.002822595415636897\n","Epoch 21, Step 57, Loss: 0.0027659370098263025\n","Epoch 21, Step 58, Loss: 0.003046285128220916\n","Epoch 21, Step 59, Loss: 0.00363108585588634\n","Epoch 21, Step 60, Loss: 0.0036343575920909643\n","Train Metric MRRs: 0.6608301214417648\n","Train Metric MAPs: 0.5254533833159292\n","Validation Metric MRRs: 0.5745985723147962\n","Validation Metric MAPs: 0.4570463080888171\n","Epoch 22, Step 1, Loss: 0.0042954240925610065\n","Epoch 22, Step 2, Loss: 0.003998148255050182\n","Epoch 22, Step 3, Loss: 0.003969963174313307\n","Epoch 22, Step 4, Loss: 0.0029847174882888794\n","Epoch 22, Step 5, Loss: 0.0027723570819944143\n","Epoch 22, Step 6, Loss: 0.002888559829443693\n","Epoch 22, Step 7, Loss: 0.00289260339923203\n","Epoch 22, Step 8, Loss: 0.004030872136354446\n","Epoch 22, Step 9, Loss: 0.0045783380046486855\n","Epoch 22, Step 10, Loss: 0.0036000991240143776\n","Epoch 22, Step 11, Loss: 0.003188068512827158\n","Epoch 22, Step 12, Loss: 0.002856526058167219\n","Epoch 22, Step 13, Loss: 0.0024578578304499388\n","Epoch 22, Step 14, Loss: 0.003111226251348853\n","Epoch 22, Step 15, Loss: 0.0035670523066073656\n","Epoch 22, Step 16, Loss: 0.00445680832490325\n","Epoch 22, Step 17, Loss: 0.003806875552982092\n","Epoch 22, Step 18, Loss: 0.004033765289932489\n","Epoch 22, Step 19, Loss: 0.002942359307780862\n","Epoch 22, Step 20, Loss: 0.0028336811810731888\n","Epoch 22, Step 21, Loss: 0.002538443775847554\n","Epoch 22, Step 22, Loss: 0.0025388651993125677\n","Epoch 22, Step 23, Loss: 0.003208247246220708\n","Epoch 22, Step 24, Loss: 0.003505808301270008\n","Epoch 22, Step 25, Loss: 0.0036182671319693327\n","Epoch 22, Step 26, Loss: 0.0028215348720550537\n","Epoch 22, Step 27, Loss: 0.0029551172628998756\n","Epoch 22, Step 28, Loss: 0.002674635499715805\n","Epoch 22, Step 29, Loss: 0.002626199973747134\n","Epoch 22, Step 30, Loss: 0.003025982528924942\n","Epoch 22, Step 31, Loss: 0.0032163590658456087\n","Epoch 22, Step 32, Loss: 0.0034092965070158243\n","Epoch 22, Step 33, Loss: 0.0038338599260896444\n","Epoch 22, Step 34, Loss: 0.0036363748367875814\n","Epoch 22, Step 35, Loss: 0.0030294007156044245\n","Epoch 22, Step 36, Loss: 0.0025996752083301544\n","Epoch 22, Step 37, Loss: 0.0028019766323268414\n","Epoch 22, Step 38, Loss: 0.0029320204630494118\n","Epoch 22, Step 39, Loss: 0.0034152031876146793\n","Epoch 22, Step 40, Loss: 0.0036703618243336678\n","Epoch 22, Step 41, Loss: 0.003699256107211113\n","Epoch 22, Step 42, Loss: 0.0038875844329595566\n","Epoch 22, Step 43, Loss: 0.003048424143344164\n","Epoch 22, Step 44, Loss: 0.002902501029893756\n","Epoch 22, Step 45, Loss: 0.00362275168299675\n","Epoch 22, Step 46, Loss: 0.003340837312862277\n","Epoch 22, Step 47, Loss: 0.004549019038677216\n","Epoch 22, Step 48, Loss: 0.005991565063595772\n","Epoch 22, Step 49, Loss: 0.0047573428601026535\n","Epoch 22, Step 50, Loss: 0.003750087693333626\n","Epoch 22, Step 51, Loss: 0.0029426305554807186\n","Epoch 22, Step 52, Loss: 0.0025494846049696207\n","Epoch 22, Step 53, Loss: 0.002856453647837043\n","Epoch 22, Step 54, Loss: 0.0038447340484708548\n","Epoch 22, Step 55, Loss: 0.0056848046369850636\n","Epoch 22, Step 56, Loss: 0.004679311066865921\n","Epoch 22, Step 57, Loss: 0.003861882956698537\n","Epoch 22, Step 58, Loss: 0.0030939814168959856\n","Epoch 22, Step 59, Loss: 0.0030392443295568228\n","Epoch 22, Step 60, Loss: 0.0031811778899282217\n","Train Metric MRRs: 0.6516440961151548\n","Train Metric MAPs: 0.5300823578017858\n","Validation Metric MRRs: 0.5608485875428992\n","Validation Metric MAPs: 0.4394392326383104\n","Epoch 23, Step 1, Loss: 0.004595972131937742\n","Epoch 23, Step 2, Loss: 0.004166526719927788\n","Epoch 23, Step 3, Loss: 0.0045494441874325275\n","Epoch 23, Step 4, Loss: 0.004286118317395449\n","Epoch 23, Step 5, Loss: 0.0038332371041178703\n","Epoch 23, Step 6, Loss: 0.00453526945784688\n","Epoch 23, Step 7, Loss: 0.0034582281950861216\n","Epoch 23, Step 8, Loss: 0.002707359381020069\n","Epoch 23, Step 9, Loss: 0.003114268183708191\n","Epoch 23, Step 10, Loss: 0.004666349850594997\n","Epoch 23, Step 11, Loss: 0.003524916246533394\n","Epoch 23, Step 12, Loss: 0.004680684767663479\n","Epoch 23, Step 13, Loss: 0.0037100440822541714\n","Epoch 23, Step 14, Loss: 0.002863692818209529\n","Epoch 23, Step 15, Loss: 0.003347424790263176\n","Epoch 23, Step 16, Loss: 0.003192635253071785\n","Epoch 23, Step 17, Loss: 0.0040679010562598705\n","Epoch 23, Step 18, Loss: 0.005167466122657061\n","Epoch 23, Step 19, Loss: 0.003467678790912032\n","Epoch 23, Step 20, Loss: 0.00329960766248405\n","Epoch 23, Step 21, Loss: 0.004012377467006445\n","Epoch 23, Step 22, Loss: 0.003274694085121155\n","Epoch 23, Step 23, Loss: 0.0031840249430388212\n","Epoch 23, Step 24, Loss: 0.0027506814803928137\n","Epoch 23, Step 25, Loss: 0.002730544190853834\n","Epoch 23, Step 26, Loss: 0.002954161260277033\n","Epoch 23, Step 27, Loss: 0.004383416846394539\n","Epoch 23, Step 28, Loss: 0.004399551078677177\n","Epoch 23, Step 29, Loss: 0.004372462630271912\n","Epoch 23, Step 30, Loss: 0.002806727308779955\n","Epoch 23, Step 31, Loss: 0.002522786846384406\n","Epoch 23, Step 32, Loss: 0.0031055279541760683\n","Epoch 23, Step 33, Loss: 0.004280796740204096\n","Epoch 23, Step 34, Loss: 0.005531799979507923\n","Epoch 23, Step 35, Loss: 0.003951632883399725\n","Epoch 23, Step 36, Loss: 0.0034800569992512465\n","Epoch 23, Step 37, Loss: 0.004234592895954847\n","Epoch 23, Step 38, Loss: 0.0033022065181285143\n","Epoch 23, Step 39, Loss: 0.0032791150733828545\n","Epoch 23, Step 40, Loss: 0.0032501062378287315\n","Epoch 23, Step 41, Loss: 0.0037633362226188183\n","Epoch 23, Step 42, Loss: 0.004579912405461073\n","Epoch 23, Step 43, Loss: 0.003930240403860807\n","Epoch 23, Step 44, Loss: 0.003552537178620696\n","Epoch 23, Step 45, Loss: 0.0032398903276771307\n","Epoch 23, Step 46, Loss: 0.0035016024485230446\n","Epoch 23, Step 47, Loss: 0.004869865253567696\n","Epoch 23, Step 48, Loss: 0.005766377318650484\n","Epoch 23, Step 49, Loss: 0.00619104690849781\n","Epoch 23, Step 50, Loss: 0.005184084177017212\n","Epoch 23, Step 51, Loss: 0.004073478747159243\n","Epoch 23, Step 52, Loss: 0.0031912513077259064\n","Epoch 23, Step 53, Loss: 0.003367904108017683\n","Epoch 23, Step 54, Loss: 0.004125863779336214\n","Epoch 23, Step 55, Loss: 0.0036834802012890577\n","Epoch 23, Step 56, Loss: 0.004332192707806826\n","Epoch 23, Step 57, Loss: 0.004320868290960789\n","Epoch 23, Step 58, Loss: 0.007674165070056915\n","Epoch 23, Step 59, Loss: 0.004009336698800325\n","Epoch 23, Step 60, Loss: 0.0027682629879564047\n","Train Metric MRRs: 0.6363222084739081\n","Train Metric MAPs: 0.5253451756322779\n","Validation Metric MRRs: 0.5513330326598197\n","Validation Metric MAPs: 0.4080572819756047\n","Epoch 24, Step 1, Loss: 0.0037482951302081347\n","Epoch 24, Step 2, Loss: 0.003706610994413495\n","Epoch 24, Step 3, Loss: 0.004209532402455807\n","Epoch 24, Step 4, Loss: 0.006799005903303623\n","Epoch 24, Step 5, Loss: 0.005209273658692837\n","Epoch 24, Step 6, Loss: 0.00672781839966774\n","Epoch 24, Step 7, Loss: 0.005198792554438114\n","Epoch 24, Step 8, Loss: 0.0039748819544911385\n","Epoch 24, Step 9, Loss: 0.003192446194589138\n","Epoch 24, Step 10, Loss: 0.003647378645837307\n","Epoch 24, Step 11, Loss: 0.003407668089494109\n","Epoch 24, Step 12, Loss: 0.0053665852174162865\n","Epoch 24, Step 13, Loss: 0.005046410020440817\n","Epoch 24, Step 14, Loss: 0.004695379175245762\n","Epoch 24, Step 15, Loss: 0.005457791965454817\n","Epoch 24, Step 16, Loss: 0.0036767981946468353\n","Epoch 24, Step 17, Loss: 0.0037435281556099653\n","Epoch 24, Step 18, Loss: 0.005589744076132774\n","Epoch 24, Step 19, Loss: 0.00375455804169178\n","Epoch 24, Step 20, Loss: 0.0034866826608777046\n","Epoch 24, Step 21, Loss: 0.0058798338286578655\n","Epoch 24, Step 22, Loss: 0.004914936143904924\n","Epoch 24, Step 23, Loss: 0.004355855751782656\n","Epoch 24, Step 24, Loss: 0.0038752062246203423\n","Epoch 24, Step 25, Loss: 0.0033326048869639635\n","Epoch 24, Step 26, Loss: 0.0029150305781513453\n","Epoch 24, Step 27, Loss: 0.0033374479971826077\n","Epoch 24, Step 28, Loss: 0.0046231853775680065\n","Epoch 24, Step 29, Loss: 0.005709838587790728\n","Epoch 24, Step 30, Loss: 0.004459527786821127\n","Epoch 24, Step 31, Loss: 0.0036276509054005146\n","Epoch 24, Step 32, Loss: 0.0038833445869386196\n","Epoch 24, Step 33, Loss: 0.003118981374427676\n","Epoch 24, Step 34, Loss: 0.0044789486564695835\n","Epoch 24, Step 35, Loss: 0.004327733535319567\n","Epoch 24, Step 36, Loss: 0.0037480401806533337\n","Epoch 24, Step 37, Loss: 0.00634677754715085\n","Epoch 24, Step 38, Loss: 0.004550142679363489\n","Epoch 24, Step 39, Loss: 0.003889175131917\n","Epoch 24, Step 40, Loss: 0.0032441187649965286\n","Epoch 24, Step 41, Loss: 0.0033405472058802843\n","Epoch 24, Step 42, Loss: 0.004389019217342138\n","Epoch 24, Step 43, Loss: 0.004762500524520874\n","Epoch 24, Step 44, Loss: 0.00537961907684803\n","Epoch 24, Step 45, Loss: 0.004969929810613394\n","Epoch 24, Step 46, Loss: 0.004502995405346155\n","Epoch 24, Step 47, Loss: 0.0033000027760863304\n","Epoch 24, Step 48, Loss: 0.0036547849886119366\n","Epoch 24, Step 49, Loss: 0.00675768218934536\n","Epoch 24, Step 50, Loss: 0.006845998112112284\n","Epoch 24, Step 51, Loss: 0.00946570374071598\n","Epoch 24, Step 52, Loss: 0.006586926523596048\n","Epoch 24, Step 53, Loss: 0.006215522065758705\n","Epoch 24, Step 54, Loss: 0.004339765291661024\n","Epoch 24, Step 55, Loss: 0.003690944751724601\n","Epoch 24, Step 56, Loss: 0.004889394156634808\n","Epoch 24, Step 57, Loss: 0.0052268593572080135\n","Epoch 24, Step 58, Loss: 0.007361707743257284\n","Epoch 24, Step 59, Loss: 0.012330274097621441\n","Epoch 24, Step 60, Loss: 0.004665383603423834\n","Train Metric MRRs: 0.6103318098024261\n","Train Metric MAPs: 0.4866702181773657\n","Validation Metric MRRs: 0.5239227221495725\n","Validation Metric MAPs: 0.37777960626281265\n","Epoch 25, Step 1, Loss: 0.004740341566503048\n","Epoch 25, Step 2, Loss: 0.0046618604101240635\n","Epoch 25, Step 3, Loss: 0.005091000813990831\n","Epoch 25, Step 4, Loss: 0.006085830274969339\n","Epoch 25, Step 5, Loss: 0.006831988226622343\n","Epoch 25, Step 6, Loss: 0.010900466702878475\n","Epoch 25, Step 7, Loss: 0.007515746168792248\n","Epoch 25, Step 8, Loss: 0.006833121180534363\n","Epoch 25, Step 9, Loss: 0.005071471445262432\n","Epoch 25, Step 10, Loss: 0.006102468352764845\n","Epoch 25, Step 11, Loss: 0.006467584520578384\n","Epoch 25, Step 12, Loss: 0.007363742217421532\n","Epoch 25, Step 13, Loss: 0.0046095699071884155\n","Epoch 25, Step 14, Loss: 0.006942645646631718\n","Epoch 25, Step 15, Loss: 0.007774421479552984\n","Epoch 25, Step 16, Loss: 0.007709309458732605\n","Epoch 25, Step 17, Loss: 0.006095715798437595\n","Epoch 25, Step 18, Loss: 0.004298084415495396\n","Epoch 25, Step 19, Loss: 0.00370427081361413\n","Epoch 25, Step 20, Loss: 0.004504858981817961\n","Epoch 25, Step 21, Loss: 0.006858248729258776\n","Epoch 25, Step 22, Loss: 0.009779279120266438\n","Epoch 25, Step 23, Loss: 0.006786657031625509\n","Epoch 25, Step 24, Loss: 0.008042302913963795\n","Epoch 25, Step 25, Loss: 0.00543158920481801\n","Epoch 25, Step 26, Loss: 0.004292723257094622\n","Epoch 25, Step 27, Loss: 0.004399541765451431\n","Epoch 25, Step 28, Loss: 0.005970077123492956\n","Epoch 25, Step 29, Loss: 0.005624090321362019\n","Epoch 25, Step 30, Loss: 0.0059152948670089245\n","Epoch 25, Step 31, Loss: 0.005888709798455238\n","Epoch 25, Step 32, Loss: 0.009390557184815407\n","Epoch 25, Step 33, Loss: 0.004452473483979702\n","Epoch 25, Step 34, Loss: 0.004714093171060085\n","Epoch 25, Step 35, Loss: 0.004581019748002291\n","Epoch 25, Step 36, Loss: 0.0049649509601294994\n","Epoch 25, Step 37, Loss: 0.0056981551460921764\n","Epoch 25, Step 38, Loss: 0.006968736182898283\n","Epoch 25, Step 39, Loss: 0.005428091622889042\n","Epoch 25, Step 40, Loss: 0.005362008698284626\n","Epoch 25, Step 41, Loss: 0.0046628681011497974\n","Epoch 25, Step 42, Loss: 0.0039177448488771915\n","Epoch 25, Step 43, Loss: 0.004333553370088339\n","Epoch 25, Step 44, Loss: 0.004383524879813194\n","Epoch 25, Step 45, Loss: 0.0048818038776516914\n","Epoch 25, Step 46, Loss: 0.005107284057885408\n","Epoch 25, Step 47, Loss: 0.0061371810734272\n","Epoch 25, Step 48, Loss: 0.004520534537732601\n","Epoch 25, Step 49, Loss: 0.0035216696560382843\n","Epoch 25, Step 50, Loss: 0.003687460208311677\n","Epoch 25, Step 51, Loss: 0.0070373681373894215\n","Epoch 25, Step 52, Loss: 0.006703470833599567\n","Epoch 25, Step 53, Loss: 0.011079221963882446\n","Epoch 25, Step 54, Loss: 0.006256881635636091\n","Epoch 25, Step 55, Loss: 0.00808416772633791\n","Epoch 25, Step 56, Loss: 0.004537077620625496\n","Epoch 25, Step 57, Loss: 0.003489975817501545\n","Epoch 25, Step 58, Loss: 0.004898303188383579\n","Epoch 25, Step 59, Loss: 0.006981801241636276\n","Epoch 25, Step 60, Loss: 0.01318354345858097\n","Train Metric MRRs: 0.5671325441935975\n","Train Metric MAPs: 0.4320379487042504\n","Validation Metric MRRs: 0.43588069805881224\n","Validation Metric MAPs: 0.3110867147406026\n","Epoch 26, Step 1, Loss: 0.009987276047468185\n","Epoch 26, Step 2, Loss: 0.006551441736519337\n","Epoch 26, Step 3, Loss: 0.004825938027352095\n","Epoch 26, Step 4, Loss: 0.005073003936558962\n","Epoch 26, Step 5, Loss: 0.006633499637246132\n","Epoch 26, Step 6, Loss: 0.007071420084685087\n","Epoch 26, Step 7, Loss: 0.007002757862210274\n","Epoch 26, Step 8, Loss: 0.014244478195905685\n","Epoch 26, Step 9, Loss: 0.008724212646484375\n","Epoch 26, Step 10, Loss: 0.0178790632635355\n","Epoch 26, Step 11, Loss: 0.012870128266513348\n","Epoch 26, Step 12, Loss: 0.005570542067289352\n","Epoch 26, Step 13, Loss: 0.00482587656006217\n","Epoch 26, Step 14, Loss: 0.004377278033643961\n","Epoch 26, Step 15, Loss: 0.005280535668134689\n","Epoch 26, Step 16, Loss: 0.008362526074051857\n","Epoch 26, Step 17, Loss: 0.023834502324461937\n","Epoch 26, Step 18, Loss: 0.015242410823702812\n","Epoch 26, Step 19, Loss: 0.006313379853963852\n","Epoch 26, Step 20, Loss: 0.005068658851087093\n","Epoch 26, Step 21, Loss: 0.00602849991992116\n","Epoch 26, Step 22, Loss: 0.009260493330657482\n","Epoch 26, Step 23, Loss: 0.009533107280731201\n","Epoch 26, Step 24, Loss: 0.018656868487596512\n","Epoch 26, Step 25, Loss: 0.013066263869404793\n","Epoch 26, Step 26, Loss: 0.010015654377639294\n","Epoch 26, Step 27, Loss: 0.008718113414943218\n","Epoch 26, Step 28, Loss: 0.012868594378232956\n","Epoch 26, Step 29, Loss: 0.010496850125491619\n","Epoch 26, Step 30, Loss: 0.004603536333888769\n","Epoch 26, Step 31, Loss: 0.0061145955696702\n","Epoch 26, Step 32, Loss: 0.007946441881358624\n","Epoch 26, Step 33, Loss: 0.011623488739132881\n","Epoch 26, Step 34, Loss: 0.01781519129872322\n","Epoch 26, Step 35, Loss: 0.007527950219810009\n","Epoch 26, Step 36, Loss: 0.005987314507365227\n","Epoch 26, Step 37, Loss: 0.0064742714166641235\n","Epoch 26, Step 38, Loss: 0.005462199449539185\n","Epoch 26, Step 39, Loss: 0.008500996977090836\n","Epoch 26, Step 40, Loss: 0.009541736915707588\n","Epoch 26, Step 41, Loss: 0.010014241561293602\n","Epoch 26, Step 42, Loss: 0.009099389426410198\n","Epoch 26, Step 43, Loss: 0.007664379198104143\n","Epoch 26, Step 44, Loss: 0.006270354613661766\n","Epoch 26, Step 45, Loss: 0.004780835937708616\n","Epoch 26, Step 46, Loss: 0.005417063366621733\n","Epoch 26, Step 47, Loss: 0.008367225527763367\n","Epoch 26, Step 48, Loss: 0.008304169401526451\n","Epoch 26, Step 49, Loss: 0.01550398301333189\n","Epoch 26, Step 50, Loss: 0.004528932273387909\n","Epoch 26, Step 51, Loss: 0.005212133750319481\n","Epoch 26, Step 52, Loss: 0.006225755903869867\n","Epoch 26, Step 53, Loss: 0.009750718250870705\n","Epoch 26, Step 54, Loss: 0.009931854903697968\n","Epoch 26, Step 55, Loss: 0.01024183351546526\n","Epoch 26, Step 56, Loss: 0.00820532813668251\n","Epoch 26, Step 57, Loss: 0.007657193578779697\n","Epoch 26, Step 58, Loss: 0.007927869446575642\n","Epoch 26, Step 59, Loss: 0.008487913757562637\n","Epoch 26, Step 60, Loss: 0.005926833488047123\n","Train Metric MRRs: 0.5094468725158059\n","Train Metric MAPs: 0.3893578111031904\n","Validation Metric MRRs: 0.36051988169812116\n","Validation Metric MAPs: 0.23773667570229526\n","Epoch 27, Step 1, Loss: 0.006561070214956999\n","Epoch 27, Step 2, Loss: 0.0077028474770486355\n","Epoch 27, Step 3, Loss: 0.012377263978123665\n","Epoch 27, Step 4, Loss: 0.00985949207097292\n","Epoch 27, Step 5, Loss: 0.006758946925401688\n","Epoch 27, Step 6, Loss: 0.009598890319466591\n","Epoch 27, Step 7, Loss: 0.004841433372348547\n","Epoch 27, Step 8, Loss: 0.00941397249698639\n","Epoch 27, Step 9, Loss: 0.009647201746702194\n","Epoch 27, Step 10, Loss: 0.008181439712643623\n","Epoch 27, Step 11, Loss: 0.007934809662401676\n","Epoch 27, Step 12, Loss: 0.009302441962063313\n","Epoch 27, Step 13, Loss: 0.007322987075895071\n","Epoch 27, Step 14, Loss: 0.008074511773884296\n","Epoch 27, Step 15, Loss: 0.008162373676896095\n","Epoch 27, Step 16, Loss: 0.0059023345820605755\n","Epoch 27, Step 17, Loss: 0.004316036123782396\n","Epoch 27, Step 18, Loss: 0.007063611876219511\n","Epoch 27, Step 19, Loss: 0.007813888601958752\n","Epoch 27, Step 20, Loss: 0.00854124128818512\n","Epoch 27, Step 21, Loss: 0.01611156202852726\n","Epoch 27, Step 22, Loss: 0.00846441276371479\n","Epoch 27, Step 23, Loss: 0.006408166605979204\n","Epoch 27, Step 24, Loss: 0.005260733421891928\n","Epoch 27, Step 25, Loss: 0.004433734808117151\n","Epoch 27, Step 26, Loss: 0.0062299855053424835\n","Epoch 27, Step 27, Loss: 0.011643181554973125\n","Epoch 27, Step 28, Loss: 0.014732460491359234\n","Epoch 27, Step 29, Loss: 0.0164654403924942\n","Epoch 27, Step 30, Loss: 0.010073743760585785\n","Epoch 27, Step 31, Loss: 0.008235789835453033\n","Epoch 27, Step 32, Loss: 0.009869663044810295\n","Epoch 27, Step 33, Loss: 0.00745216803625226\n","Epoch 27, Step 34, Loss: 0.006471148692071438\n","Epoch 27, Step 35, Loss: 0.005608114879578352\n","Epoch 27, Step 36, Loss: 0.006804031785577536\n","Epoch 27, Step 37, Loss: 0.011804376728832722\n","Epoch 27, Step 38, Loss: 0.013843393884599209\n","Epoch 27, Step 39, Loss: 0.006948720198124647\n","Epoch 27, Step 40, Loss: 0.00588243966922164\n","Epoch 27, Step 41, Loss: 0.00874192826449871\n","Epoch 27, Step 42, Loss: 0.00492027448490262\n","Epoch 27, Step 43, Loss: 0.005755533929914236\n","Epoch 27, Step 44, Loss: 0.0056282998993992805\n","Epoch 27, Step 45, Loss: 0.006315778009593487\n","Epoch 27, Step 46, Loss: 0.007588310167193413\n","Epoch 27, Step 47, Loss: 0.004623416345566511\n","Epoch 27, Step 48, Loss: 0.0043091680854558945\n","Epoch 27, Step 49, Loss: 0.004473675508052111\n","Epoch 27, Step 50, Loss: 0.005516816396266222\n","Epoch 27, Step 51, Loss: 0.005282634869217873\n","Epoch 27, Step 52, Loss: 0.005144251510500908\n","Epoch 27, Step 53, Loss: 0.004340885207056999\n","Epoch 27, Step 54, Loss: 0.0048931860364973545\n","Epoch 27, Step 55, Loss: 0.0041465843096375465\n","Epoch 27, Step 56, Loss: 0.004497138783335686\n","Epoch 27, Step 57, Loss: 0.005202778615057468\n","Epoch 27, Step 58, Loss: 0.0045529929921031\n","Epoch 27, Step 59, Loss: 0.004801186732947826\n","Epoch 27, Step 60, Loss: 0.005177284590899944\n","Train Metric MRRs: 0.5050333416110966\n","Train Metric MAPs: 0.36312384166261313\n","Validation Metric MRRs: 0.5265818824539951\n","Validation Metric MAPs: 0.312588482373059\n","Epoch 28, Step 1, Loss: 0.00532517721876502\n","Epoch 28, Step 2, Loss: 0.004027230199426413\n","Epoch 28, Step 3, Loss: 0.004188647493720055\n","Epoch 28, Step 4, Loss: 0.003970375284552574\n","Epoch 28, Step 5, Loss: 0.004244619514793158\n","Epoch 28, Step 6, Loss: 0.010539064183831215\n","Epoch 28, Step 7, Loss: 0.008404497057199478\n","Epoch 28, Step 8, Loss: 0.004901230335235596\n","Epoch 28, Step 9, Loss: 0.0036416901275515556\n","Epoch 28, Step 10, Loss: 0.0049412548542022705\n","Epoch 28, Step 11, Loss: 0.0060788728296756744\n","Epoch 28, Step 12, Loss: 0.006209015846252441\n","Epoch 28, Step 13, Loss: 0.005755840800702572\n","Epoch 28, Step 14, Loss: 0.007261125836521387\n","Epoch 28, Step 15, Loss: 0.006620187312364578\n","Epoch 28, Step 16, Loss: 0.010484983213245869\n","Epoch 28, Step 17, Loss: 0.006462575402110815\n","Epoch 28, Step 18, Loss: 0.004376164637506008\n","Epoch 28, Step 19, Loss: 0.00425395043566823\n","Epoch 28, Step 20, Loss: 0.0034376292023807764\n","Epoch 28, Step 21, Loss: 0.004612286575138569\n","Epoch 28, Step 22, Loss: 0.008282829076051712\n","Epoch 28, Step 23, Loss: 0.009959975257515907\n","Epoch 28, Step 24, Loss: 0.009814108721911907\n","Epoch 28, Step 25, Loss: 0.006532652303576469\n","Epoch 28, Step 26, Loss: 0.0038051519077271223\n","Epoch 28, Step 27, Loss: 0.0038454041350632906\n","Epoch 28, Step 28, Loss: 0.004770943429321051\n","Epoch 28, Step 29, Loss: 0.00855773314833641\n","Epoch 28, Step 30, Loss: 0.007629540748894215\n","Epoch 28, Step 31, Loss: 0.007002337370067835\n","Epoch 28, Step 32, Loss: 0.011564932763576508\n","Epoch 28, Step 33, Loss: 0.006510721519589424\n","Epoch 28, Step 34, Loss: 0.007400290574878454\n","Epoch 28, Step 35, Loss: 0.006350514944642782\n","Epoch 28, Step 36, Loss: 0.005388448480516672\n","Epoch 28, Step 37, Loss: 0.004577546380460262\n","Epoch 28, Step 38, Loss: 0.003922347445040941\n","Epoch 28, Step 39, Loss: 0.004030751995742321\n","Epoch 28, Step 40, Loss: 0.005362067371606827\n","Epoch 28, Step 41, Loss: 0.007422661408782005\n","Epoch 28, Step 42, Loss: 0.005079366732388735\n","Epoch 28, Step 43, Loss: 0.005106374155730009\n","Epoch 28, Step 44, Loss: 0.004922064486891031\n","Epoch 28, Step 45, Loss: 0.0037766452878713608\n","Epoch 28, Step 46, Loss: 0.0038561017718166113\n","Epoch 28, Step 47, Loss: 0.005524990614503622\n","Epoch 28, Step 48, Loss: 0.005068805068731308\n","Epoch 28, Step 49, Loss: 0.00650151027366519\n","Epoch 28, Step 50, Loss: 0.004011817742139101\n","Epoch 28, Step 51, Loss: 0.004619792569428682\n","Epoch 28, Step 52, Loss: 0.004019439686089754\n","Epoch 28, Step 53, Loss: 0.0038160067051649094\n","Epoch 28, Step 54, Loss: 0.00559953274205327\n","Epoch 28, Step 55, Loss: 0.0038762742187827826\n","Epoch 28, Step 56, Loss: 0.00322832353413105\n","Epoch 28, Step 57, Loss: 0.003592951223254204\n","Epoch 28, Step 58, Loss: 0.0032480177469551563\n","Epoch 28, Step 59, Loss: 0.0027273837476968765\n","Epoch 28, Step 60, Loss: 0.0032971601467579603\n","Train Metric MRRs: 0.563028362434959\n","Train Metric MAPs: 0.43291432502952554\n","Validation Metric MRRs: 0.5621145289455168\n","Validation Metric MAPs: 0.40088605656752985\n","Epoch 29, Step 1, Loss: 0.003192921169102192\n","Epoch 29, Step 2, Loss: 0.0033986817579716444\n","Epoch 29, Step 3, Loss: 0.0031225793063640594\n","Epoch 29, Step 4, Loss: 0.0030643336940556765\n","Epoch 29, Step 5, Loss: 0.0030119840521365404\n","Epoch 29, Step 6, Loss: 0.002969213528558612\n","Epoch 29, Step 7, Loss: 0.0031746504828333855\n","Epoch 29, Step 8, Loss: 0.0034786940086632967\n","Epoch 29, Step 9, Loss: 0.003163238288834691\n","Epoch 29, Step 10, Loss: 0.003331111278384924\n","Epoch 29, Step 11, Loss: 0.00747179938480258\n","Epoch 29, Step 12, Loss: 0.0032101788092404604\n","Epoch 29, Step 13, Loss: 0.0028886159416288137\n","Epoch 29, Step 14, Loss: 0.0033922812435775995\n","Epoch 29, Step 15, Loss: 0.0037616330664604902\n","Epoch 29, Step 16, Loss: 0.0035826549865305424\n","Epoch 29, Step 17, Loss: 0.005117255728691816\n","Epoch 29, Step 18, Loss: 0.003956759348511696\n","Epoch 29, Step 19, Loss: 0.0037922717165201902\n","Epoch 29, Step 20, Loss: 0.0035121655091643333\n","Epoch 29, Step 21, Loss: 0.0027174961287528276\n","Epoch 29, Step 22, Loss: 0.0031235155183821917\n","Epoch 29, Step 23, Loss: 0.003186015645042062\n","Epoch 29, Step 24, Loss: 0.004406286403536797\n","Epoch 29, Step 25, Loss: 0.003943589050322771\n","Epoch 29, Step 26, Loss: 0.003982652444392443\n","Epoch 29, Step 27, Loss: 0.00422391202300787\n","Epoch 29, Step 28, Loss: 0.00382817629724741\n","Epoch 29, Step 29, Loss: 0.003327911254018545\n","Epoch 29, Step 30, Loss: 0.003038640832528472\n","Epoch 29, Step 31, Loss: 0.0029215863905847073\n","Epoch 29, Step 32, Loss: 0.0038671016227453947\n","Epoch 29, Step 33, Loss: 0.004337773192673922\n","Epoch 29, Step 34, Loss: 0.004439593758434057\n","Epoch 29, Step 35, Loss: 0.004275524523109198\n","Epoch 29, Step 36, Loss: 0.003191658528521657\n","Epoch 29, Step 37, Loss: 0.0050975242629647255\n","Epoch 29, Step 38, Loss: 0.0028861933387815952\n","Epoch 29, Step 39, Loss: 0.002674649003893137\n","Epoch 29, Step 40, Loss: 0.002692179987207055\n","Epoch 29, Step 41, Loss: 0.0026498567312955856\n","Epoch 29, Step 42, Loss: 0.0027292470913380384\n","Epoch 29, Step 43, Loss: 0.002857593121007085\n","Epoch 29, Step 44, Loss: 0.003576613264158368\n","Epoch 29, Step 45, Loss: 0.004327952861785889\n","Epoch 29, Step 46, Loss: 0.003933351952582598\n","Epoch 29, Step 47, Loss: 0.0031832274980843067\n","Epoch 29, Step 48, Loss: 0.00360554619692266\n","Epoch 29, Step 49, Loss: 0.003623681841418147\n","Epoch 29, Step 50, Loss: 0.00286746839992702\n","Epoch 29, Step 51, Loss: 0.0029866104014217854\n","Epoch 29, Step 52, Loss: 0.0031294834334403276\n","Epoch 29, Step 53, Loss: 0.0035990688484162092\n","Epoch 29, Step 54, Loss: 0.0031559281051158905\n","Epoch 29, Step 55, Loss: 0.00473965285345912\n","Epoch 29, Step 56, Loss: 0.003701713401824236\n","Epoch 29, Step 57, Loss: 0.003380265785381198\n","Epoch 29, Step 58, Loss: 0.0030760818626731634\n","Epoch 29, Step 59, Loss: 0.002610502066090703\n","Epoch 29, Step 60, Loss: 0.002817438915371895\n","Train Metric MRRs: 0.6241880282484539\n","Train Metric MAPs: 0.4837206908141856\n","Validation Metric MRRs: 0.5649354546194438\n","Validation Metric MAPs: 0.41459228580066987\n","Early stopping triggered!\n"]}],"source":["trainer.train(config['num_epochs'])"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"brUW-S9GWkh3","executionInfo":{"status":"ok","timestamp":1692208358637,"user_tz":-60,"elapsed":121068,"user":{"displayName":"Quandary zhang","userId":"14333747180371872055"}},"outputId":"38b20c8f-33dc-43b6-b441-9086bb35a6ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Metric MRRs: 0.5970122737258886\n","Validation Metric MAPs: 0.4045650161204452\n","Test Metric MRRs: 0.5670882021358256\n","Test Metric MAPs: 0.3903435745433434\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.5670882021358256, 0.3903435745433434)"]},"metadata":{},"execution_count":6}],"source":["trainer.restore_best_checkpoint()\n","trainer.validate('Validation')\n","trainer.validate('Test')"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1pkFR0zADJ1_8XnOfD8tBnpcIvUHFEqUM","timestamp":1691764013031},{"file_id":"1pBEj0pxNgq6XrBwUPufSzhud3nzdxhxq","timestamp":1691741843673}],"authorship_tag":"ABX9TyPu+vvhZq9IkLgs3/CUXyz1"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}