{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7IVQVEDWEWB",
        "outputId": "a231e954-2bd0-4d1e-e550-9f55eb62f1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/mlds\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/mlds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import data_bitcoin as btc\n",
        "import tasker_link_prediction as t_lp\n",
        "from splitter import splitter\n",
        "from models import TwoStream_GCN\n",
        "from trainer import Trainer\n",
        "import yaml\n",
        "import os"
      ],
      "metadata": {
        "id": "ElDsah6AWVwk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep = False\n",
        "\n",
        "with open('config/config_BTC_OTC.yaml', 'r') as file:\n",
        "    config = yaml.safe_load(file)\n",
        "\n",
        "model_path = config['model_path']\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    os.mkdir(model_path)\n",
        "    os.mkdir(model_path + \"best_checkpoints/\")\n",
        "    os.mkdir(model_path + \"latest_checkpoints/\")\n",
        "\n",
        "with open(model_path + 'config.yaml', 'w') as file:\n",
        "    yaml.dump(config, file)\n",
        "\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doo1WcuyWZAC",
        "outputId": "7459a5bd-1f64-4aec-af65-81fcc6115a9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'adam_config': {'adam_beta_1': 0.9,\n",
              "  'adam_beta_2': 0.999,\n",
              "  'adam_epsilon': 1e-07,\n",
              "  'adam_learning_rate': 0.005},\n",
              " 'model_path': 'models/BTC_OTC_TwoStream/',\n",
              " 'classifier_hidden_size': 16,\n",
              " 'ffn_fusion_size': 8,\n",
              " 'ffn_hidden_size': 8,\n",
              " 'gcn_fusion_size': 16,\n",
              " 'spatial_hidden_size': 16,\n",
              " 'spatial_input_dim': 64,\n",
              " 'temporal_hidden_size': 16,\n",
              " 'temporal_input_dim': 80,\n",
              " 'num_epochs': 1000,\n",
              " 'patience': 10,\n",
              " 'major_threshold': None,\n",
              " 'train_proportion': 0.7,\n",
              " 'dev_proportion': 0.1,\n",
              " 'data_path': 'data/BTC-OTC/',\n",
              " 'prep_data_path': 'prep_data/BTC_OTC_neg/'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = btc.Bitcoin_Dataset(config['data_path'])\n",
        "tasker = t_lp.Link_Pred_Tasker(data, path=config['prep_data_path'], prep=prep,\n",
        "                               embs_dim=config['spatial_input_dim'], temp_dim=int(config['temporal_input_dim']/10),\n",
        "                               major_threshold=config['major_threshold'], smart_neg_sampling=True)\n",
        "\n",
        "splitter_as = splitter(tasker, train_proportion = config['train_proportion'], dev_proportion = config['dev_proportion'])\n",
        "\n",
        "model= TwoStream_GCN(spatial_input_dim=config['spatial_input_dim'],\n",
        "                 temporal_input_dim=config['temporal_input_dim'],\n",
        "                 spatial_hidden_size=config['spatial_hidden_size'],\n",
        "                 temporal_hidden_size=config['temporal_hidden_size'],\n",
        "                 classifier_hidden_size=config['classifier_hidden_size'],\n",
        "                 gcn_fusion_size=config['gcn_fusion_size'],\n",
        "                 ffn_fusion_size=config['ffn_fusion_size'],\n",
        "                 ffn_hiden_size=config['ffn_hidden_size'])\n",
        "\n",
        "trainer = Trainer(model=model, splitter=splitter_as, model_path=model_path, adam_config=config['adam_config'], patience=config['patience'])\n",
        "print(trainer.patience)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unCQbLC8Wb0S",
        "outputId": "3590b701-2264-47be-8381-cdf88ef7adf9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset splits sizes:  train 86 dev 14 test 28\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(config['num_epochs'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuhZRFQVognD",
        "outputId": "c216754f-5965-4974-a50c-8ad656e13c1f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7d6dc2341630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7d6dc2341630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Step 1, Loss: 0.19549354910850525\n",
            "Epoch 1, Step 2, Loss: 0.15056417882442474\n",
            "Epoch 1, Step 3, Loss: 0.08804231882095337\n",
            "Epoch 1, Step 4, Loss: 0.07055313885211945\n",
            "Epoch 1, Step 5, Loss: 0.04142430052161217\n",
            "Epoch 1, Step 6, Loss: 0.030554957687854767\n",
            "Epoch 1, Step 7, Loss: 0.023494461551308632\n",
            "Epoch 1, Step 8, Loss: 0.026641974225640297\n",
            "Epoch 1, Step 9, Loss: 0.033453844487667084\n",
            "Epoch 1, Step 10, Loss: 0.03025112673640251\n",
            "Epoch 1, Step 11, Loss: 0.02547905594110489\n",
            "Epoch 1, Step 12, Loss: 0.026202065870165825\n",
            "Epoch 1, Step 13, Loss: 0.0225321426987648\n",
            "Epoch 1, Step 14, Loss: 0.02101152203977108\n",
            "Epoch 1, Step 15, Loss: 0.017704090103507042\n",
            "Epoch 1, Step 16, Loss: 0.020572448149323463\n",
            "Epoch 1, Step 17, Loss: 0.03221656754612923\n",
            "Epoch 1, Step 18, Loss: 0.03895222023129463\n",
            "Epoch 1, Step 19, Loss: 0.041033729910850525\n",
            "Epoch 1, Step 20, Loss: 0.03700125217437744\n",
            "Epoch 1, Step 21, Loss: 0.03457086533308029\n",
            "Epoch 1, Step 22, Loss: 0.03227464482188225\n",
            "Epoch 1, Step 23, Loss: 0.027000633999705315\n",
            "Epoch 1, Step 24, Loss: 0.032955147325992584\n",
            "Epoch 1, Step 25, Loss: 0.033569879829883575\n",
            "Epoch 1, Step 26, Loss: 0.03274047002196312\n",
            "Epoch 1, Step 27, Loss: 0.021907880902290344\n",
            "Epoch 1, Step 28, Loss: 0.020072462037205696\n",
            "Epoch 1, Step 29, Loss: 0.01877630315721035\n",
            "Epoch 1, Step 30, Loss: 0.01898990385234356\n",
            "Epoch 1, Step 31, Loss: 0.022060072049498558\n",
            "Epoch 1, Step 32, Loss: 0.017363207414746284\n",
            "Epoch 1, Step 33, Loss: 0.01911722682416439\n",
            "Epoch 1, Step 34, Loss: 0.018498487770557404\n",
            "Epoch 1, Step 35, Loss: 0.01739874668419361\n",
            "Epoch 1, Step 36, Loss: 0.015432048588991165\n",
            "Epoch 1, Step 37, Loss: 0.017736466601490974\n",
            "Epoch 1, Step 38, Loss: 0.014092789962887764\n",
            "Epoch 1, Step 39, Loss: 0.017083851620554924\n",
            "Epoch 1, Step 40, Loss: 0.01714261993765831\n",
            "Epoch 1, Step 41, Loss: 0.013121490366756916\n",
            "Epoch 1, Step 42, Loss: 0.012109152041375637\n",
            "Epoch 1, Step 43, Loss: 0.012918584048748016\n",
            "Epoch 1, Step 44, Loss: 0.013281675986945629\n",
            "Epoch 1, Step 45, Loss: 0.013474229723215103\n",
            "Epoch 1, Step 46, Loss: 0.012620924971997738\n",
            "Epoch 1, Step 47, Loss: 0.013953180983662605\n",
            "Epoch 1, Step 48, Loss: 0.012319630943238735\n",
            "Epoch 1, Step 49, Loss: 0.012546814046800137\n",
            "Epoch 1, Step 50, Loss: 0.012791587971150875\n",
            "Epoch 1, Step 51, Loss: 0.01248769648373127\n",
            "Epoch 1, Step 52, Loss: 0.011584198102355003\n",
            "Epoch 1, Step 53, Loss: 0.014553210698068142\n",
            "Epoch 1, Step 54, Loss: 0.011926756240427494\n",
            "Epoch 1, Step 55, Loss: 0.011923535726964474\n",
            "Epoch 1, Step 56, Loss: 0.011950071901082993\n",
            "Epoch 1, Step 57, Loss: 0.010708773508667946\n",
            "Epoch 1, Step 58, Loss: 0.011729790829122066\n",
            "Epoch 1, Step 59, Loss: 0.010279922746121883\n",
            "Epoch 1, Step 60, Loss: 0.0105551453307271\n",
            "Epoch 1, Step 61, Loss: 0.01268626470118761\n",
            "Epoch 1, Step 62, Loss: 0.011532431468367577\n",
            "Epoch 1, Step 63, Loss: 0.025766197592020035\n",
            "Epoch 1, Step 64, Loss: 0.011567533947527409\n",
            "Epoch 1, Step 65, Loss: 0.010394839569926262\n",
            "Epoch 1, Step 66, Loss: 0.009259955026209354\n",
            "Epoch 1, Step 67, Loss: 0.010294069536030293\n",
            "Epoch 1, Step 68, Loss: 0.010150140151381493\n",
            "Epoch 1, Step 69, Loss: 0.010186010040342808\n",
            "Epoch 1, Step 70, Loss: 0.01134796254336834\n",
            "Epoch 1, Step 71, Loss: 0.011473044753074646\n",
            "Epoch 1, Step 72, Loss: 0.011629692278802395\n",
            "Epoch 1, Step 73, Loss: 0.010452425107359886\n",
            "Epoch 1, Step 74, Loss: 0.009468820877373219\n",
            "Epoch 1, Step 75, Loss: 0.010328643955290318\n",
            "Epoch 1, Step 76, Loss: 0.010136854834854603\n",
            "Epoch 1, Step 77, Loss: 0.0101632634177804\n",
            "Epoch 1, Step 78, Loss: 0.009047584608197212\n",
            "Epoch 1, Step 79, Loss: 0.009719994850456715\n",
            "Epoch 1, Step 80, Loss: 0.009385473094880581\n",
            "Epoch 1, Step 81, Loss: 0.013784877024590969\n",
            "Epoch 1, Step 82, Loss: 0.011211805045604706\n",
            "Epoch 1, Step 83, Loss: 0.0110401576384902\n",
            "Epoch 1, Step 84, Loss: 0.01249648816883564\n",
            "Epoch 1, Step 85, Loss: 0.012673923745751381\n",
            "Epoch 1, Step 86, Loss: 0.012530744075775146\n",
            "Train Metric MRRs: 0.022566400630404094\n",
            "Train Metric MAPs: 0.007270636466459026\n",
            "Validation Metric MRRs: 0.01950065993442285\n",
            "Validation Metric MAPs: 0.004338648729797861\n",
            "Epoch 2, Step 1, Loss: 0.0678897500038147\n",
            "Epoch 2, Step 2, Loss: 0.03645167499780655\n",
            "Epoch 2, Step 3, Loss: 0.03481399267911911\n",
            "Epoch 2, Step 4, Loss: 0.028575118631124496\n",
            "Epoch 2, Step 5, Loss: 0.024263791739940643\n",
            "Epoch 2, Step 6, Loss: 0.01952211558818817\n",
            "Epoch 2, Step 7, Loss: 0.015291104093194008\n",
            "Epoch 2, Step 8, Loss: 0.012263210490345955\n",
            "Epoch 2, Step 9, Loss: 0.011345140635967255\n",
            "Epoch 2, Step 10, Loss: 0.0116647994145751\n",
            "Epoch 2, Step 11, Loss: 0.011232760734856129\n",
            "Epoch 2, Step 12, Loss: 0.011099984869360924\n",
            "Epoch 2, Step 13, Loss: 0.011024159379303455\n",
            "Epoch 2, Step 14, Loss: 0.01100910734385252\n",
            "Epoch 2, Step 15, Loss: 0.010922168381512165\n",
            "Epoch 2, Step 16, Loss: 0.011093514040112495\n",
            "Epoch 2, Step 17, Loss: 0.016855191439390182\n",
            "Epoch 2, Step 18, Loss: 0.02147349715232849\n",
            "Epoch 2, Step 19, Loss: 0.024750856682658195\n",
            "Epoch 2, Step 20, Loss: 0.02471170760691166\n",
            "Epoch 2, Step 21, Loss: 0.021046219393610954\n",
            "Epoch 2, Step 22, Loss: 0.022847304120659828\n",
            "Epoch 2, Step 23, Loss: 0.022968681529164314\n",
            "Epoch 2, Step 24, Loss: 0.02272976003587246\n",
            "Epoch 2, Step 25, Loss: 0.01988498866558075\n",
            "Epoch 2, Step 26, Loss: 0.019675834104418755\n",
            "Epoch 2, Step 27, Loss: 0.015966493636369705\n",
            "Epoch 2, Step 28, Loss: 0.016756068915128708\n",
            "Epoch 2, Step 29, Loss: 0.016790764406323433\n",
            "Epoch 2, Step 30, Loss: 0.01695966348052025\n",
            "Epoch 2, Step 31, Loss: 0.018525244668126106\n",
            "Epoch 2, Step 32, Loss: 0.016243668273091316\n",
            "Epoch 2, Step 33, Loss: 0.015964355319738388\n",
            "Epoch 2, Step 34, Loss: 0.016174575313925743\n",
            "Epoch 2, Step 35, Loss: 0.01435683760792017\n",
            "Epoch 2, Step 36, Loss: 0.013473534025251865\n",
            "Epoch 2, Step 37, Loss: 0.013846900314092636\n",
            "Epoch 2, Step 38, Loss: 0.011644666083157063\n",
            "Epoch 2, Step 39, Loss: 0.013774627819657326\n",
            "Epoch 2, Step 40, Loss: 0.014158277772367\n",
            "Epoch 2, Step 41, Loss: 0.010713346302509308\n",
            "Epoch 2, Step 42, Loss: 0.009800951927900314\n",
            "Epoch 2, Step 43, Loss: 0.009276087395846844\n",
            "Epoch 2, Step 44, Loss: 0.009897752664983273\n",
            "Epoch 2, Step 45, Loss: 0.010706630535423756\n",
            "Epoch 2, Step 46, Loss: 0.010299023240804672\n",
            "Epoch 2, Step 47, Loss: 0.011712836101651192\n",
            "Epoch 2, Step 48, Loss: 0.010351115837693214\n",
            "Epoch 2, Step 49, Loss: 0.010709313675761223\n",
            "Epoch 2, Step 50, Loss: 0.010143727995455265\n",
            "Epoch 2, Step 51, Loss: 0.011020198464393616\n",
            "Epoch 2, Step 52, Loss: 0.00974841695278883\n",
            "Epoch 2, Step 53, Loss: 0.012537811882793903\n",
            "Epoch 2, Step 54, Loss: 0.010716352611780167\n",
            "Epoch 2, Step 55, Loss: 0.010282469913363457\n",
            "Epoch 2, Step 56, Loss: 0.010868203826248646\n",
            "Epoch 2, Step 57, Loss: 0.00950885284692049\n",
            "Epoch 2, Step 58, Loss: 0.010264193639159203\n",
            "Epoch 2, Step 59, Loss: 0.009532317519187927\n",
            "Epoch 2, Step 60, Loss: 0.009638224728405476\n",
            "Epoch 2, Step 61, Loss: 0.011266851797699928\n",
            "Epoch 2, Step 62, Loss: 0.010741948150098324\n",
            "Epoch 2, Step 63, Loss: 0.022546442225575447\n",
            "Epoch 2, Step 64, Loss: 0.009529455564916134\n",
            "Epoch 2, Step 65, Loss: 0.009601598605513573\n",
            "Epoch 2, Step 66, Loss: 0.008704160340130329\n",
            "Epoch 2, Step 67, Loss: 0.009488232433795929\n",
            "Epoch 2, Step 68, Loss: 0.008663367480039597\n",
            "Epoch 2, Step 69, Loss: 0.009067690931260586\n",
            "Epoch 2, Step 70, Loss: 0.010204619728028774\n",
            "Epoch 2, Step 71, Loss: 0.010567561723291874\n",
            "Epoch 2, Step 72, Loss: 0.010362519882619381\n",
            "Epoch 2, Step 73, Loss: 0.009171689860522747\n",
            "Epoch 2, Step 74, Loss: 0.00831847544759512\n",
            "Epoch 2, Step 75, Loss: 0.009411192499101162\n",
            "Epoch 2, Step 76, Loss: 0.008997994475066662\n",
            "Epoch 2, Step 77, Loss: 0.009353403933346272\n",
            "Epoch 2, Step 78, Loss: 0.008481807075440884\n",
            "Epoch 2, Step 79, Loss: 0.008621340617537498\n",
            "Epoch 2, Step 80, Loss: 0.008669879287481308\n",
            "Epoch 2, Step 81, Loss: 0.012684674002230167\n",
            "Epoch 2, Step 82, Loss: 0.010436616837978363\n",
            "Epoch 2, Step 83, Loss: 0.010307635180652142\n",
            "Epoch 2, Step 84, Loss: 0.011548226699233055\n",
            "Epoch 2, Step 85, Loss: 0.011824078857898712\n",
            "Epoch 2, Step 86, Loss: 0.012022316455841064\n",
            "Train Metric MRRs: 0.023237855273557043\n",
            "Train Metric MAPs: 0.005100329070021592\n",
            "Validation Metric MRRs: 0.03168247991671291\n",
            "Validation Metric MAPs: 0.006342799021291238\n",
            "Epoch 3, Step 1, Loss: 0.04606620594859123\n",
            "Epoch 3, Step 2, Loss: 0.03457958623766899\n",
            "Epoch 3, Step 3, Loss: 0.03311070427298546\n",
            "Epoch 3, Step 4, Loss: 0.024629171937704086\n",
            "Epoch 3, Step 5, Loss: 0.0198112390935421\n",
            "Epoch 3, Step 6, Loss: 0.017597347497940063\n",
            "Epoch 3, Step 7, Loss: 0.015455645509064198\n",
            "Epoch 3, Step 8, Loss: 0.013674208894371986\n",
            "Epoch 3, Step 9, Loss: 0.012891522608697414\n",
            "Epoch 3, Step 10, Loss: 0.012604588642716408\n",
            "Epoch 3, Step 11, Loss: 0.011906675063073635\n",
            "Epoch 3, Step 12, Loss: 0.011018590070307255\n",
            "Epoch 3, Step 13, Loss: 0.009933230467140675\n",
            "Epoch 3, Step 14, Loss: 0.009313643909990788\n",
            "Epoch 3, Step 15, Loss: 0.007940932177007198\n",
            "Epoch 3, Step 16, Loss: 0.008958971127867699\n",
            "Epoch 3, Step 17, Loss: 0.011066763661801815\n",
            "Epoch 3, Step 18, Loss: 0.012897294946014881\n",
            "Epoch 3, Step 19, Loss: 0.016819868236780167\n",
            "Epoch 3, Step 20, Loss: 0.017835941165685654\n",
            "Epoch 3, Step 21, Loss: 0.0168029572814703\n",
            "Epoch 3, Step 22, Loss: 0.018155891448259354\n",
            "Epoch 3, Step 23, Loss: 0.019997166469693184\n",
            "Epoch 3, Step 24, Loss: 0.021132539957761765\n",
            "Epoch 3, Step 25, Loss: 0.019172821193933487\n",
            "Epoch 3, Step 26, Loss: 0.01904807612299919\n",
            "Epoch 3, Step 27, Loss: 0.014704110100865364\n",
            "Epoch 3, Step 28, Loss: 0.014615550637245178\n",
            "Epoch 3, Step 29, Loss: 0.01468629576265812\n",
            "Epoch 3, Step 30, Loss: 0.013923386111855507\n",
            "Epoch 3, Step 31, Loss: 0.016141369938850403\n",
            "Epoch 3, Step 32, Loss: 0.013911928981542587\n",
            "Epoch 3, Step 33, Loss: 0.014205744490027428\n",
            "Epoch 3, Step 34, Loss: 0.014082411304116249\n",
            "Epoch 3, Step 35, Loss: 0.013351712375879288\n",
            "Epoch 3, Step 36, Loss: 0.013011298142373562\n",
            "Epoch 3, Step 37, Loss: 0.013152282685041428\n",
            "Epoch 3, Step 38, Loss: 0.011469408869743347\n",
            "Epoch 3, Step 39, Loss: 0.0130575280636549\n",
            "Epoch 3, Step 40, Loss: 0.013338347896933556\n",
            "Epoch 3, Step 41, Loss: 0.010549734346568584\n",
            "Epoch 3, Step 42, Loss: 0.009727331809699535\n",
            "Epoch 3, Step 43, Loss: 0.008979934267699718\n",
            "Epoch 3, Step 44, Loss: 0.009199380874633789\n",
            "Epoch 3, Step 45, Loss: 0.009745699353516102\n",
            "Epoch 3, Step 46, Loss: 0.009133822284638882\n",
            "Epoch 3, Step 47, Loss: 0.009875976480543613\n",
            "Epoch 3, Step 48, Loss: 0.009234474040567875\n",
            "Epoch 3, Step 49, Loss: 0.009158141911029816\n",
            "Epoch 3, Step 50, Loss: 0.008842376992106438\n",
            "Epoch 3, Step 51, Loss: 0.009959952905774117\n",
            "Epoch 3, Step 52, Loss: 0.008849977515637875\n",
            "Epoch 3, Step 53, Loss: 0.012209054082632065\n",
            "Epoch 3, Step 54, Loss: 0.010180511511862278\n",
            "Epoch 3, Step 55, Loss: 0.00949352141469717\n",
            "Epoch 3, Step 56, Loss: 0.010427296161651611\n",
            "Epoch 3, Step 57, Loss: 0.009004274383187294\n",
            "Epoch 3, Step 58, Loss: 0.00974409282207489\n",
            "Epoch 3, Step 59, Loss: 0.008559332229197025\n",
            "Epoch 3, Step 60, Loss: 0.008885310962796211\n",
            "Epoch 3, Step 61, Loss: 0.010863314382731915\n",
            "Epoch 3, Step 62, Loss: 0.009858553297817707\n",
            "Epoch 3, Step 63, Loss: 0.022562777623534203\n",
            "Epoch 3, Step 64, Loss: 0.009048884734511375\n",
            "Epoch 3, Step 65, Loss: 0.008735459297895432\n",
            "Epoch 3, Step 66, Loss: 0.00824296846985817\n",
            "Epoch 3, Step 67, Loss: 0.00895243976265192\n",
            "Epoch 3, Step 68, Loss: 0.007972143590450287\n",
            "Epoch 3, Step 69, Loss: 0.008594026789069176\n",
            "Epoch 3, Step 70, Loss: 0.009500665590167046\n",
            "Epoch 3, Step 71, Loss: 0.009946079924702644\n",
            "Epoch 3, Step 72, Loss: 0.0096866674721241\n",
            "Epoch 3, Step 73, Loss: 0.008934237994253635\n",
            "Epoch 3, Step 74, Loss: 0.007785119581967592\n",
            "Epoch 3, Step 75, Loss: 0.008082546293735504\n",
            "Epoch 3, Step 76, Loss: 0.00825024489313364\n",
            "Epoch 3, Step 77, Loss: 0.008526422083377838\n",
            "Epoch 3, Step 78, Loss: 0.007639412768185139\n",
            "Epoch 3, Step 79, Loss: 0.00788592267781496\n",
            "Epoch 3, Step 80, Loss: 0.008016113191843033\n",
            "Epoch 3, Step 81, Loss: 0.011725705116987228\n",
            "Epoch 3, Step 82, Loss: 0.009417980909347534\n",
            "Epoch 3, Step 83, Loss: 0.009705325588583946\n",
            "Epoch 3, Step 84, Loss: 0.011073693633079529\n",
            "Epoch 3, Step 85, Loss: 0.010993104428052902\n",
            "Epoch 3, Step 86, Loss: 0.011431996710598469\n",
            "Train Metric MRRs: 0.04287555307208291\n",
            "Train Metric MAPs: 0.008803697052324875\n",
            "Validation Metric MRRs: 0.04961702490804017\n",
            "Validation Metric MAPs: 0.011000685144600208\n",
            "Epoch 4, Step 1, Loss: 0.03957528993487358\n",
            "Epoch 4, Step 2, Loss: 0.03206319734454155\n",
            "Epoch 4, Step 3, Loss: 0.03245067223906517\n",
            "Epoch 4, Step 4, Loss: 0.023230096325278282\n",
            "Epoch 4, Step 5, Loss: 0.017148932442069054\n",
            "Epoch 4, Step 6, Loss: 0.014807259663939476\n",
            "Epoch 4, Step 7, Loss: 0.012305565178394318\n",
            "Epoch 4, Step 8, Loss: 0.01063895970582962\n",
            "Epoch 4, Step 9, Loss: 0.01029211562126875\n",
            "Epoch 4, Step 10, Loss: 0.009442683309316635\n",
            "Epoch 4, Step 11, Loss: 0.009387629106640816\n",
            "Epoch 4, Step 12, Loss: 0.009054409340023994\n",
            "Epoch 4, Step 13, Loss: 0.00799647532403469\n",
            "Epoch 4, Step 14, Loss: 0.007706908043473959\n",
            "Epoch 4, Step 15, Loss: 0.006969144102185965\n",
            "Epoch 4, Step 16, Loss: 0.008015529252588749\n",
            "Epoch 4, Step 17, Loss: 0.010011275298893452\n",
            "Epoch 4, Step 18, Loss: 0.01169205829501152\n",
            "Epoch 4, Step 19, Loss: 0.01472801249474287\n",
            "Epoch 4, Step 20, Loss: 0.01605847105383873\n",
            "Epoch 4, Step 21, Loss: 0.01453387551009655\n",
            "Epoch 4, Step 22, Loss: 0.015398074872791767\n",
            "Epoch 4, Step 23, Loss: 0.0177298691123724\n",
            "Epoch 4, Step 24, Loss: 0.01857813633978367\n",
            "Epoch 4, Step 25, Loss: 0.016984950751066208\n",
            "Epoch 4, Step 26, Loss: 0.017393184825778008\n",
            "Epoch 4, Step 27, Loss: 0.013039713725447655\n",
            "Epoch 4, Step 28, Loss: 0.013420122675597668\n",
            "Epoch 4, Step 29, Loss: 0.013558258302509785\n",
            "Epoch 4, Step 30, Loss: 0.012514354661107063\n",
            "Epoch 4, Step 31, Loss: 0.014842300675809383\n",
            "Epoch 4, Step 32, Loss: 0.012760872021317482\n",
            "Epoch 4, Step 33, Loss: 0.013179976493120193\n",
            "Epoch 4, Step 34, Loss: 0.012990310788154602\n",
            "Epoch 4, Step 35, Loss: 0.012042264454066753\n",
            "Epoch 4, Step 36, Loss: 0.011763381771743298\n",
            "Epoch 4, Step 37, Loss: 0.01179476547986269\n",
            "Epoch 4, Step 38, Loss: 0.010207605548202991\n",
            "Epoch 4, Step 39, Loss: 0.010882855392992496\n",
            "Epoch 4, Step 40, Loss: 0.0112993773072958\n",
            "Epoch 4, Step 41, Loss: 0.008694760501384735\n",
            "Epoch 4, Step 42, Loss: 0.00821745116263628\n",
            "Epoch 4, Step 43, Loss: 0.007744715083390474\n",
            "Epoch 4, Step 44, Loss: 0.008000800386071205\n",
            "Epoch 4, Step 45, Loss: 0.008721334859728813\n",
            "Epoch 4, Step 46, Loss: 0.007950524799525738\n",
            "Epoch 4, Step 47, Loss: 0.008635295554995537\n",
            "Epoch 4, Step 48, Loss: 0.007998582907021046\n",
            "Epoch 4, Step 49, Loss: 0.008243704214692116\n",
            "Epoch 4, Step 50, Loss: 0.007446335628628731\n",
            "Epoch 4, Step 51, Loss: 0.008023816160857677\n",
            "Epoch 4, Step 52, Loss: 0.007203524466603994\n",
            "Epoch 4, Step 53, Loss: 0.009028471074998379\n",
            "Epoch 4, Step 54, Loss: 0.007951052859425545\n",
            "Epoch 4, Step 55, Loss: 0.008406445384025574\n",
            "Epoch 4, Step 56, Loss: 0.009012882597744465\n",
            "Epoch 4, Step 57, Loss: 0.008077067323029041\n",
            "Epoch 4, Step 58, Loss: 0.00874694436788559\n",
            "Epoch 4, Step 59, Loss: 0.007439333014190197\n",
            "Epoch 4, Step 60, Loss: 0.007745240814983845\n",
            "Epoch 4, Step 61, Loss: 0.01000166591256857\n",
            "Epoch 4, Step 62, Loss: 0.008223769254982471\n",
            "Epoch 4, Step 63, Loss: 0.016326190903782845\n",
            "Epoch 4, Step 64, Loss: 0.00780566455796361\n",
            "Epoch 4, Step 65, Loss: 0.0074903033673763275\n",
            "Epoch 4, Step 66, Loss: 0.007134790066629648\n",
            "Epoch 4, Step 67, Loss: 0.007492453791201115\n",
            "Epoch 4, Step 68, Loss: 0.0064125522039830685\n",
            "Epoch 4, Step 69, Loss: 0.007040124386548996\n",
            "Epoch 4, Step 70, Loss: 0.007533323485404253\n",
            "Epoch 4, Step 71, Loss: 0.007785912603139877\n",
            "Epoch 4, Step 72, Loss: 0.007080641575157642\n",
            "Epoch 4, Step 73, Loss: 0.006901684682816267\n",
            "Epoch 4, Step 74, Loss: 0.006240111775696278\n",
            "Epoch 4, Step 75, Loss: 0.006433043163269758\n",
            "Epoch 4, Step 76, Loss: 0.006545700132846832\n",
            "Epoch 4, Step 77, Loss: 0.006623169407248497\n",
            "Epoch 4, Step 78, Loss: 0.006163503043353558\n",
            "Epoch 4, Step 79, Loss: 0.006390844006091356\n",
            "Epoch 4, Step 80, Loss: 0.006311701610684395\n",
            "Epoch 4, Step 81, Loss: 0.009962888434529305\n",
            "Epoch 4, Step 82, Loss: 0.0076353768818080425\n",
            "Epoch 4, Step 83, Loss: 0.00795358419418335\n",
            "Epoch 4, Step 84, Loss: 0.009569311514496803\n",
            "Epoch 4, Step 85, Loss: 0.008798071183264256\n",
            "Epoch 4, Step 86, Loss: 0.008574792183935642\n",
            "Train Metric MRRs: 0.1580555052488181\n",
            "Train Metric MAPs: 0.13246614188286956\n",
            "Validation Metric MRRs: 0.19249378355639865\n",
            "Validation Metric MAPs: 0.21253425199384438\n",
            "Epoch 5, Step 1, Loss: 0.027243778109550476\n",
            "Epoch 5, Step 2, Loss: 0.024488890543580055\n",
            "Epoch 5, Step 3, Loss: 0.019208122044801712\n",
            "Epoch 5, Step 4, Loss: 0.01643027737736702\n",
            "Epoch 5, Step 5, Loss: 0.012093099765479565\n",
            "Epoch 5, Step 6, Loss: 0.009982743300497532\n",
            "Epoch 5, Step 7, Loss: 0.009420927613973618\n",
            "Epoch 5, Step 8, Loss: 0.008192785084247589\n",
            "Epoch 5, Step 9, Loss: 0.006478603463619947\n",
            "Epoch 5, Step 10, Loss: 0.006179537624120712\n",
            "Epoch 5, Step 11, Loss: 0.006861471571028233\n",
            "Epoch 5, Step 12, Loss: 0.006494872737675905\n",
            "Epoch 5, Step 13, Loss: 0.006015182007104158\n",
            "Epoch 5, Step 14, Loss: 0.005596829112619162\n",
            "Epoch 5, Step 15, Loss: 0.005747945513576269\n",
            "Epoch 5, Step 16, Loss: 0.006331947166472673\n",
            "Epoch 5, Step 17, Loss: 0.007930797524750233\n",
            "Epoch 5, Step 18, Loss: 0.00959833338856697\n",
            "Epoch 5, Step 19, Loss: 0.011832954362034798\n",
            "Epoch 5, Step 20, Loss: 0.012473821640014648\n",
            "Epoch 5, Step 21, Loss: 0.01196774747222662\n",
            "Epoch 5, Step 22, Loss: 0.012546278536319733\n",
            "Epoch 5, Step 23, Loss: 0.012647720053792\n",
            "Epoch 5, Step 24, Loss: 0.014589302241802216\n",
            "Epoch 5, Step 25, Loss: 0.014024173840880394\n",
            "Epoch 5, Step 26, Loss: 0.013877030462026596\n",
            "Epoch 5, Step 27, Loss: 0.010735346004366875\n",
            "Epoch 5, Step 28, Loss: 0.01104667317122221\n",
            "Epoch 5, Step 29, Loss: 0.011519256979227066\n",
            "Epoch 5, Step 30, Loss: 0.009874250739812851\n",
            "Epoch 5, Step 31, Loss: 0.012199493125081062\n",
            "Epoch 5, Step 32, Loss: 0.010199475102126598\n",
            "Epoch 5, Step 33, Loss: 0.011154002510011196\n",
            "Epoch 5, Step 34, Loss: 0.010863736271858215\n",
            "Epoch 5, Step 35, Loss: 0.009950079023838043\n",
            "Epoch 5, Step 36, Loss: 0.010169782675802708\n",
            "Epoch 5, Step 37, Loss: 0.00987505167722702\n",
            "Epoch 5, Step 38, Loss: 0.008305037394165993\n",
            "Epoch 5, Step 39, Loss: 0.00832954328507185\n",
            "Epoch 5, Step 40, Loss: 0.008988318964838982\n",
            "Epoch 5, Step 41, Loss: 0.006924790795892477\n",
            "Epoch 5, Step 42, Loss: 0.006646543275564909\n",
            "Epoch 5, Step 43, Loss: 0.006561138667166233\n",
            "Epoch 5, Step 44, Loss: 0.00687364349141717\n",
            "Epoch 5, Step 45, Loss: 0.007584613282233477\n",
            "Epoch 5, Step 46, Loss: 0.006906514056026936\n",
            "Epoch 5, Step 47, Loss: 0.007389305159449577\n",
            "Epoch 5, Step 48, Loss: 0.006897501181811094\n",
            "Epoch 5, Step 49, Loss: 0.007117515429854393\n",
            "Epoch 5, Step 50, Loss: 0.006422228179872036\n",
            "Epoch 5, Step 51, Loss: 0.006533225066959858\n",
            "Epoch 5, Step 52, Loss: 0.006310797296464443\n",
            "Epoch 5, Step 53, Loss: 0.0076402039267122746\n",
            "Epoch 5, Step 54, Loss: 0.006579367443919182\n",
            "Epoch 5, Step 55, Loss: 0.006752007640898228\n",
            "Epoch 5, Step 56, Loss: 0.00753718102350831\n",
            "Epoch 5, Step 57, Loss: 0.007074251770973206\n",
            "Epoch 5, Step 58, Loss: 0.007338578347116709\n",
            "Epoch 5, Step 59, Loss: 0.006537985056638718\n",
            "Epoch 5, Step 60, Loss: 0.006936792284250259\n",
            "Epoch 5, Step 61, Loss: 0.008862806484103203\n",
            "Epoch 5, Step 62, Loss: 0.006944173481315374\n",
            "Epoch 5, Step 63, Loss: 0.013307149522006512\n",
            "Epoch 5, Step 64, Loss: 0.007615501992404461\n",
            "Epoch 5, Step 65, Loss: 0.007379171904176474\n",
            "Epoch 5, Step 66, Loss: 0.007081272080540657\n",
            "Epoch 5, Step 67, Loss: 0.007047845516353846\n",
            "Epoch 5, Step 68, Loss: 0.006054614670574665\n",
            "Epoch 5, Step 69, Loss: 0.0066019813530147076\n",
            "Epoch 5, Step 70, Loss: 0.006928654853254557\n",
            "Epoch 5, Step 71, Loss: 0.00718726497143507\n",
            "Epoch 5, Step 72, Loss: 0.00631072698161006\n",
            "Epoch 5, Step 73, Loss: 0.006173537578433752\n",
            "Epoch 5, Step 74, Loss: 0.005785794463008642\n",
            "Epoch 5, Step 75, Loss: 0.005920744035393\n",
            "Epoch 5, Step 76, Loss: 0.006108391098678112\n",
            "Epoch 5, Step 77, Loss: 0.0062466575764119625\n",
            "Epoch 5, Step 78, Loss: 0.005910149309784174\n",
            "Epoch 5, Step 79, Loss: 0.006006013602018356\n",
            "Epoch 5, Step 80, Loss: 0.00602641561999917\n",
            "Epoch 5, Step 81, Loss: 0.009698170237243176\n",
            "Epoch 5, Step 82, Loss: 0.0073402575217187405\n",
            "Epoch 5, Step 83, Loss: 0.00767227029427886\n",
            "Epoch 5, Step 84, Loss: 0.009132223203778267\n",
            "Epoch 5, Step 85, Loss: 0.008328789845108986\n",
            "Epoch 5, Step 86, Loss: 0.008288112469017506\n",
            "Train Metric MRRs: 0.2453095114314905\n",
            "Train Metric MAPs: 0.2877171186181717\n",
            "Validation Metric MRRs: 0.19542281478832824\n",
            "Validation Metric MAPs: 0.20987222530200086\n",
            "Epoch 6, Step 1, Loss: 0.024325449019670486\n",
            "Epoch 6, Step 2, Loss: 0.022055594250559807\n",
            "Epoch 6, Step 3, Loss: 0.01802583411335945\n",
            "Epoch 6, Step 4, Loss: 0.015611921437084675\n",
            "Epoch 6, Step 5, Loss: 0.011233421042561531\n",
            "Epoch 6, Step 6, Loss: 0.00919572077691555\n",
            "Epoch 6, Step 7, Loss: 0.007494952064007521\n",
            "Epoch 6, Step 8, Loss: 0.007156100589782\n",
            "Epoch 6, Step 9, Loss: 0.006242197938263416\n",
            "Epoch 6, Step 10, Loss: 0.006245896220207214\n",
            "Epoch 6, Step 11, Loss: 0.006725301034748554\n",
            "Epoch 6, Step 12, Loss: 0.0062712798826396465\n",
            "Epoch 6, Step 13, Loss: 0.005928380414843559\n",
            "Epoch 6, Step 14, Loss: 0.0055361720733344555\n",
            "Epoch 6, Step 15, Loss: 0.005836003925651312\n",
            "Epoch 6, Step 16, Loss: 0.006215729285031557\n",
            "Epoch 6, Step 17, Loss: 0.007533493917435408\n",
            "Epoch 6, Step 18, Loss: 0.009166620671749115\n",
            "Epoch 6, Step 19, Loss: 0.01068824902176857\n",
            "Epoch 6, Step 20, Loss: 0.010932316072285175\n",
            "Epoch 6, Step 21, Loss: 0.011095595546066761\n",
            "Epoch 6, Step 22, Loss: 0.011481867171823978\n",
            "Epoch 6, Step 23, Loss: 0.011934022419154644\n",
            "Epoch 6, Step 24, Loss: 0.013884147629141808\n",
            "Epoch 6, Step 25, Loss: 0.013389809988439083\n",
            "Epoch 6, Step 26, Loss: 0.013415619730949402\n",
            "Epoch 6, Step 27, Loss: 0.010246791876852512\n",
            "Epoch 6, Step 28, Loss: 0.010743360035121441\n",
            "Epoch 6, Step 29, Loss: 0.011332625523209572\n",
            "Epoch 6, Step 30, Loss: 0.009297541342675686\n",
            "Epoch 6, Step 31, Loss: 0.011840215884149075\n",
            "Epoch 6, Step 32, Loss: 0.010037682950496674\n",
            "Epoch 6, Step 33, Loss: 0.010816117748618126\n",
            "Epoch 6, Step 34, Loss: 0.010154878720641136\n",
            "Epoch 6, Step 35, Loss: 0.009422560222446918\n",
            "Epoch 6, Step 36, Loss: 0.009346545673906803\n",
            "Epoch 6, Step 37, Loss: 0.009361927397549152\n",
            "Epoch 6, Step 38, Loss: 0.008024142123758793\n",
            "Epoch 6, Step 39, Loss: 0.008002554066479206\n",
            "Epoch 6, Step 40, Loss: 0.008494634181261063\n",
            "Epoch 6, Step 41, Loss: 0.0067641218192875385\n",
            "Epoch 6, Step 42, Loss: 0.0063606188632547855\n",
            "Epoch 6, Step 43, Loss: 0.006371878087520599\n",
            "Epoch 6, Step 44, Loss: 0.006549150217324495\n",
            "Epoch 6, Step 45, Loss: 0.007283731363713741\n",
            "Epoch 6, Step 46, Loss: 0.006610658951103687\n",
            "Epoch 6, Step 47, Loss: 0.007127532735466957\n",
            "Epoch 6, Step 48, Loss: 0.006625892128795385\n",
            "Epoch 6, Step 49, Loss: 0.006817059591412544\n",
            "Epoch 6, Step 50, Loss: 0.006211832631379366\n",
            "Epoch 6, Step 51, Loss: 0.006223534233868122\n",
            "Epoch 6, Step 52, Loss: 0.005965035408735275\n",
            "Epoch 6, Step 53, Loss: 0.007311951369047165\n",
            "Epoch 6, Step 54, Loss: 0.006347255315631628\n",
            "Epoch 6, Step 55, Loss: 0.006520559079945087\n",
            "Epoch 6, Step 56, Loss: 0.007425554562360048\n",
            "Epoch 6, Step 57, Loss: 0.006805315613746643\n",
            "Epoch 6, Step 58, Loss: 0.0072369882836937904\n",
            "Epoch 6, Step 59, Loss: 0.0063940915279090405\n",
            "Epoch 6, Step 60, Loss: 0.006708452478051186\n",
            "Epoch 6, Step 61, Loss: 0.008592167869210243\n",
            "Epoch 6, Step 62, Loss: 0.006967775523662567\n",
            "Epoch 6, Step 63, Loss: 0.013146752491593361\n",
            "Epoch 6, Step 64, Loss: 0.007622293196618557\n",
            "Epoch 6, Step 65, Loss: 0.006822482217103243\n",
            "Epoch 6, Step 66, Loss: 0.006559382658451796\n",
            "Epoch 6, Step 67, Loss: 0.007068273611366749\n",
            "Epoch 6, Step 68, Loss: 0.00585336796939373\n",
            "Epoch 6, Step 69, Loss: 0.006334098521620035\n",
            "Epoch 6, Step 70, Loss: 0.006667166016995907\n",
            "Epoch 6, Step 71, Loss: 0.00697963684797287\n",
            "Epoch 6, Step 72, Loss: 0.005951385013759136\n",
            "Epoch 6, Step 73, Loss: 0.005969162564724684\n",
            "Epoch 6, Step 74, Loss: 0.0055478294380009174\n",
            "Epoch 6, Step 75, Loss: 0.00576802808791399\n",
            "Epoch 6, Step 76, Loss: 0.005966283846646547\n",
            "Epoch 6, Step 77, Loss: 0.006108079571276903\n",
            "Epoch 6, Step 78, Loss: 0.005702625494450331\n",
            "Epoch 6, Step 79, Loss: 0.0058156568557024\n",
            "Epoch 6, Step 80, Loss: 0.005852393340319395\n",
            "Epoch 6, Step 81, Loss: 0.009355652146041393\n",
            "Epoch 6, Step 82, Loss: 0.00714217871427536\n",
            "Epoch 6, Step 83, Loss: 0.0075224111787974834\n",
            "Epoch 6, Step 84, Loss: 0.00881900917738676\n",
            "Epoch 6, Step 85, Loss: 0.008189904503524303\n",
            "Epoch 6, Step 86, Loss: 0.008155785501003265\n",
            "Train Metric MRRs: 0.2579762867225216\n",
            "Train Metric MAPs: 0.29801043399504135\n",
            "Validation Metric MRRs: 0.2059642382630239\n",
            "Validation Metric MAPs: 0.22022603337018237\n",
            "Epoch 7, Step 1, Loss: 0.022521741688251495\n",
            "Epoch 7, Step 2, Loss: 0.02155240811407566\n",
            "Epoch 7, Step 3, Loss: 0.01759885437786579\n",
            "Epoch 7, Step 4, Loss: 0.015219639055430889\n",
            "Epoch 7, Step 5, Loss: 0.010550263337790966\n",
            "Epoch 7, Step 6, Loss: 0.00851893238723278\n",
            "Epoch 7, Step 7, Loss: 0.0066465106792747974\n",
            "Epoch 7, Step 8, Loss: 0.006410026922821999\n",
            "Epoch 7, Step 9, Loss: 0.00590389221906662\n",
            "Epoch 7, Step 10, Loss: 0.00570627162232995\n",
            "Epoch 7, Step 11, Loss: 0.006426935084164143\n",
            "Epoch 7, Step 12, Loss: 0.005875272676348686\n",
            "Epoch 7, Step 13, Loss: 0.005776355043053627\n",
            "Epoch 7, Step 14, Loss: 0.0051339371129870415\n",
            "Epoch 7, Step 15, Loss: 0.005524881184101105\n",
            "Epoch 7, Step 16, Loss: 0.0058824047446250916\n",
            "Epoch 7, Step 17, Loss: 0.007430182304233313\n",
            "Epoch 7, Step 18, Loss: 0.00876145251095295\n",
            "Epoch 7, Step 19, Loss: 0.010273698717355728\n",
            "Epoch 7, Step 20, Loss: 0.010046367533504963\n",
            "Epoch 7, Step 21, Loss: 0.010540233924984932\n",
            "Epoch 7, Step 22, Loss: 0.010990768671035767\n",
            "Epoch 7, Step 23, Loss: 0.011495551094412804\n",
            "Epoch 7, Step 24, Loss: 0.012931386940181255\n",
            "Epoch 7, Step 25, Loss: 0.012897281907498837\n",
            "Epoch 7, Step 26, Loss: 0.012973589822649956\n",
            "Epoch 7, Step 27, Loss: 0.010110604576766491\n",
            "Epoch 7, Step 28, Loss: 0.010278735309839249\n",
            "Epoch 7, Step 29, Loss: 0.011164230294525623\n",
            "Epoch 7, Step 30, Loss: 0.00876020360738039\n",
            "Epoch 7, Step 31, Loss: 0.011694512329995632\n",
            "Epoch 7, Step 32, Loss: 0.00970823597162962\n",
            "Epoch 7, Step 33, Loss: 0.010600456036627293\n",
            "Epoch 7, Step 34, Loss: 0.009809249080717564\n",
            "Epoch 7, Step 35, Loss: 0.009246277622878551\n",
            "Epoch 7, Step 36, Loss: 0.009288477711379528\n",
            "Epoch 7, Step 37, Loss: 0.00915984995663166\n",
            "Epoch 7, Step 38, Loss: 0.007872451096773148\n",
            "Epoch 7, Step 39, Loss: 0.007881363853812218\n",
            "Epoch 7, Step 40, Loss: 0.008169541135430336\n",
            "Epoch 7, Step 41, Loss: 0.006661273539066315\n",
            "Epoch 7, Step 42, Loss: 0.006187656428664923\n",
            "Epoch 7, Step 43, Loss: 0.006213329266756773\n",
            "Epoch 7, Step 44, Loss: 0.0062829977832734585\n",
            "Epoch 7, Step 45, Loss: 0.007025076076388359\n",
            "Epoch 7, Step 46, Loss: 0.006434700917452574\n",
            "Epoch 7, Step 47, Loss: 0.006794440560042858\n",
            "Epoch 7, Step 48, Loss: 0.006385276559740305\n",
            "Epoch 7, Step 49, Loss: 0.006508559919893742\n",
            "Epoch 7, Step 50, Loss: 0.005986772011965513\n",
            "Epoch 7, Step 51, Loss: 0.005921627394855022\n",
            "Epoch 7, Step 52, Loss: 0.005721691995859146\n",
            "Epoch 7, Step 53, Loss: 0.007206720300018787\n",
            "Epoch 7, Step 54, Loss: 0.006140778306871653\n",
            "Epoch 7, Step 55, Loss: 0.006364211905747652\n",
            "Epoch 7, Step 56, Loss: 0.007306430954486132\n",
            "Epoch 7, Step 57, Loss: 0.006704226601868868\n",
            "Epoch 7, Step 58, Loss: 0.007119064684957266\n",
            "Epoch 7, Step 59, Loss: 0.006254535634070635\n",
            "Epoch 7, Step 60, Loss: 0.006782300770282745\n",
            "Epoch 7, Step 61, Loss: 0.008457457646727562\n",
            "Epoch 7, Step 62, Loss: 0.006482960656285286\n",
            "Epoch 7, Step 63, Loss: 0.014491496607661247\n",
            "Epoch 7, Step 64, Loss: 0.00743572972714901\n",
            "Epoch 7, Step 65, Loss: 0.0065348888747394085\n",
            "Epoch 7, Step 66, Loss: 0.006192984990775585\n",
            "Epoch 7, Step 67, Loss: 0.006839757785201073\n",
            "Epoch 7, Step 68, Loss: 0.005752942990511656\n",
            "Epoch 7, Step 69, Loss: 0.006199296098202467\n",
            "Epoch 7, Step 70, Loss: 0.006531460676342249\n",
            "Epoch 7, Step 71, Loss: 0.006816357374191284\n",
            "Epoch 7, Step 72, Loss: 0.005857749842107296\n",
            "Epoch 7, Step 73, Loss: 0.005766029469668865\n",
            "Epoch 7, Step 74, Loss: 0.0054801893420517445\n",
            "Epoch 7, Step 75, Loss: 0.005629953928291798\n",
            "Epoch 7, Step 76, Loss: 0.00574896764010191\n",
            "Epoch 7, Step 77, Loss: 0.00598594406619668\n",
            "Epoch 7, Step 78, Loss: 0.005483242217451334\n",
            "Epoch 7, Step 79, Loss: 0.005612797103822231\n",
            "Epoch 7, Step 80, Loss: 0.005761223379522562\n",
            "Epoch 7, Step 81, Loss: 0.009040183387696743\n",
            "Epoch 7, Step 82, Loss: 0.007017160300165415\n",
            "Epoch 7, Step 83, Loss: 0.007416750770062208\n",
            "Epoch 7, Step 84, Loss: 0.008567972108721733\n",
            "Epoch 7, Step 85, Loss: 0.00814011599868536\n",
            "Epoch 7, Step 86, Loss: 0.008089084178209305\n",
            "Train Metric MRRs: 0.2651717478331625\n",
            "Train Metric MAPs: 0.30211568880472095\n",
            "Validation Metric MRRs: 0.20956820737497386\n",
            "Validation Metric MAPs: 0.22327281439653804\n",
            "Epoch 8, Step 1, Loss: 0.021778415888547897\n",
            "Epoch 8, Step 2, Loss: 0.021409224718809128\n",
            "Epoch 8, Step 3, Loss: 0.017768172547221184\n",
            "Epoch 8, Step 4, Loss: 0.014905025251209736\n",
            "Epoch 8, Step 5, Loss: 0.01038117054849863\n",
            "Epoch 8, Step 6, Loss: 0.008339357562363148\n",
            "Epoch 8, Step 7, Loss: 0.00638622185215354\n",
            "Epoch 8, Step 8, Loss: 0.006294358987361193\n",
            "Epoch 8, Step 9, Loss: 0.005939524620771408\n",
            "Epoch 8, Step 10, Loss: 0.005676653701812029\n",
            "Epoch 8, Step 11, Loss: 0.00620822561904788\n",
            "Epoch 8, Step 12, Loss: 0.005758677609264851\n",
            "Epoch 8, Step 13, Loss: 0.005689611192792654\n",
            "Epoch 8, Step 14, Loss: 0.004899007733911276\n",
            "Epoch 8, Step 15, Loss: 0.00539817800745368\n",
            "Epoch 8, Step 16, Loss: 0.005741685628890991\n",
            "Epoch 8, Step 17, Loss: 0.007286498323082924\n",
            "Epoch 8, Step 18, Loss: 0.008582377806305885\n",
            "Epoch 8, Step 19, Loss: 0.010037614963948727\n",
            "Epoch 8, Step 20, Loss: 0.00976167805492878\n",
            "Epoch 8, Step 21, Loss: 0.010326064191758633\n",
            "Epoch 8, Step 22, Loss: 0.010883178561925888\n",
            "Epoch 8, Step 23, Loss: 0.011258621700108051\n",
            "Epoch 8, Step 24, Loss: 0.012383164837956429\n",
            "Epoch 8, Step 25, Loss: 0.01248865108937025\n",
            "Epoch 8, Step 26, Loss: 0.012522799894213676\n",
            "Epoch 8, Step 27, Loss: 0.009868589229881763\n",
            "Epoch 8, Step 28, Loss: 0.010118582285940647\n",
            "Epoch 8, Step 29, Loss: 0.01090045366436243\n",
            "Epoch 8, Step 30, Loss: 0.008563458919525146\n",
            "Epoch 8, Step 31, Loss: 0.01160594541579485\n",
            "Epoch 8, Step 32, Loss: 0.009458823129534721\n",
            "Epoch 8, Step 33, Loss: 0.01051337644457817\n",
            "Epoch 8, Step 34, Loss: 0.009525940753519535\n",
            "Epoch 8, Step 35, Loss: 0.009089215658605099\n",
            "Epoch 8, Step 36, Loss: 0.009079543873667717\n",
            "Epoch 8, Step 37, Loss: 0.009040774777531624\n",
            "Epoch 8, Step 38, Loss: 0.007821724750101566\n",
            "Epoch 8, Step 39, Loss: 0.007757024373859167\n",
            "Epoch 8, Step 40, Loss: 0.008021188899874687\n",
            "Epoch 8, Step 41, Loss: 0.006571255624294281\n",
            "Epoch 8, Step 42, Loss: 0.0060858107171952724\n",
            "Epoch 8, Step 43, Loss: 0.006097504403442144\n",
            "Epoch 8, Step 44, Loss: 0.0061640082858502865\n",
            "Epoch 8, Step 45, Loss: 0.006908206734806299\n",
            "Epoch 8, Step 46, Loss: 0.006322893779724836\n",
            "Epoch 8, Step 47, Loss: 0.0066336989402771\n",
            "Epoch 8, Step 48, Loss: 0.006245585158467293\n",
            "Epoch 8, Step 49, Loss: 0.00638297013938427\n",
            "Epoch 8, Step 50, Loss: 0.0057879420928657055\n",
            "Epoch 8, Step 51, Loss: 0.006281981244683266\n",
            "Epoch 8, Step 52, Loss: 0.005616785027086735\n",
            "Epoch 8, Step 53, Loss: 0.007101715076714754\n",
            "Epoch 8, Step 54, Loss: 0.006011391989886761\n",
            "Epoch 8, Step 55, Loss: 0.006260617636144161\n",
            "Epoch 8, Step 56, Loss: 0.007212843280285597\n",
            "Epoch 8, Step 57, Loss: 0.006638745311647654\n",
            "Epoch 8, Step 58, Loss: 0.007022997364401817\n",
            "Epoch 8, Step 59, Loss: 0.006145001854747534\n",
            "Epoch 8, Step 60, Loss: 0.006508675869554281\n",
            "Epoch 8, Step 61, Loss: 0.008317067287862301\n",
            "Epoch 8, Step 62, Loss: 0.006339573301374912\n",
            "Epoch 8, Step 63, Loss: 0.013161279261112213\n",
            "Epoch 8, Step 64, Loss: 0.007164034526795149\n",
            "Epoch 8, Step 65, Loss: 0.00656653568148613\n",
            "Epoch 8, Step 66, Loss: 0.006245260126888752\n",
            "Epoch 8, Step 67, Loss: 0.0070606013759970665\n",
            "Epoch 8, Step 68, Loss: 0.005968320183455944\n",
            "Epoch 8, Step 69, Loss: 0.006209626328200102\n",
            "Epoch 8, Step 70, Loss: 0.006685654167085886\n",
            "Epoch 8, Step 71, Loss: 0.0066850739531219006\n",
            "Epoch 8, Step 72, Loss: 0.005627290811389685\n",
            "Epoch 8, Step 73, Loss: 0.005572406109422445\n",
            "Epoch 8, Step 74, Loss: 0.005408663768321276\n",
            "Epoch 8, Step 75, Loss: 0.00565731106325984\n",
            "Epoch 8, Step 76, Loss: 0.005726065952330828\n",
            "Epoch 8, Step 77, Loss: 0.006071529351174831\n",
            "Epoch 8, Step 78, Loss: 0.005444755312055349\n",
            "Epoch 8, Step 79, Loss: 0.005591623019427061\n",
            "Epoch 8, Step 80, Loss: 0.005760510917752981\n",
            "Epoch 8, Step 81, Loss: 0.008823273703455925\n",
            "Epoch 8, Step 82, Loss: 0.006981926970183849\n",
            "Epoch 8, Step 83, Loss: 0.007325427141040564\n",
            "Epoch 8, Step 84, Loss: 0.008485345169901848\n",
            "Epoch 8, Step 85, Loss: 0.008080377243459225\n",
            "Epoch 8, Step 86, Loss: 0.0080273961648345\n",
            "Train Metric MRRs: 0.2703102594540633\n",
            "Train Metric MAPs: 0.3072633542864964\n",
            "Validation Metric MRRs: 0.21209607025328853\n",
            "Validation Metric MAPs: 0.22454418709881968\n",
            "Epoch 9, Step 1, Loss: 0.021525781601667404\n",
            "Epoch 9, Step 2, Loss: 0.02080237865447998\n",
            "Epoch 9, Step 3, Loss: 0.017599385231733322\n",
            "Epoch 9, Step 4, Loss: 0.014712641946971416\n",
            "Epoch 9, Step 5, Loss: 0.010308853350579739\n",
            "Epoch 9, Step 6, Loss: 0.00822164211422205\n",
            "Epoch 9, Step 7, Loss: 0.0062132952734827995\n",
            "Epoch 9, Step 8, Loss: 0.00622803857550025\n",
            "Epoch 9, Step 9, Loss: 0.005778511520475149\n",
            "Epoch 9, Step 10, Loss: 0.005726412869989872\n",
            "Epoch 9, Step 11, Loss: 0.006260921247303486\n",
            "Epoch 9, Step 12, Loss: 0.005792382638901472\n",
            "Epoch 9, Step 13, Loss: 0.005551591515541077\n",
            "Epoch 9, Step 14, Loss: 0.004783278796821833\n",
            "Epoch 9, Step 15, Loss: 0.0052563948556780815\n",
            "Epoch 9, Step 16, Loss: 0.005673498380929232\n",
            "Epoch 9, Step 17, Loss: 0.007142279762774706\n",
            "Epoch 9, Step 18, Loss: 0.008423986844718456\n",
            "Epoch 9, Step 19, Loss: 0.009906543418765068\n",
            "Epoch 9, Step 20, Loss: 0.0096508227288723\n",
            "Epoch 9, Step 21, Loss: 0.010169954039156437\n",
            "Epoch 9, Step 22, Loss: 0.010582340881228447\n",
            "Epoch 9, Step 23, Loss: 0.011111482977867126\n",
            "Epoch 9, Step 24, Loss: 0.012147369794547558\n",
            "Epoch 9, Step 25, Loss: 0.01226675696671009\n",
            "Epoch 9, Step 26, Loss: 0.012266404926776886\n",
            "Epoch 9, Step 27, Loss: 0.00966891460120678\n",
            "Epoch 9, Step 28, Loss: 0.009982151910662651\n",
            "Epoch 9, Step 29, Loss: 0.010798022150993347\n",
            "Epoch 9, Step 30, Loss: 0.008389230817556381\n",
            "Epoch 9, Step 31, Loss: 0.011446511372923851\n",
            "Epoch 9, Step 32, Loss: 0.009338232688605785\n",
            "Epoch 9, Step 33, Loss: 0.010371493175625801\n",
            "Epoch 9, Step 34, Loss: 0.00929408147931099\n",
            "Epoch 9, Step 35, Loss: 0.008962553925812244\n",
            "Epoch 9, Step 36, Loss: 0.00891833659261465\n",
            "Epoch 9, Step 37, Loss: 0.008942748419940472\n",
            "Epoch 9, Step 38, Loss: 0.007732130587100983\n",
            "Epoch 9, Step 39, Loss: 0.007728263270109892\n",
            "Epoch 9, Step 40, Loss: 0.007947839796543121\n",
            "Epoch 9, Step 41, Loss: 0.006474968511611223\n",
            "Epoch 9, Step 42, Loss: 0.00597546948119998\n",
            "Epoch 9, Step 43, Loss: 0.005974017083644867\n",
            "Epoch 9, Step 44, Loss: 0.006063942331820726\n",
            "Epoch 9, Step 45, Loss: 0.006819336209446192\n",
            "Epoch 9, Step 46, Loss: 0.006239904090762138\n",
            "Epoch 9, Step 47, Loss: 0.006501316092908382\n",
            "Epoch 9, Step 48, Loss: 0.006065702997148037\n",
            "Epoch 9, Step 49, Loss: 0.006212413311004639\n",
            "Epoch 9, Step 50, Loss: 0.005544495768845081\n",
            "Epoch 9, Step 51, Loss: 0.005682948511093855\n",
            "Epoch 9, Step 52, Loss: 0.005512479692697525\n",
            "Epoch 9, Step 53, Loss: 0.007053466979414225\n",
            "Epoch 9, Step 54, Loss: 0.00589359737932682\n",
            "Epoch 9, Step 55, Loss: 0.006213471293449402\n",
            "Epoch 9, Step 56, Loss: 0.00709518464282155\n",
            "Epoch 9, Step 57, Loss: 0.006578327156603336\n",
            "Epoch 9, Step 58, Loss: 0.006863198708742857\n",
            "Epoch 9, Step 59, Loss: 0.00608748197555542\n",
            "Epoch 9, Step 60, Loss: 0.006462386809289455\n",
            "Epoch 9, Step 61, Loss: 0.008239666931331158\n",
            "Epoch 9, Step 62, Loss: 0.006243925541639328\n",
            "Epoch 9, Step 63, Loss: 0.0128835029900074\n",
            "Epoch 9, Step 64, Loss: 0.006744817830622196\n",
            "Epoch 9, Step 65, Loss: 0.00640914449468255\n",
            "Epoch 9, Step 66, Loss: 0.005987691693007946\n",
            "Epoch 9, Step 67, Loss: 0.006763685494661331\n",
            "Epoch 9, Step 68, Loss: 0.0057547613978385925\n",
            "Epoch 9, Step 69, Loss: 0.006068041548132896\n",
            "Epoch 9, Step 70, Loss: 0.0063755870796740055\n",
            "Epoch 9, Step 71, Loss: 0.006604363210499287\n",
            "Epoch 9, Step 72, Loss: 0.0054408954456448555\n",
            "Epoch 9, Step 73, Loss: 0.005484350025653839\n",
            "Epoch 9, Step 74, Loss: 0.005274672526866198\n",
            "Epoch 9, Step 75, Loss: 0.005431905388832092\n",
            "Epoch 9, Step 76, Loss: 0.0055641354992985725\n",
            "Epoch 9, Step 77, Loss: 0.005916888359934092\n",
            "Epoch 9, Step 78, Loss: 0.005284807179123163\n",
            "Epoch 9, Step 79, Loss: 0.005476105026900768\n",
            "Epoch 9, Step 80, Loss: 0.005664867348968983\n",
            "Epoch 9, Step 81, Loss: 0.008744033053517342\n",
            "Epoch 9, Step 82, Loss: 0.006944595370441675\n",
            "Epoch 9, Step 83, Loss: 0.007245970424264669\n",
            "Epoch 9, Step 84, Loss: 0.008429056033492088\n",
            "Epoch 9, Step 85, Loss: 0.00811790768057108\n",
            "Epoch 9, Step 86, Loss: 0.00800991803407669\n",
            "Train Metric MRRs: 0.277134525287643\n",
            "Train Metric MAPs: 0.31142523016517754\n",
            "Validation Metric MRRs: 0.21045732112156715\n",
            "Validation Metric MAPs: 0.224044864684469\n",
            "Epoch 10, Step 1, Loss: 0.021362369880080223\n",
            "Epoch 10, Step 2, Loss: 0.020547913387417793\n",
            "Epoch 10, Step 3, Loss: 0.017247838899493217\n",
            "Epoch 10, Step 4, Loss: 0.014530572108924389\n",
            "Epoch 10, Step 5, Loss: 0.010247516445815563\n",
            "Epoch 10, Step 6, Loss: 0.008166836574673653\n",
            "Epoch 10, Step 7, Loss: 0.00607310701161623\n",
            "Epoch 10, Step 8, Loss: 0.0061668772250413895\n",
            "Epoch 10, Step 9, Loss: 0.005688350182026625\n",
            "Epoch 10, Step 10, Loss: 0.005633779801428318\n",
            "Epoch 10, Step 11, Loss: 0.0062118773348629475\n",
            "Epoch 10, Step 12, Loss: 0.005602057557553053\n",
            "Epoch 10, Step 13, Loss: 0.005448333453387022\n",
            "Epoch 10, Step 14, Loss: 0.004722165875136852\n",
            "Epoch 10, Step 15, Loss: 0.00532270735129714\n",
            "Epoch 10, Step 16, Loss: 0.0056975409388542175\n",
            "Epoch 10, Step 17, Loss: 0.006938299164175987\n",
            "Epoch 10, Step 18, Loss: 0.008307035081088543\n",
            "Epoch 10, Step 19, Loss: 0.009768329560756683\n",
            "Epoch 10, Step 20, Loss: 0.009607493877410889\n",
            "Epoch 10, Step 21, Loss: 0.010085195302963257\n",
            "Epoch 10, Step 22, Loss: 0.010372776538133621\n",
            "Epoch 10, Step 23, Loss: 0.011017720215022564\n",
            "Epoch 10, Step 24, Loss: 0.012050679884850979\n",
            "Epoch 10, Step 25, Loss: 0.012060292065143585\n",
            "Epoch 10, Step 26, Loss: 0.012097856029868126\n",
            "Epoch 10, Step 27, Loss: 0.00947466492652893\n",
            "Epoch 10, Step 28, Loss: 0.009841134771704674\n",
            "Epoch 10, Step 29, Loss: 0.010679797269403934\n",
            "Epoch 10, Step 30, Loss: 0.008319302462041378\n",
            "Epoch 10, Step 31, Loss: 0.01117327157407999\n",
            "Epoch 10, Step 32, Loss: 0.009285345673561096\n",
            "Epoch 10, Step 33, Loss: 0.0103091886267066\n",
            "Epoch 10, Step 34, Loss: 0.009193403646349907\n",
            "Epoch 10, Step 35, Loss: 0.008901395834982395\n",
            "Epoch 10, Step 36, Loss: 0.008806655183434486\n",
            "Epoch 10, Step 37, Loss: 0.008828730322420597\n",
            "Epoch 10, Step 38, Loss: 0.007716910447925329\n",
            "Epoch 10, Step 39, Loss: 0.007600977551192045\n",
            "Epoch 10, Step 40, Loss: 0.007907993160188198\n",
            "Epoch 10, Step 41, Loss: 0.006482298951596022\n",
            "Epoch 10, Step 42, Loss: 0.005897708237171173\n",
            "Epoch 10, Step 43, Loss: 0.005885594990104437\n",
            "Epoch 10, Step 44, Loss: 0.005964853800833225\n",
            "Epoch 10, Step 45, Loss: 0.006751391105353832\n",
            "Epoch 10, Step 46, Loss: 0.006179885473102331\n",
            "Epoch 10, Step 47, Loss: 0.006399518810212612\n",
            "Epoch 10, Step 48, Loss: 0.005958241876214743\n",
            "Epoch 10, Step 49, Loss: 0.006070426199585199\n",
            "Epoch 10, Step 50, Loss: 0.0053786179050803185\n",
            "Epoch 10, Step 51, Loss: 0.005621036048978567\n",
            "Epoch 10, Step 52, Loss: 0.005440685898065567\n",
            "Epoch 10, Step 53, Loss: 0.006990861147642136\n",
            "Epoch 10, Step 54, Loss: 0.005809596739709377\n",
            "Epoch 10, Step 55, Loss: 0.006186987739056349\n",
            "Epoch 10, Step 56, Loss: 0.006982673890888691\n",
            "Epoch 10, Step 57, Loss: 0.006509602535516024\n",
            "Epoch 10, Step 58, Loss: 0.0067543392069637775\n",
            "Epoch 10, Step 59, Loss: 0.006042656488716602\n",
            "Epoch 10, Step 60, Loss: 0.006439420394599438\n",
            "Epoch 10, Step 61, Loss: 0.008200283162295818\n",
            "Epoch 10, Step 62, Loss: 0.006201152224093676\n",
            "Epoch 10, Step 63, Loss: 0.012527993880212307\n",
            "Epoch 10, Step 64, Loss: 0.006558881141245365\n",
            "Epoch 10, Step 65, Loss: 0.006330446805804968\n",
            "Epoch 10, Step 66, Loss: 0.005913224071264267\n",
            "Epoch 10, Step 67, Loss: 0.006686010863631964\n",
            "Epoch 10, Step 68, Loss: 0.005665850825607777\n",
            "Epoch 10, Step 69, Loss: 0.0059130191802978516\n",
            "Epoch 10, Step 70, Loss: 0.006360729690641165\n",
            "Epoch 10, Step 71, Loss: 0.006512795574963093\n",
            "Epoch 10, Step 72, Loss: 0.005376915913075209\n",
            "Epoch 10, Step 73, Loss: 0.005437084939330816\n",
            "Epoch 10, Step 74, Loss: 0.005245610140264034\n",
            "Epoch 10, Step 75, Loss: 0.005363537929952145\n",
            "Epoch 10, Step 76, Loss: 0.005481705069541931\n",
            "Epoch 10, Step 77, Loss: 0.005867844447493553\n",
            "Epoch 10, Step 78, Loss: 0.005166297312825918\n",
            "Epoch 10, Step 79, Loss: 0.005386486183851957\n",
            "Epoch 10, Step 80, Loss: 0.005596780683845282\n",
            "Epoch 10, Step 81, Loss: 0.008580110035836697\n",
            "Epoch 10, Step 82, Loss: 0.006891096942126751\n",
            "Epoch 10, Step 83, Loss: 0.0072025335393846035\n",
            "Epoch 10, Step 84, Loss: 0.008242039009928703\n",
            "Epoch 10, Step 85, Loss: 0.008036017417907715\n",
            "Epoch 10, Step 86, Loss: 0.007947178557515144\n",
            "Train Metric MRRs: 0.2819286721193892\n",
            "Train Metric MAPs: 0.31502755096724794\n",
            "Validation Metric MRRs: 0.21012576099070684\n",
            "Validation Metric MAPs: 0.22561248891408056\n",
            "Epoch 11, Step 1, Loss: 0.020574266090989113\n",
            "Epoch 11, Step 2, Loss: 0.020439384505152702\n",
            "Epoch 11, Step 3, Loss: 0.01710534282028675\n",
            "Epoch 11, Step 4, Loss: 0.014443513005971909\n",
            "Epoch 11, Step 5, Loss: 0.010200917720794678\n",
            "Epoch 11, Step 6, Loss: 0.008128106594085693\n",
            "Epoch 11, Step 7, Loss: 0.006000280845910311\n",
            "Epoch 11, Step 8, Loss: 0.006070012226700783\n",
            "Epoch 11, Step 9, Loss: 0.005584400147199631\n",
            "Epoch 11, Step 10, Loss: 0.005555890500545502\n",
            "Epoch 11, Step 11, Loss: 0.006072494201362133\n",
            "Epoch 11, Step 12, Loss: 0.005479106213897467\n",
            "Epoch 11, Step 13, Loss: 0.005300228949636221\n",
            "Epoch 11, Step 14, Loss: 0.004632945172488689\n",
            "Epoch 11, Step 15, Loss: 0.00522558344528079\n",
            "Epoch 11, Step 16, Loss: 0.005565516650676727\n",
            "Epoch 11, Step 17, Loss: 0.0067813885398209095\n",
            "Epoch 11, Step 18, Loss: 0.008185241371393204\n",
            "Epoch 11, Step 19, Loss: 0.009639554657042027\n",
            "Epoch 11, Step 20, Loss: 0.009412970393896103\n",
            "Epoch 11, Step 21, Loss: 0.010034652426838875\n",
            "Epoch 11, Step 22, Loss: 0.010198069736361504\n",
            "Epoch 11, Step 23, Loss: 0.010973045602440834\n",
            "Epoch 11, Step 24, Loss: 0.011749718338251114\n",
            "Epoch 11, Step 25, Loss: 0.011853284202516079\n",
            "Epoch 11, Step 26, Loss: 0.012003922834992409\n",
            "Epoch 11, Step 27, Loss: 0.009335916489362717\n",
            "Epoch 11, Step 28, Loss: 0.009784694761037827\n",
            "Epoch 11, Step 29, Loss: 0.01056497823446989\n",
            "Epoch 11, Step 30, Loss: 0.008218277245759964\n",
            "Epoch 11, Step 31, Loss: 0.010997414588928223\n",
            "Epoch 11, Step 32, Loss: 0.009242895990610123\n",
            "Epoch 11, Step 33, Loss: 0.010217011906206608\n",
            "Epoch 11, Step 34, Loss: 0.009071129374206066\n",
            "Epoch 11, Step 35, Loss: 0.008839044719934464\n",
            "Epoch 11, Step 36, Loss: 0.008726139552891254\n",
            "Epoch 11, Step 37, Loss: 0.008759519085288048\n",
            "Epoch 11, Step 38, Loss: 0.0076456316746771336\n",
            "Epoch 11, Step 39, Loss: 0.0075316899456083775\n",
            "Epoch 11, Step 40, Loss: 0.007846756838262081\n",
            "Epoch 11, Step 41, Loss: 0.0064999558962881565\n",
            "Epoch 11, Step 42, Loss: 0.0058682686649262905\n",
            "Epoch 11, Step 43, Loss: 0.0058056204579770565\n",
            "Epoch 11, Step 44, Loss: 0.005850976333022118\n",
            "Epoch 11, Step 45, Loss: 0.006656623911112547\n",
            "Epoch 11, Step 46, Loss: 0.006103966850787401\n",
            "Epoch 11, Step 47, Loss: 0.006378860678523779\n",
            "Epoch 11, Step 48, Loss: 0.005856884643435478\n",
            "Epoch 11, Step 49, Loss: 0.005994569044560194\n",
            "Epoch 11, Step 50, Loss: 0.005304654594510794\n",
            "Epoch 11, Step 51, Loss: 0.005578908137977123\n",
            "Epoch 11, Step 52, Loss: 0.005411307793110609\n",
            "Epoch 11, Step 53, Loss: 0.006941003259271383\n",
            "Epoch 11, Step 54, Loss: 0.005732517223805189\n",
            "Epoch 11, Step 55, Loss: 0.0061715394258499146\n",
            "Epoch 11, Step 56, Loss: 0.006882650777697563\n",
            "Epoch 11, Step 57, Loss: 0.006453861948102713\n",
            "Epoch 11, Step 58, Loss: 0.006634025368839502\n",
            "Epoch 11, Step 59, Loss: 0.005982761271297932\n",
            "Epoch 11, Step 60, Loss: 0.006410510279238224\n",
            "Epoch 11, Step 61, Loss: 0.008110753260552883\n",
            "Epoch 11, Step 62, Loss: 0.00616291631013155\n",
            "Epoch 11, Step 63, Loss: 0.012136970646679401\n",
            "Epoch 11, Step 64, Loss: 0.00636336812749505\n",
            "Epoch 11, Step 65, Loss: 0.006265704985707998\n",
            "Epoch 11, Step 66, Loss: 0.005865604151040316\n",
            "Epoch 11, Step 67, Loss: 0.006582999601960182\n",
            "Epoch 11, Step 68, Loss: 0.00564685370773077\n",
            "Epoch 11, Step 69, Loss: 0.005812708754092455\n",
            "Epoch 11, Step 70, Loss: 0.0063064927235245705\n",
            "Epoch 11, Step 71, Loss: 0.0064622764475643635\n",
            "Epoch 11, Step 72, Loss: 0.005362019408494234\n",
            "Epoch 11, Step 73, Loss: 0.005423145834356546\n",
            "Epoch 11, Step 74, Loss: 0.005212370306253433\n",
            "Epoch 11, Step 75, Loss: 0.005329288076609373\n",
            "Epoch 11, Step 76, Loss: 0.005425982642918825\n",
            "Epoch 11, Step 77, Loss: 0.005842122249305248\n",
            "Epoch 11, Step 78, Loss: 0.0051122610457241535\n",
            "Epoch 11, Step 79, Loss: 0.005335468333214521\n",
            "Epoch 11, Step 80, Loss: 0.0055729239247739315\n",
            "Epoch 11, Step 81, Loss: 0.008455988019704819\n",
            "Epoch 11, Step 82, Loss: 0.006832350976765156\n",
            "Epoch 11, Step 83, Loss: 0.007145243231207132\n",
            "Epoch 11, Step 84, Loss: 0.00814907532185316\n",
            "Epoch 11, Step 85, Loss: 0.007978585548698902\n",
            "Epoch 11, Step 86, Loss: 0.007895796559751034\n",
            "Train Metric MRRs: 0.285634984909897\n",
            "Train Metric MAPs: 0.3178222253671372\n",
            "Validation Metric MRRs: 0.20892242485021398\n",
            "Validation Metric MAPs: 0.22563601942480618\n",
            "Epoch 12, Step 1, Loss: 0.020241977646946907\n",
            "Epoch 12, Step 2, Loss: 0.020224716514348984\n",
            "Epoch 12, Step 3, Loss: 0.016859114170074463\n",
            "Epoch 12, Step 4, Loss: 0.01431525032967329\n",
            "Epoch 12, Step 5, Loss: 0.010173930786550045\n",
            "Epoch 12, Step 6, Loss: 0.00804470106959343\n",
            "Epoch 12, Step 7, Loss: 0.005960489623248577\n",
            "Epoch 12, Step 8, Loss: 0.006014997139573097\n",
            "Epoch 12, Step 9, Loss: 0.005562076345086098\n",
            "Epoch 12, Step 10, Loss: 0.005518125370144844\n",
            "Epoch 12, Step 11, Loss: 0.006013698875904083\n",
            "Epoch 12, Step 12, Loss: 0.005440237931907177\n",
            "Epoch 12, Step 13, Loss: 0.005244016647338867\n",
            "Epoch 12, Step 14, Loss: 0.00458144024014473\n",
            "Epoch 12, Step 15, Loss: 0.005306803155690432\n",
            "Epoch 12, Step 16, Loss: 0.00554582430049777\n",
            "Epoch 12, Step 17, Loss: 0.006627826485782862\n",
            "Epoch 12, Step 18, Loss: 0.008063183166086674\n",
            "Epoch 12, Step 19, Loss: 0.009563070721924305\n",
            "Epoch 12, Step 20, Loss: 0.009334336966276169\n",
            "Epoch 12, Step 21, Loss: 0.009904665872454643\n",
            "Epoch 12, Step 22, Loss: 0.010155736468732357\n",
            "Epoch 12, Step 23, Loss: 0.010847853496670723\n",
            "Epoch 12, Step 24, Loss: 0.011722182855010033\n",
            "Epoch 12, Step 25, Loss: 0.01180201768875122\n",
            "Epoch 12, Step 26, Loss: 0.01189136877655983\n",
            "Epoch 12, Step 27, Loss: 0.009226786904036999\n",
            "Epoch 12, Step 28, Loss: 0.009773563593626022\n",
            "Epoch 12, Step 29, Loss: 0.01044949796050787\n",
            "Epoch 12, Step 30, Loss: 0.008248183876276016\n",
            "Epoch 12, Step 31, Loss: 0.010859043337404728\n",
            "Epoch 12, Step 32, Loss: 0.009153525345027447\n",
            "Epoch 12, Step 33, Loss: 0.010170047171413898\n",
            "Epoch 12, Step 34, Loss: 0.009025410749018192\n",
            "Epoch 12, Step 35, Loss: 0.008796066045761108\n",
            "Epoch 12, Step 36, Loss: 0.00861829798668623\n",
            "Epoch 12, Step 37, Loss: 0.008672942407429218\n",
            "Epoch 12, Step 38, Loss: 0.007669184356927872\n",
            "Epoch 12, Step 39, Loss: 0.007475343998521566\n",
            "Epoch 12, Step 40, Loss: 0.007793061435222626\n",
            "Epoch 12, Step 41, Loss: 0.006407685577869415\n",
            "Epoch 12, Step 42, Loss: 0.005822503007948399\n",
            "Epoch 12, Step 43, Loss: 0.0057348706759512424\n",
            "Epoch 12, Step 44, Loss: 0.005803307518362999\n",
            "Epoch 12, Step 45, Loss: 0.0066419788636267185\n",
            "Epoch 12, Step 46, Loss: 0.006044136360287666\n",
            "Epoch 12, Step 47, Loss: 0.006342035252600908\n",
            "Epoch 12, Step 48, Loss: 0.005823160987347364\n",
            "Epoch 12, Step 49, Loss: 0.0058927638456225395\n",
            "Epoch 12, Step 50, Loss: 0.005285289604216814\n",
            "Epoch 12, Step 51, Loss: 0.005551447160542011\n",
            "Epoch 12, Step 52, Loss: 0.005387251265347004\n",
            "Epoch 12, Step 53, Loss: 0.006932350341230631\n",
            "Epoch 12, Step 54, Loss: 0.0056343674659729\n",
            "Epoch 12, Step 55, Loss: 0.006161227356642485\n",
            "Epoch 12, Step 56, Loss: 0.0067944033071398735\n",
            "Epoch 12, Step 57, Loss: 0.006448791828006506\n",
            "Epoch 12, Step 58, Loss: 0.006513252388685942\n",
            "Epoch 12, Step 59, Loss: 0.0059652309864759445\n",
            "Epoch 12, Step 60, Loss: 0.006381109356880188\n",
            "Epoch 12, Step 61, Loss: 0.008050664328038692\n",
            "Epoch 12, Step 62, Loss: 0.006121248006820679\n",
            "Epoch 12, Step 63, Loss: 0.011681361123919487\n",
            "Epoch 12, Step 64, Loss: 0.0061865621246397495\n",
            "Epoch 12, Step 65, Loss: 0.006177652161568403\n",
            "Epoch 12, Step 66, Loss: 0.005810392554849386\n",
            "Epoch 12, Step 67, Loss: 0.0064932783134281635\n",
            "Epoch 12, Step 68, Loss: 0.005569897126406431\n",
            "Epoch 12, Step 69, Loss: 0.005728193558752537\n",
            "Epoch 12, Step 70, Loss: 0.006329028867185116\n",
            "Epoch 12, Step 71, Loss: 0.006406040396541357\n",
            "Epoch 12, Step 72, Loss: 0.005332694388926029\n",
            "Epoch 12, Step 73, Loss: 0.005385855212807655\n",
            "Epoch 12, Step 74, Loss: 0.005188623908907175\n",
            "Epoch 12, Step 75, Loss: 0.005283256061375141\n",
            "Epoch 12, Step 76, Loss: 0.005396675318479538\n",
            "Epoch 12, Step 77, Loss: 0.005807583685964346\n",
            "Epoch 12, Step 78, Loss: 0.005053493194282055\n",
            "Epoch 12, Step 79, Loss: 0.0052961851470172405\n",
            "Epoch 12, Step 80, Loss: 0.00555918226018548\n",
            "Epoch 12, Step 81, Loss: 0.008350780233740807\n",
            "Epoch 12, Step 82, Loss: 0.006810097489506006\n",
            "Epoch 12, Step 83, Loss: 0.0071339718997478485\n",
            "Epoch 12, Step 84, Loss: 0.008030884899199009\n",
            "Epoch 12, Step 85, Loss: 0.007894979789853096\n",
            "Epoch 12, Step 86, Loss: 0.007818690501153469\n",
            "Train Metric MRRs: 0.2890084874073569\n",
            "Train Metric MAPs: 0.3202731239855662\n",
            "Validation Metric MRRs: 0.2103784774741578\n",
            "Validation Metric MAPs: 0.22656295642492394\n",
            "Epoch 13, Step 1, Loss: 0.019863102585077286\n",
            "Epoch 13, Step 2, Loss: 0.01995880715548992\n",
            "Epoch 13, Step 3, Loss: 0.0167433712631464\n",
            "Epoch 13, Step 4, Loss: 0.014281580224633217\n",
            "Epoch 13, Step 5, Loss: 0.010140953585505486\n",
            "Epoch 13, Step 6, Loss: 0.007992291823029518\n",
            "Epoch 13, Step 7, Loss: 0.005946808494627476\n",
            "Epoch 13, Step 8, Loss: 0.005952127743512392\n",
            "Epoch 13, Step 9, Loss: 0.0055525717325508595\n",
            "Epoch 13, Step 10, Loss: 0.005487639922648668\n",
            "Epoch 13, Step 11, Loss: 0.00590298417955637\n",
            "Epoch 13, Step 12, Loss: 0.005373533349484205\n",
            "Epoch 13, Step 13, Loss: 0.005199998151510954\n",
            "Epoch 13, Step 14, Loss: 0.004487270954996347\n",
            "Epoch 13, Step 15, Loss: 0.005175900645554066\n",
            "Epoch 13, Step 16, Loss: 0.005440579727292061\n",
            "Epoch 13, Step 17, Loss: 0.006517247296869755\n",
            "Epoch 13, Step 18, Loss: 0.008012141101062298\n",
            "Epoch 13, Step 19, Loss: 0.009515571407973766\n",
            "Epoch 13, Step 20, Loss: 0.009158107452094555\n",
            "Epoch 13, Step 21, Loss: 0.009828008711338043\n",
            "Epoch 13, Step 22, Loss: 0.010063470341265202\n",
            "Epoch 13, Step 23, Loss: 0.010858744382858276\n",
            "Epoch 13, Step 24, Loss: 0.0115770623087883\n",
            "Epoch 13, Step 25, Loss: 0.011808217503130436\n",
            "Epoch 13, Step 26, Loss: 0.011723401956260204\n",
            "Epoch 13, Step 27, Loss: 0.009196195751428604\n",
            "Epoch 13, Step 28, Loss: 0.009736909531056881\n",
            "Epoch 13, Step 29, Loss: 0.010347394272685051\n",
            "Epoch 13, Step 30, Loss: 0.008172838017344475\n",
            "Epoch 13, Step 31, Loss: 0.010796916671097279\n",
            "Epoch 13, Step 32, Loss: 0.00912031251937151\n",
            "Epoch 13, Step 33, Loss: 0.010073792189359665\n",
            "Epoch 13, Step 34, Loss: 0.00893369596451521\n",
            "Epoch 13, Step 35, Loss: 0.008728543296456337\n",
            "Epoch 13, Step 36, Loss: 0.008535793051123619\n",
            "Epoch 13, Step 37, Loss: 0.008609703741967678\n",
            "Epoch 13, Step 38, Loss: 0.007592725567519665\n",
            "Epoch 13, Step 39, Loss: 0.007387673947960138\n",
            "Epoch 13, Step 40, Loss: 0.007748340256512165\n",
            "Epoch 13, Step 41, Loss: 0.006365561857819557\n",
            "Epoch 13, Step 42, Loss: 0.005825414322316647\n",
            "Epoch 13, Step 43, Loss: 0.005680064205080271\n",
            "Epoch 13, Step 44, Loss: 0.005739194806665182\n",
            "Epoch 13, Step 45, Loss: 0.006566802971065044\n",
            "Epoch 13, Step 46, Loss: 0.006004181690514088\n",
            "Epoch 13, Step 47, Loss: 0.006291064899414778\n",
            "Epoch 13, Step 48, Loss: 0.005779991392046213\n",
            "Epoch 13, Step 49, Loss: 0.0058103459887206554\n",
            "Epoch 13, Step 50, Loss: 0.005257658660411835\n",
            "Epoch 13, Step 51, Loss: 0.005552677903324366\n",
            "Epoch 13, Step 52, Loss: 0.005348855629563332\n",
            "Epoch 13, Step 53, Loss: 0.006894818972796202\n",
            "Epoch 13, Step 54, Loss: 0.005596890579909086\n",
            "Epoch 13, Step 55, Loss: 0.006157776340842247\n",
            "Epoch 13, Step 56, Loss: 0.00672740675508976\n",
            "Epoch 13, Step 57, Loss: 0.0063997674733400345\n",
            "Epoch 13, Step 58, Loss: 0.006449931766837835\n",
            "Epoch 13, Step 59, Loss: 0.005949906073510647\n",
            "Epoch 13, Step 60, Loss: 0.006324097979813814\n",
            "Epoch 13, Step 61, Loss: 0.008032861165702343\n",
            "Epoch 13, Step 62, Loss: 0.006085503846406937\n",
            "Epoch 13, Step 63, Loss: 0.01127884816378355\n",
            "Epoch 13, Step 64, Loss: 0.006167530082166195\n",
            "Epoch 13, Step 65, Loss: 0.006138066295534372\n",
            "Epoch 13, Step 66, Loss: 0.005811457522213459\n",
            "Epoch 13, Step 67, Loss: 0.006487149745225906\n",
            "Epoch 13, Step 68, Loss: 0.005553425755351782\n",
            "Epoch 13, Step 69, Loss: 0.005673632025718689\n",
            "Epoch 13, Step 70, Loss: 0.006324302405118942\n",
            "Epoch 13, Step 71, Loss: 0.006379661150276661\n",
            "Epoch 13, Step 72, Loss: 0.005324115045368671\n",
            "Epoch 13, Step 73, Loss: 0.005353554151952267\n",
            "Epoch 13, Step 74, Loss: 0.00516787962988019\n",
            "Epoch 13, Step 75, Loss: 0.005268999375402927\n",
            "Epoch 13, Step 76, Loss: 0.00537790497764945\n",
            "Epoch 13, Step 77, Loss: 0.0057876938953995705\n",
            "Epoch 13, Step 78, Loss: 0.0050391447730362415\n",
            "Epoch 13, Step 79, Loss: 0.005299966782331467\n",
            "Epoch 13, Step 80, Loss: 0.005554120056331158\n",
            "Epoch 13, Step 81, Loss: 0.008254904299974442\n",
            "Epoch 13, Step 82, Loss: 0.006811800878494978\n",
            "Epoch 13, Step 83, Loss: 0.00714285159483552\n",
            "Epoch 13, Step 84, Loss: 0.00796904694288969\n",
            "Epoch 13, Step 85, Loss: 0.007841445505619049\n",
            "Epoch 13, Step 86, Loss: 0.007822451181709766\n",
            "Train Metric MRRs: 0.29133209663057225\n",
            "Train Metric MAPs: 0.32322616284703515\n",
            "Validation Metric MRRs: 0.2096002993197492\n",
            "Validation Metric MAPs: 0.22656084279788016\n",
            "Epoch 14, Step 1, Loss: 0.0196077823638916\n",
            "Epoch 14, Step 2, Loss: 0.019841602072119713\n",
            "Epoch 14, Step 3, Loss: 0.016613343730568886\n",
            "Epoch 14, Step 4, Loss: 0.014178760349750519\n",
            "Epoch 14, Step 5, Loss: 0.010151492431759834\n",
            "Epoch 14, Step 6, Loss: 0.00794909056276083\n",
            "Epoch 14, Step 7, Loss: 0.005941618233919144\n",
            "Epoch 14, Step 8, Loss: 0.005919883493334055\n",
            "Epoch 14, Step 9, Loss: 0.005536595359444618\n",
            "Epoch 14, Step 10, Loss: 0.00545473163947463\n",
            "Epoch 14, Step 11, Loss: 0.0059133111499249935\n",
            "Epoch 14, Step 12, Loss: 0.0053502824157476425\n",
            "Epoch 14, Step 13, Loss: 0.00517995236441493\n",
            "Epoch 14, Step 14, Loss: 0.004429775755852461\n",
            "Epoch 14, Step 15, Loss: 0.0051719448529183865\n",
            "Epoch 14, Step 16, Loss: 0.00541025772690773\n",
            "Epoch 14, Step 17, Loss: 0.006476865615695715\n",
            "Epoch 14, Step 18, Loss: 0.0079912468791008\n",
            "Epoch 14, Step 19, Loss: 0.009493781253695488\n",
            "Epoch 14, Step 20, Loss: 0.009109513834118843\n",
            "Epoch 14, Step 21, Loss: 0.009759541600942612\n",
            "Epoch 14, Step 22, Loss: 0.009965308010578156\n",
            "Epoch 14, Step 23, Loss: 0.010667869821190834\n",
            "Epoch 14, Step 24, Loss: 0.011471468955278397\n",
            "Epoch 14, Step 25, Loss: 0.01167075801640749\n",
            "Epoch 14, Step 26, Loss: 0.011732840910553932\n",
            "Epoch 14, Step 27, Loss: 0.00918375700712204\n",
            "Epoch 14, Step 28, Loss: 0.009677158668637276\n",
            "Epoch 14, Step 29, Loss: 0.01034077163785696\n",
            "Epoch 14, Step 30, Loss: 0.008164645172655582\n",
            "Epoch 14, Step 31, Loss: 0.010681748390197754\n",
            "Epoch 14, Step 32, Loss: 0.009062172845005989\n",
            "Epoch 14, Step 33, Loss: 0.010018466040492058\n",
            "Epoch 14, Step 34, Loss: 0.009004163555800915\n",
            "Epoch 14, Step 35, Loss: 0.00869680568575859\n",
            "Epoch 14, Step 36, Loss: 0.0084858862683177\n",
            "Epoch 14, Step 37, Loss: 0.0085765914991498\n",
            "Epoch 14, Step 38, Loss: 0.00754642765969038\n",
            "Epoch 14, Step 39, Loss: 0.007334116846323013\n",
            "Epoch 14, Step 40, Loss: 0.007703578565269709\n",
            "Epoch 14, Step 41, Loss: 0.006312493234872818\n",
            "Epoch 14, Step 42, Loss: 0.005792053882032633\n",
            "Epoch 14, Step 43, Loss: 0.005682243499904871\n",
            "Epoch 14, Step 44, Loss: 0.005696824286133051\n",
            "Epoch 14, Step 45, Loss: 0.006569723132997751\n",
            "Epoch 14, Step 46, Loss: 0.005972897168248892\n",
            "Epoch 14, Step 47, Loss: 0.006273187231272459\n",
            "Epoch 14, Step 48, Loss: 0.005828286521136761\n",
            "Epoch 14, Step 49, Loss: 0.005762964487075806\n",
            "Epoch 14, Step 50, Loss: 0.005250940565019846\n",
            "Epoch 14, Step 51, Loss: 0.005538187455385923\n",
            "Epoch 14, Step 52, Loss: 0.005338899791240692\n",
            "Epoch 14, Step 53, Loss: 0.006860288325697184\n",
            "Epoch 14, Step 54, Loss: 0.0054961759597063065\n",
            "Epoch 14, Step 55, Loss: 0.006129064131528139\n",
            "Epoch 14, Step 56, Loss: 0.006695476360619068\n",
            "Epoch 14, Step 57, Loss: 0.006361990701407194\n",
            "Epoch 14, Step 58, Loss: 0.0064261294901371\n",
            "Epoch 14, Step 59, Loss: 0.0058928560465574265\n",
            "Epoch 14, Step 60, Loss: 0.006317992229014635\n",
            "Epoch 14, Step 61, Loss: 0.008021402172744274\n",
            "Epoch 14, Step 62, Loss: 0.006077520549297333\n",
            "Epoch 14, Step 63, Loss: 0.010891243815422058\n",
            "Epoch 14, Step 64, Loss: 0.006095023360103369\n",
            "Epoch 14, Step 65, Loss: 0.006113611161708832\n",
            "Epoch 14, Step 66, Loss: 0.005801782477647066\n",
            "Epoch 14, Step 67, Loss: 0.006420495454221964\n",
            "Epoch 14, Step 68, Loss: 0.00553197180852294\n",
            "Epoch 14, Step 69, Loss: 0.005638886243104935\n",
            "Epoch 14, Step 70, Loss: 0.006331297568976879\n",
            "Epoch 14, Step 71, Loss: 0.006347603630274534\n",
            "Epoch 14, Step 72, Loss: 0.005260811187326908\n",
            "Epoch 14, Step 73, Loss: 0.005360567010939121\n",
            "Epoch 14, Step 74, Loss: 0.005168762058019638\n",
            "Epoch 14, Step 75, Loss: 0.005232289433479309\n",
            "Epoch 14, Step 76, Loss: 0.005372513551265001\n",
            "Epoch 14, Step 77, Loss: 0.005813936237245798\n",
            "Epoch 14, Step 78, Loss: 0.005014961119741201\n",
            "Epoch 14, Step 79, Loss: 0.00527723366394639\n",
            "Epoch 14, Step 80, Loss: 0.005548103246837854\n",
            "Epoch 14, Step 81, Loss: 0.008247621357440948\n",
            "Epoch 14, Step 82, Loss: 0.006800997070968151\n",
            "Epoch 14, Step 83, Loss: 0.0071517666801810265\n",
            "Epoch 14, Step 84, Loss: 0.007926019839942455\n",
            "Epoch 14, Step 85, Loss: 0.007788848131895065\n",
            "Epoch 14, Step 86, Loss: 0.007734459824860096\n",
            "Train Metric MRRs: 0.29388423730748525\n",
            "Train Metric MAPs: 0.3253684967062599\n",
            "Validation Metric MRRs: 0.21346994120133417\n",
            "Validation Metric MAPs: 0.2290553134776074\n",
            "Epoch 15, Step 1, Loss: 0.020016849040985107\n",
            "Epoch 15, Step 2, Loss: 0.019622402265667915\n",
            "Epoch 15, Step 3, Loss: 0.016566885635256767\n",
            "Epoch 15, Step 4, Loss: 0.014138901606202126\n",
            "Epoch 15, Step 5, Loss: 0.010078287683427334\n",
            "Epoch 15, Step 6, Loss: 0.007890556938946247\n",
            "Epoch 15, Step 7, Loss: 0.005925738252699375\n",
            "Epoch 15, Step 8, Loss: 0.005877514835447073\n",
            "Epoch 15, Step 9, Loss: 0.005483456887304783\n",
            "Epoch 15, Step 10, Loss: 0.00542575865983963\n",
            "Epoch 15, Step 11, Loss: 0.005933051463216543\n",
            "Epoch 15, Step 12, Loss: 0.00531211169436574\n",
            "Epoch 15, Step 13, Loss: 0.0050503285601735115\n",
            "Epoch 15, Step 14, Loss: 0.0043186889961361885\n",
            "Epoch 15, Step 15, Loss: 0.005208837799727917\n",
            "Epoch 15, Step 16, Loss: 0.005425394978374243\n",
            "Epoch 15, Step 17, Loss: 0.006288453936576843\n",
            "Epoch 15, Step 18, Loss: 0.007861660793423653\n",
            "Epoch 15, Step 19, Loss: 0.009409389458596706\n",
            "Epoch 15, Step 20, Loss: 0.008975347504019737\n",
            "Epoch 15, Step 21, Loss: 0.009672729298472404\n",
            "Epoch 15, Step 22, Loss: 0.009921858087182045\n",
            "Epoch 15, Step 23, Loss: 0.010581419803202152\n",
            "Epoch 15, Step 24, Loss: 0.011464226059615612\n",
            "Epoch 15, Step 25, Loss: 0.01162710040807724\n",
            "Epoch 15, Step 26, Loss: 0.011321579106152058\n",
            "Epoch 15, Step 27, Loss: 0.009097522124648094\n",
            "Epoch 15, Step 28, Loss: 0.009717347100377083\n",
            "Epoch 15, Step 29, Loss: 0.010250245220959187\n",
            "Epoch 15, Step 30, Loss: 0.00821185763925314\n",
            "Epoch 15, Step 31, Loss: 0.010639394633471966\n",
            "Epoch 15, Step 32, Loss: 0.009081585332751274\n",
            "Epoch 15, Step 33, Loss: 0.009982074610888958\n",
            "Epoch 15, Step 34, Loss: 0.008902769535779953\n",
            "Epoch 15, Step 35, Loss: 0.008644848130643368\n",
            "Epoch 15, Step 36, Loss: 0.008447248488664627\n",
            "Epoch 15, Step 37, Loss: 0.008483831770718098\n",
            "Epoch 15, Step 38, Loss: 0.0075275651179254055\n",
            "Epoch 15, Step 39, Loss: 0.007225039415061474\n",
            "Epoch 15, Step 40, Loss: 0.007691527251154184\n",
            "Epoch 15, Step 41, Loss: 0.006336024031043053\n",
            "Epoch 15, Step 42, Loss: 0.005800415296107531\n",
            "Epoch 15, Step 43, Loss: 0.0056142681278288364\n",
            "Epoch 15, Step 44, Loss: 0.005710164550691843\n",
            "Epoch 15, Step 45, Loss: 0.006559501402080059\n",
            "Epoch 15, Step 46, Loss: 0.005975989159196615\n",
            "Epoch 15, Step 47, Loss: 0.006220882758498192\n",
            "Epoch 15, Step 48, Loss: 0.005722355097532272\n",
            "Epoch 15, Step 49, Loss: 0.0057444036938250065\n",
            "Epoch 15, Step 50, Loss: 0.005186564289033413\n",
            "Epoch 15, Step 51, Loss: 0.0055218227207660675\n",
            "Epoch 15, Step 52, Loss: 0.005308789666742086\n",
            "Epoch 15, Step 53, Loss: 0.006842814851552248\n",
            "Epoch 15, Step 54, Loss: 0.005466954782605171\n",
            "Epoch 15, Step 55, Loss: 0.006088661961257458\n",
            "Epoch 15, Step 56, Loss: 0.0066811577416956425\n",
            "Epoch 15, Step 57, Loss: 0.006334624718874693\n",
            "Epoch 15, Step 58, Loss: 0.006400516722351313\n",
            "Epoch 15, Step 59, Loss: 0.005809394177049398\n",
            "Epoch 15, Step 60, Loss: 0.0062559316866099834\n",
            "Epoch 15, Step 61, Loss: 0.007994074374437332\n",
            "Epoch 15, Step 62, Loss: 0.0060755349695682526\n",
            "Epoch 15, Step 63, Loss: 0.010517308488488197\n",
            "Epoch 15, Step 64, Loss: 0.006152069661766291\n",
            "Epoch 15, Step 65, Loss: 0.006025202106684446\n",
            "Epoch 15, Step 66, Loss: 0.00578210549429059\n",
            "Epoch 15, Step 67, Loss: 0.006364704109728336\n",
            "Epoch 15, Step 68, Loss: 0.0054558622650802135\n",
            "Epoch 15, Step 69, Loss: 0.005575793329626322\n",
            "Epoch 15, Step 70, Loss: 0.0062420982867479324\n",
            "Epoch 15, Step 71, Loss: 0.006339771207422018\n",
            "Epoch 15, Step 72, Loss: 0.0052939970046281815\n",
            "Epoch 15, Step 73, Loss: 0.005285060033202171\n",
            "Epoch 15, Step 74, Loss: 0.005142151843756437\n",
            "Epoch 15, Step 75, Loss: 0.005216644611209631\n",
            "Epoch 15, Step 76, Loss: 0.005311543587595224\n",
            "Epoch 15, Step 77, Loss: 0.005765993148088455\n",
            "Epoch 15, Step 78, Loss: 0.0049704331904649734\n",
            "Epoch 15, Step 79, Loss: 0.005252274218946695\n",
            "Epoch 15, Step 80, Loss: 0.0055166007950901985\n",
            "Epoch 15, Step 81, Loss: 0.008182449266314507\n",
            "Epoch 15, Step 82, Loss: 0.006758678704500198\n",
            "Epoch 15, Step 83, Loss: 0.00707736611366272\n",
            "Epoch 15, Step 84, Loss: 0.007768281735479832\n",
            "Epoch 15, Step 85, Loss: 0.007715555373579264\n",
            "Epoch 15, Step 86, Loss: 0.00771106593310833\n",
            "Train Metric MRRs: 0.2942652695697444\n",
            "Train Metric MAPs: 0.32889531082165163\n",
            "Validation Metric MRRs: 0.21336714760130004\n",
            "Validation Metric MAPs: 0.23005742570564808\n",
            "Epoch 16, Step 1, Loss: 0.019277794286608696\n",
            "Epoch 16, Step 2, Loss: 0.01957283727824688\n",
            "Epoch 16, Step 3, Loss: 0.016340378671884537\n",
            "Epoch 16, Step 4, Loss: 0.014137165620923042\n",
            "Epoch 16, Step 5, Loss: 0.010114537551999092\n",
            "Epoch 16, Step 6, Loss: 0.007854331284761429\n",
            "Epoch 16, Step 7, Loss: 0.005893617868423462\n",
            "Epoch 16, Step 8, Loss: 0.005831598769873381\n",
            "Epoch 16, Step 9, Loss: 0.005435829982161522\n",
            "Epoch 16, Step 10, Loss: 0.005345395766198635\n",
            "Epoch 16, Step 11, Loss: 0.005939162336289883\n",
            "Epoch 16, Step 12, Loss: 0.005235781893134117\n",
            "Epoch 16, Step 13, Loss: 0.005093482322990894\n",
            "Epoch 16, Step 14, Loss: 0.004306427203118801\n",
            "Epoch 16, Step 15, Loss: 0.00513512222096324\n",
            "Epoch 16, Step 16, Loss: 0.005397405941039324\n",
            "Epoch 16, Step 17, Loss: 0.0063280388712882996\n",
            "Epoch 16, Step 18, Loss: 0.007737539708614349\n",
            "Epoch 16, Step 19, Loss: 0.00935298204421997\n",
            "Epoch 16, Step 20, Loss: 0.008956072852015495\n",
            "Epoch 16, Step 21, Loss: 0.009616263210773468\n",
            "Epoch 16, Step 22, Loss: 0.009881732054054737\n",
            "Epoch 16, Step 23, Loss: 0.01052138116210699\n",
            "Epoch 16, Step 24, Loss: 0.011289755813777447\n",
            "Epoch 16, Step 25, Loss: 0.011425874195992947\n",
            "Epoch 16, Step 26, Loss: 0.011239467188715935\n",
            "Epoch 16, Step 27, Loss: 0.00900186225771904\n",
            "Epoch 16, Step 28, Loss: 0.009641419164836407\n",
            "Epoch 16, Step 29, Loss: 0.010122285224497318\n",
            "Epoch 16, Step 30, Loss: 0.00812937319278717\n",
            "Epoch 16, Step 31, Loss: 0.010595491155982018\n",
            "Epoch 16, Step 32, Loss: 0.009080929681658745\n",
            "Epoch 16, Step 33, Loss: 0.010018939152359962\n",
            "Epoch 16, Step 34, Loss: 0.008903667330741882\n",
            "Epoch 16, Step 35, Loss: 0.008614487946033478\n",
            "Epoch 16, Step 36, Loss: 0.008489459753036499\n",
            "Epoch 16, Step 37, Loss: 0.00842097494751215\n",
            "Epoch 16, Step 38, Loss: 0.007538484409451485\n",
            "Epoch 16, Step 39, Loss: 0.007256317883729935\n",
            "Epoch 16, Step 40, Loss: 0.007692225743085146\n",
            "Epoch 16, Step 41, Loss: 0.006285130511969328\n",
            "Epoch 16, Step 42, Loss: 0.005804358050227165\n",
            "Epoch 16, Step 43, Loss: 0.0055818031542003155\n",
            "Epoch 16, Step 44, Loss: 0.005747411400079727\n",
            "Epoch 16, Step 45, Loss: 0.006546382792294025\n",
            "Epoch 16, Step 46, Loss: 0.005955837666988373\n",
            "Epoch 16, Step 47, Loss: 0.006219194270670414\n",
            "Epoch 16, Step 48, Loss: 0.005730293691158295\n",
            "Epoch 16, Step 49, Loss: 0.005713304504752159\n",
            "Epoch 16, Step 50, Loss: 0.005207237787544727\n",
            "Epoch 16, Step 51, Loss: 0.005502045154571533\n",
            "Epoch 16, Step 52, Loss: 0.005316976457834244\n",
            "Epoch 16, Step 53, Loss: 0.006814275402575731\n",
            "Epoch 16, Step 54, Loss: 0.005439336411654949\n",
            "Epoch 16, Step 55, Loss: 0.0060862586833536625\n",
            "Epoch 16, Step 56, Loss: 0.00665181502699852\n",
            "Epoch 16, Step 57, Loss: 0.006296781823039055\n",
            "Epoch 16, Step 58, Loss: 0.00636416906490922\n",
            "Epoch 16, Step 59, Loss: 0.005822205916047096\n",
            "Epoch 16, Step 60, Loss: 0.006264170166105032\n",
            "Epoch 16, Step 61, Loss: 0.007979066111147404\n",
            "Epoch 16, Step 62, Loss: 0.006021385546773672\n",
            "Epoch 16, Step 63, Loss: 0.010204607620835304\n",
            "Epoch 16, Step 64, Loss: 0.0060437750071287155\n",
            "Epoch 16, Step 65, Loss: 0.006023446563631296\n",
            "Epoch 16, Step 66, Loss: 0.005734312813729048\n",
            "Epoch 16, Step 67, Loss: 0.006291199941188097\n",
            "Epoch 16, Step 68, Loss: 0.005413352511823177\n",
            "Epoch 16, Step 69, Loss: 0.005520234350115061\n",
            "Epoch 16, Step 70, Loss: 0.00657427404075861\n",
            "Epoch 16, Step 71, Loss: 0.006294203922152519\n",
            "Epoch 16, Step 72, Loss: 0.005169746465981007\n",
            "Epoch 16, Step 73, Loss: 0.00520720798522234\n",
            "Epoch 16, Step 74, Loss: 0.005143410991877317\n",
            "Epoch 16, Step 75, Loss: 0.005203958135098219\n",
            "Epoch 16, Step 76, Loss: 0.005284108687192202\n",
            "Epoch 16, Step 77, Loss: 0.00575700169429183\n",
            "Epoch 16, Step 78, Loss: 0.00494220620021224\n",
            "Epoch 16, Step 79, Loss: 0.005199506413191557\n",
            "Epoch 16, Step 80, Loss: 0.005490534473210573\n",
            "Epoch 16, Step 81, Loss: 0.008015425875782967\n",
            "Epoch 16, Step 82, Loss: 0.006792050786316395\n",
            "Epoch 16, Step 83, Loss: 0.007081042975187302\n",
            "Epoch 16, Step 84, Loss: 0.007649629842489958\n",
            "Epoch 16, Step 85, Loss: 0.007757034618407488\n",
            "Epoch 16, Step 86, Loss: 0.007677650079131126\n",
            "Train Metric MRRs: 0.29676491807471356\n",
            "Train Metric MAPs: 0.3302989065049229\n",
            "Validation Metric MRRs: 0.2138058940046334\n",
            "Validation Metric MAPs: 0.2321422846212153\n",
            "Epoch 17, Step 1, Loss: 0.01874106377363205\n",
            "Epoch 17, Step 2, Loss: 0.019417779520154\n",
            "Epoch 17, Step 3, Loss: 0.016190258786082268\n",
            "Epoch 17, Step 4, Loss: 0.01405447255820036\n",
            "Epoch 17, Step 5, Loss: 0.009974870830774307\n",
            "Epoch 17, Step 6, Loss: 0.007798988837748766\n",
            "Epoch 17, Step 7, Loss: 0.00590424844995141\n",
            "Epoch 17, Step 8, Loss: 0.005786570720374584\n",
            "Epoch 17, Step 9, Loss: 0.005420621484518051\n",
            "Epoch 17, Step 10, Loss: 0.005246104206889868\n",
            "Epoch 17, Step 11, Loss: 0.005904188379645348\n",
            "Epoch 17, Step 12, Loss: 0.00520605081692338\n",
            "Epoch 17, Step 13, Loss: 0.004957021679729223\n",
            "Epoch 17, Step 14, Loss: 0.004149911925196648\n",
            "Epoch 17, Step 15, Loss: 0.005088983103632927\n",
            "Epoch 17, Step 16, Loss: 0.0055062174797058105\n",
            "Epoch 17, Step 17, Loss: 0.006299106869846582\n",
            "Epoch 17, Step 18, Loss: 0.0078090280294418335\n",
            "Epoch 17, Step 19, Loss: 0.009382601827383041\n",
            "Epoch 17, Step 20, Loss: 0.00896273460239172\n",
            "Epoch 17, Step 21, Loss: 0.009624744765460491\n",
            "Epoch 17, Step 22, Loss: 0.009823821485042572\n",
            "Epoch 17, Step 23, Loss: 0.010507444851100445\n",
            "Epoch 17, Step 24, Loss: 0.011457020416855812\n",
            "Epoch 17, Step 25, Loss: 0.011460858397185802\n",
            "Epoch 17, Step 26, Loss: 0.011044817045331001\n",
            "Epoch 17, Step 27, Loss: 0.008994228206574917\n",
            "Epoch 17, Step 28, Loss: 0.009631146676838398\n",
            "Epoch 17, Step 29, Loss: 0.010143036022782326\n",
            "Epoch 17, Step 30, Loss: 0.008202179335057735\n",
            "Epoch 17, Step 31, Loss: 0.010511907748878002\n",
            "Epoch 17, Step 32, Loss: 0.00911592599004507\n",
            "Epoch 17, Step 33, Loss: 0.00987826008349657\n",
            "Epoch 17, Step 34, Loss: 0.008842087350785732\n",
            "Epoch 17, Step 35, Loss: 0.008639423176646233\n",
            "Epoch 17, Step 36, Loss: 0.008413191884756088\n",
            "Epoch 17, Step 37, Loss: 0.008383087813854218\n",
            "Epoch 17, Step 38, Loss: 0.007484307046979666\n",
            "Epoch 17, Step 39, Loss: 0.007177073508501053\n",
            "Epoch 17, Step 40, Loss: 0.007649905048310757\n",
            "Epoch 17, Step 41, Loss: 0.006278276909142733\n",
            "Epoch 17, Step 42, Loss: 0.005731662269681692\n",
            "Epoch 17, Step 43, Loss: 0.00557160098105669\n",
            "Epoch 17, Step 44, Loss: 0.0057024420239031315\n",
            "Epoch 17, Step 45, Loss: 0.006497231777757406\n",
            "Epoch 17, Step 46, Loss: 0.005944710224866867\n",
            "Epoch 17, Step 47, Loss: 0.006191793363541365\n",
            "Epoch 17, Step 48, Loss: 0.005666595418006182\n",
            "Epoch 17, Step 49, Loss: 0.0056825727224349976\n",
            "Epoch 17, Step 50, Loss: 0.005168921779841185\n",
            "Epoch 17, Step 51, Loss: 0.005515757016837597\n",
            "Epoch 17, Step 52, Loss: 0.005273500457406044\n",
            "Epoch 17, Step 53, Loss: 0.006856646854430437\n",
            "Epoch 17, Step 54, Loss: 0.005407839082181454\n",
            "Epoch 17, Step 55, Loss: 0.0061085643246769905\n",
            "Epoch 17, Step 56, Loss: 0.0066562858410179615\n",
            "Epoch 17, Step 57, Loss: 0.006297048181295395\n",
            "Epoch 17, Step 58, Loss: 0.006375439465045929\n",
            "Epoch 17, Step 59, Loss: 0.005756687838584185\n",
            "Epoch 17, Step 60, Loss: 0.0062372018583118916\n",
            "Epoch 17, Step 61, Loss: 0.007953315041959286\n",
            "Epoch 17, Step 62, Loss: 0.006077608093619347\n",
            "Epoch 17, Step 63, Loss: 0.010004619136452675\n",
            "Epoch 17, Step 64, Loss: 0.006108548492193222\n",
            "Epoch 17, Step 65, Loss: 0.006004484370350838\n",
            "Epoch 17, Step 66, Loss: 0.005762321874499321\n",
            "Epoch 17, Step 67, Loss: 0.00636962428689003\n",
            "Epoch 17, Step 68, Loss: 0.0054055736400187016\n",
            "Epoch 17, Step 69, Loss: 0.0054950956255197525\n",
            "Epoch 17, Step 70, Loss: 0.006182437762618065\n",
            "Epoch 17, Step 71, Loss: 0.006254356354475021\n",
            "Epoch 17, Step 72, Loss: 0.005107923876494169\n",
            "Epoch 17, Step 73, Loss: 0.005245768930763006\n",
            "Epoch 17, Step 74, Loss: 0.005198468919843435\n",
            "Epoch 17, Step 75, Loss: 0.0052084713242948055\n",
            "Epoch 17, Step 76, Loss: 0.005306530743837357\n",
            "Epoch 17, Step 77, Loss: 0.005774315446615219\n",
            "Epoch 17, Step 78, Loss: 0.004944974556565285\n",
            "Epoch 17, Step 79, Loss: 0.005195651203393936\n",
            "Epoch 17, Step 80, Loss: 0.005470525939017534\n",
            "Epoch 17, Step 81, Loss: 0.0079689621925354\n",
            "Epoch 17, Step 82, Loss: 0.006737315095961094\n",
            "Epoch 17, Step 83, Loss: 0.0070120347663760185\n",
            "Epoch 17, Step 84, Loss: 0.007605290040373802\n",
            "Epoch 17, Step 85, Loss: 0.0076752821914851665\n",
            "Epoch 17, Step 86, Loss: 0.00758408335968852\n",
            "Train Metric MRRs: 0.2979463341088757\n",
            "Train Metric MAPs: 0.33095875021323357\n",
            "Validation Metric MRRs: 0.21454326166685178\n",
            "Validation Metric MAPs: 0.23174167330207882\n",
            "Epoch 18, Step 1, Loss: 0.018467111513018608\n",
            "Epoch 18, Step 2, Loss: 0.019248344004154205\n",
            "Epoch 18, Step 3, Loss: 0.016115454956889153\n",
            "Epoch 18, Step 4, Loss: 0.013930159620940685\n",
            "Epoch 18, Step 5, Loss: 0.009966225363314152\n",
            "Epoch 18, Step 6, Loss: 0.0077440692111849785\n",
            "Epoch 18, Step 7, Loss: 0.005867570172995329\n",
            "Epoch 18, Step 8, Loss: 0.005781817715615034\n",
            "Epoch 18, Step 9, Loss: 0.0053987447172403336\n",
            "Epoch 18, Step 10, Loss: 0.005274749826639891\n",
            "Epoch 18, Step 11, Loss: 0.005960918962955475\n",
            "Epoch 18, Step 12, Loss: 0.005176437087357044\n",
            "Epoch 18, Step 13, Loss: 0.005030733998864889\n",
            "Epoch 18, Step 14, Loss: 0.004071393515914679\n",
            "Epoch 18, Step 15, Loss: 0.005036517046391964\n",
            "Epoch 18, Step 16, Loss: 0.0054387375712394714\n",
            "Epoch 18, Step 17, Loss: 0.006186152342706919\n",
            "Epoch 18, Step 18, Loss: 0.007633162196725607\n",
            "Epoch 18, Step 19, Loss: 0.008939838968217373\n",
            "Epoch 18, Step 20, Loss: 0.00882529653608799\n",
            "Epoch 18, Step 21, Loss: 0.009565388783812523\n",
            "Epoch 18, Step 22, Loss: 0.009797262027859688\n",
            "Epoch 18, Step 23, Loss: 0.010369179770350456\n",
            "Epoch 18, Step 24, Loss: 0.011241083964705467\n",
            "Epoch 18, Step 25, Loss: 0.011376258917152882\n",
            "Epoch 18, Step 26, Loss: 0.010975761339068413\n",
            "Epoch 18, Step 27, Loss: 0.00892971083521843\n",
            "Epoch 18, Step 28, Loss: 0.009572423994541168\n",
            "Epoch 18, Step 29, Loss: 0.01005836296826601\n",
            "Epoch 18, Step 30, Loss: 0.008144394494593143\n",
            "Epoch 18, Step 31, Loss: 0.010427488945424557\n",
            "Epoch 18, Step 32, Loss: 0.009058876894414425\n",
            "Epoch 18, Step 33, Loss: 0.009884380735456944\n",
            "Epoch 18, Step 34, Loss: 0.008834954351186752\n",
            "Epoch 18, Step 35, Loss: 0.008545923978090286\n",
            "Epoch 18, Step 36, Loss: 0.008405808359384537\n",
            "Epoch 18, Step 37, Loss: 0.008312174119055271\n",
            "Epoch 18, Step 38, Loss: 0.0074350954964756966\n",
            "Epoch 18, Step 39, Loss: 0.00713987834751606\n",
            "Epoch 18, Step 40, Loss: 0.007617463357746601\n",
            "Epoch 18, Step 41, Loss: 0.006234429776668549\n",
            "Epoch 18, Step 42, Loss: 0.005716703832149506\n",
            "Epoch 18, Step 43, Loss: 0.005523290019482374\n",
            "Epoch 18, Step 44, Loss: 0.0056825983338057995\n",
            "Epoch 18, Step 45, Loss: 0.0064630345441401005\n",
            "Epoch 18, Step 46, Loss: 0.005866944789886475\n",
            "Epoch 18, Step 47, Loss: 0.00616323621943593\n",
            "Epoch 18, Step 48, Loss: 0.005656376946717501\n",
            "Epoch 18, Step 49, Loss: 0.005646122619509697\n",
            "Epoch 18, Step 50, Loss: 0.005141218658536673\n",
            "Epoch 18, Step 51, Loss: 0.005490284413099289\n",
            "Epoch 18, Step 52, Loss: 0.005289060529321432\n",
            "Epoch 18, Step 53, Loss: 0.006784709170460701\n",
            "Epoch 18, Step 54, Loss: 0.005376786459237337\n",
            "Epoch 18, Step 55, Loss: 0.00612713024020195\n",
            "Epoch 18, Step 56, Loss: 0.006632714997977018\n",
            "Epoch 18, Step 57, Loss: 0.006268132012337446\n",
            "Epoch 18, Step 58, Loss: 0.006350561045110226\n",
            "Epoch 18, Step 59, Loss: 0.00575229711830616\n",
            "Epoch 18, Step 60, Loss: 0.006298756692558527\n",
            "Epoch 18, Step 61, Loss: 0.007924709469079971\n",
            "Epoch 18, Step 62, Loss: 0.006041347049176693\n",
            "Epoch 18, Step 63, Loss: 0.009820395149290562\n",
            "Epoch 18, Step 64, Loss: 0.005965747404843569\n",
            "Epoch 18, Step 65, Loss: 0.005998407490551472\n",
            "Epoch 18, Step 66, Loss: 0.005684133619070053\n",
            "Epoch 18, Step 67, Loss: 0.006216059438884258\n",
            "Epoch 18, Step 68, Loss: 0.005349115002900362\n",
            "Epoch 18, Step 69, Loss: 0.005444779526442289\n",
            "Epoch 18, Step 70, Loss: 0.006173179019242525\n",
            "Epoch 18, Step 71, Loss: 0.006261587142944336\n",
            "Epoch 18, Step 72, Loss: 0.005086974240839481\n",
            "Epoch 18, Step 73, Loss: 0.0052468543872237206\n",
            "Epoch 18, Step 74, Loss: 0.005150421988219023\n",
            "Epoch 18, Step 75, Loss: 0.005164865404367447\n",
            "Epoch 18, Step 76, Loss: 0.005228930618613958\n",
            "Epoch 18, Step 77, Loss: 0.005754443816840649\n",
            "Epoch 18, Step 78, Loss: 0.004906320478767157\n",
            "Epoch 18, Step 79, Loss: 0.005211799871176481\n",
            "Epoch 18, Step 80, Loss: 0.005416676867753267\n",
            "Epoch 18, Step 81, Loss: 0.007865894585847855\n",
            "Epoch 18, Step 82, Loss: 0.006728570442646742\n",
            "Epoch 18, Step 83, Loss: 0.006941865663975477\n",
            "Epoch 18, Step 84, Loss: 0.007480897009372711\n",
            "Epoch 18, Step 85, Loss: 0.00769788445904851\n",
            "Epoch 18, Step 86, Loss: 0.007567468099296093\n",
            "Train Metric MRRs: 0.2999030654920306\n",
            "Train Metric MAPs: 0.3345255823342846\n",
            "Validation Metric MRRs: 0.2153835320584412\n",
            "Validation Metric MAPs: 0.23239876050653338\n",
            "Epoch 19, Step 1, Loss: 0.018142875283956528\n",
            "Epoch 19, Step 2, Loss: 0.019245736300945282\n",
            "Epoch 19, Step 3, Loss: 0.015996146947145462\n",
            "Epoch 19, Step 4, Loss: 0.013910455629229546\n",
            "Epoch 19, Step 5, Loss: 0.009939143434166908\n",
            "Epoch 19, Step 6, Loss: 0.00772968539968133\n",
            "Epoch 19, Step 7, Loss: 0.0058762445114552975\n",
            "Epoch 19, Step 8, Loss: 0.005766137037426233\n",
            "Epoch 19, Step 9, Loss: 0.0053859008476138115\n",
            "Epoch 19, Step 10, Loss: 0.005255117546766996\n",
            "Epoch 19, Step 11, Loss: 0.005960072390735149\n",
            "Epoch 19, Step 12, Loss: 0.005124154035001993\n",
            "Epoch 19, Step 13, Loss: 0.005010552704334259\n",
            "Epoch 19, Step 14, Loss: 0.004003259353339672\n",
            "Epoch 19, Step 15, Loss: 0.005002167541533709\n",
            "Epoch 19, Step 16, Loss: 0.005303426645696163\n",
            "Epoch 19, Step 17, Loss: 0.00615667924284935\n",
            "Epoch 19, Step 18, Loss: 0.007512770593166351\n",
            "Epoch 19, Step 19, Loss: 0.009198908694088459\n",
            "Epoch 19, Step 20, Loss: 0.008763195015490055\n",
            "Epoch 19, Step 21, Loss: 0.009572982788085938\n",
            "Epoch 19, Step 22, Loss: 0.009710392914712429\n",
            "Epoch 19, Step 23, Loss: 0.010315656661987305\n",
            "Epoch 19, Step 24, Loss: 0.011151974089443684\n",
            "Epoch 19, Step 25, Loss: 0.01130005531013012\n",
            "Epoch 19, Step 26, Loss: 0.010929926298558712\n",
            "Epoch 19, Step 27, Loss: 0.008981838822364807\n",
            "Epoch 19, Step 28, Loss: 0.009538134559988976\n",
            "Epoch 19, Step 29, Loss: 0.01007277425378561\n",
            "Epoch 19, Step 30, Loss: 0.008085629902780056\n",
            "Epoch 19, Step 31, Loss: 0.010467716492712498\n",
            "Epoch 19, Step 32, Loss: 0.009150946512818336\n",
            "Epoch 19, Step 33, Loss: 0.009859471581876278\n",
            "Epoch 19, Step 34, Loss: 0.008804826997220516\n",
            "Epoch 19, Step 35, Loss: 0.008554608561098576\n",
            "Epoch 19, Step 36, Loss: 0.008347432129085064\n",
            "Epoch 19, Step 37, Loss: 0.00825671385973692\n",
            "Epoch 19, Step 38, Loss: 0.007408658973872662\n",
            "Epoch 19, Step 39, Loss: 0.00711578456684947\n",
            "Epoch 19, Step 40, Loss: 0.007568339351564646\n",
            "Epoch 19, Step 41, Loss: 0.006210785824805498\n",
            "Epoch 19, Step 42, Loss: 0.005730849225074053\n",
            "Epoch 19, Step 43, Loss: 0.0055438196286559105\n",
            "Epoch 19, Step 44, Loss: 0.005653277970850468\n",
            "Epoch 19, Step 45, Loss: 0.0064363484270870686\n",
            "Epoch 19, Step 46, Loss: 0.005863246973603964\n",
            "Epoch 19, Step 47, Loss: 0.006121035199612379\n",
            "Epoch 19, Step 48, Loss: 0.0056202588602900505\n",
            "Epoch 19, Step 49, Loss: 0.005676686763763428\n",
            "Epoch 19, Step 50, Loss: 0.00515278335660696\n",
            "Epoch 19, Step 51, Loss: 0.005449272226542234\n",
            "Epoch 19, Step 52, Loss: 0.005228920374065638\n",
            "Epoch 19, Step 53, Loss: 0.006773193832486868\n",
            "Epoch 19, Step 54, Loss: 0.005350847728550434\n",
            "Epoch 19, Step 55, Loss: 0.006126695778220892\n",
            "Epoch 19, Step 56, Loss: 0.006624567322432995\n",
            "Epoch 19, Step 57, Loss: 0.006241624243557453\n",
            "Epoch 19, Step 58, Loss: 0.006357321050018072\n",
            "Epoch 19, Step 59, Loss: 0.005758206360042095\n",
            "Epoch 19, Step 60, Loss: 0.006192984525114298\n",
            "Epoch 19, Step 61, Loss: 0.00794307142496109\n",
            "Epoch 19, Step 62, Loss: 0.006037337239831686\n",
            "Epoch 19, Step 63, Loss: 0.010051473043859005\n",
            "Epoch 19, Step 64, Loss: 0.005984930787235498\n",
            "Epoch 19, Step 65, Loss: 0.006046959664672613\n",
            "Epoch 19, Step 66, Loss: 0.005688515026122332\n",
            "Epoch 19, Step 67, Loss: 0.0063134655356407166\n",
            "Epoch 19, Step 68, Loss: 0.005402225069701672\n",
            "Epoch 19, Step 69, Loss: 0.005420739762485027\n",
            "Epoch 19, Step 70, Loss: 0.006253559608012438\n",
            "Epoch 19, Step 71, Loss: 0.006225385703146458\n",
            "Epoch 19, Step 72, Loss: 0.005096530541777611\n",
            "Epoch 19, Step 73, Loss: 0.005236645229160786\n",
            "Epoch 19, Step 74, Loss: 0.005206503439694643\n",
            "Epoch 19, Step 75, Loss: 0.0052022975869476795\n",
            "Epoch 19, Step 76, Loss: 0.0053144171833992004\n",
            "Epoch 19, Step 77, Loss: 0.005742998328059912\n",
            "Epoch 19, Step 78, Loss: 0.004931730218231678\n",
            "Epoch 19, Step 79, Loss: 0.00525836693122983\n",
            "Epoch 19, Step 80, Loss: 0.005334748886525631\n",
            "Epoch 19, Step 81, Loss: 0.007869311608374119\n",
            "Epoch 19, Step 82, Loss: 0.0067177871242165565\n",
            "Epoch 19, Step 83, Loss: 0.006984464358538389\n",
            "Epoch 19, Step 84, Loss: 0.0075510647147893906\n",
            "Epoch 19, Step 85, Loss: 0.007645330857485533\n",
            "Epoch 19, Step 86, Loss: 0.007579761557281017\n",
            "Train Metric MRRs: 0.3007856581800667\n",
            "Train Metric MAPs: 0.3339409977596383\n",
            "Validation Metric MRRs: 0.2096993687234073\n",
            "Validation Metric MAPs: 0.2317201070406259\n",
            "Epoch 20, Step 1, Loss: 0.018132036551833153\n",
            "Epoch 20, Step 2, Loss: 0.01914104074239731\n",
            "Epoch 20, Step 3, Loss: 0.015957703813910484\n",
            "Epoch 20, Step 4, Loss: 0.013817568309605122\n",
            "Epoch 20, Step 5, Loss: 0.00989892240613699\n",
            "Epoch 20, Step 6, Loss: 0.007663516327738762\n",
            "Epoch 20, Step 7, Loss: 0.005886007100343704\n",
            "Epoch 20, Step 8, Loss: 0.005778410006314516\n",
            "Epoch 20, Step 9, Loss: 0.005359935108572245\n",
            "Epoch 20, Step 10, Loss: 0.005218170583248138\n",
            "Epoch 20, Step 11, Loss: 0.005942984949797392\n",
            "Epoch 20, Step 12, Loss: 0.00514434278011322\n",
            "Epoch 20, Step 13, Loss: 0.004997411742806435\n",
            "Epoch 20, Step 14, Loss: 0.004007988143712282\n",
            "Epoch 20, Step 15, Loss: 0.004982027690857649\n",
            "Epoch 20, Step 16, Loss: 0.005366600584238768\n",
            "Epoch 20, Step 17, Loss: 0.006074492819607258\n",
            "Epoch 20, Step 18, Loss: 0.007539795711636543\n",
            "Epoch 20, Step 19, Loss: 0.009218554012477398\n",
            "Epoch 20, Step 20, Loss: 0.008691338822245598\n",
            "Epoch 20, Step 21, Loss: 0.009574583731591702\n",
            "Epoch 20, Step 22, Loss: 0.009729684330523014\n",
            "Epoch 20, Step 23, Loss: 0.010259448550641537\n",
            "Epoch 20, Step 24, Loss: 0.011266665533185005\n",
            "Epoch 20, Step 25, Loss: 0.011128702200949192\n",
            "Epoch 20, Step 26, Loss: 0.010693385265767574\n",
            "Epoch 20, Step 27, Loss: 0.009015578776597977\n",
            "Epoch 20, Step 28, Loss: 0.009481054730713367\n",
            "Epoch 20, Step 29, Loss: 0.009979290887713432\n",
            "Epoch 20, Step 30, Loss: 0.008078143931925297\n",
            "Epoch 20, Step 31, Loss: 0.01041977759450674\n",
            "Epoch 20, Step 32, Loss: 0.009027461521327496\n",
            "Epoch 20, Step 33, Loss: 0.009838934056460857\n",
            "Epoch 20, Step 34, Loss: 0.008799190632998943\n",
            "Epoch 20, Step 35, Loss: 0.008500026538968086\n",
            "Epoch 20, Step 36, Loss: 0.008352546952664852\n",
            "Epoch 20, Step 37, Loss: 0.008199051953852177\n",
            "Epoch 20, Step 38, Loss: 0.007381231989711523\n",
            "Epoch 20, Step 39, Loss: 0.007099976297467947\n",
            "Epoch 20, Step 40, Loss: 0.0076086451299488544\n",
            "Epoch 20, Step 41, Loss: 0.006255727726966143\n",
            "Epoch 20, Step 42, Loss: 0.005711634177714586\n",
            "Epoch 20, Step 43, Loss: 0.005481699015945196\n",
            "Epoch 20, Step 44, Loss: 0.005601856391876936\n",
            "Epoch 20, Step 45, Loss: 0.0063976324163377285\n",
            "Epoch 20, Step 46, Loss: 0.005864377599209547\n",
            "Epoch 20, Step 47, Loss: 0.006089781876653433\n",
            "Epoch 20, Step 48, Loss: 0.005646656733006239\n",
            "Epoch 20, Step 49, Loss: 0.005617036949843168\n",
            "Epoch 20, Step 50, Loss: 0.005101317539811134\n",
            "Epoch 20, Step 51, Loss: 0.005474225617945194\n",
            "Epoch 20, Step 52, Loss: 0.00520175090059638\n",
            "Epoch 20, Step 53, Loss: 0.006684112828224897\n",
            "Epoch 20, Step 54, Loss: 0.005367910489439964\n",
            "Epoch 20, Step 55, Loss: 0.006108487024903297\n",
            "Epoch 20, Step 56, Loss: 0.006596731953322887\n",
            "Epoch 20, Step 57, Loss: 0.006237930618226528\n",
            "Epoch 20, Step 58, Loss: 0.006335660815238953\n",
            "Epoch 20, Step 59, Loss: 0.005786520428955555\n",
            "Epoch 20, Step 60, Loss: 0.006211891304701567\n",
            "Epoch 20, Step 61, Loss: 0.007835865020751953\n",
            "Epoch 20, Step 62, Loss: 0.006004062481224537\n",
            "Epoch 20, Step 63, Loss: 0.009638859890401363\n",
            "Epoch 20, Step 64, Loss: 0.0059412517584860325\n",
            "Epoch 20, Step 65, Loss: 0.00598899694159627\n",
            "Epoch 20, Step 66, Loss: 0.005628603510558605\n",
            "Epoch 20, Step 67, Loss: 0.006238719914108515\n",
            "Epoch 20, Step 68, Loss: 0.005373528692871332\n",
            "Epoch 20, Step 69, Loss: 0.005383889190852642\n",
            "Epoch 20, Step 70, Loss: 0.006146125961095095\n",
            "Epoch 20, Step 71, Loss: 0.006211337633430958\n",
            "Epoch 20, Step 72, Loss: 0.005102213472127914\n",
            "Epoch 20, Step 73, Loss: 0.005206984002143145\n",
            "Epoch 20, Step 74, Loss: 0.005164088681340218\n",
            "Epoch 20, Step 75, Loss: 0.005179314408451319\n",
            "Epoch 20, Step 76, Loss: 0.00522344745695591\n",
            "Epoch 20, Step 77, Loss: 0.005711421370506287\n",
            "Epoch 20, Step 78, Loss: 0.004882672801613808\n",
            "Epoch 20, Step 79, Loss: 0.005207842215895653\n",
            "Epoch 20, Step 80, Loss: 0.005287459120154381\n",
            "Epoch 20, Step 81, Loss: 0.007774957921355963\n",
            "Epoch 20, Step 82, Loss: 0.0066799139603972435\n",
            "Epoch 20, Step 83, Loss: 0.00690425094217062\n",
            "Epoch 20, Step 84, Loss: 0.007423311937600374\n",
            "Epoch 20, Step 85, Loss: 0.007623862475156784\n",
            "Epoch 20, Step 86, Loss: 0.00751822954043746\n",
            "Train Metric MRRs: 0.3023415235164743\n",
            "Train Metric MAPs: 0.33670845357483825\n",
            "Validation Metric MRRs: 0.2122160005850534\n",
            "Validation Metric MAPs: 0.23227375395212232\n",
            "Epoch 21, Step 1, Loss: 0.017932988703250885\n",
            "Epoch 21, Step 2, Loss: 0.01917116716504097\n",
            "Epoch 21, Step 3, Loss: 0.01585470885038376\n",
            "Epoch 21, Step 4, Loss: 0.013783666305243969\n",
            "Epoch 21, Step 5, Loss: 0.009851908311247826\n",
            "Epoch 21, Step 6, Loss: 0.007636018563061953\n",
            "Epoch 21, Step 7, Loss: 0.005823966581374407\n",
            "Epoch 21, Step 8, Loss: 0.0057159122079610825\n",
            "Epoch 21, Step 9, Loss: 0.00528202299028635\n",
            "Epoch 21, Step 10, Loss: 0.005262573715299368\n",
            "Epoch 21, Step 11, Loss: 0.005926322191953659\n",
            "Epoch 21, Step 12, Loss: 0.005112085025757551\n",
            "Epoch 21, Step 13, Loss: 0.004927267786115408\n",
            "Epoch 21, Step 14, Loss: 0.0039512719959020615\n",
            "Epoch 21, Step 15, Loss: 0.004998450167477131\n",
            "Epoch 21, Step 16, Loss: 0.0053035663440823555\n",
            "Epoch 21, Step 17, Loss: 0.0060643297620117664\n",
            "Epoch 21, Step 18, Loss: 0.007435747422277927\n",
            "Epoch 21, Step 19, Loss: 0.00905925128608942\n",
            "Epoch 21, Step 20, Loss: 0.008634557016193867\n",
            "Epoch 21, Step 21, Loss: 0.009642663411796093\n",
            "Epoch 21, Step 22, Loss: 0.009731540456414223\n",
            "Epoch 21, Step 23, Loss: 0.01019409392029047\n",
            "Epoch 21, Step 24, Loss: 0.011122399009764194\n",
            "Epoch 21, Step 25, Loss: 0.011047721840441227\n",
            "Epoch 21, Step 26, Loss: 0.010685973800718784\n",
            "Epoch 21, Step 27, Loss: 0.008977513760328293\n",
            "Epoch 21, Step 28, Loss: 0.009530351497232914\n",
            "Epoch 21, Step 29, Loss: 0.009973292239010334\n",
            "Epoch 21, Step 30, Loss: 0.008050170727074146\n",
            "Epoch 21, Step 31, Loss: 0.010385951027274132\n",
            "Epoch 21, Step 32, Loss: 0.009068301878869534\n",
            "Epoch 21, Step 33, Loss: 0.009857598692178726\n",
            "Epoch 21, Step 34, Loss: 0.008770289830863476\n",
            "Epoch 21, Step 35, Loss: 0.008536815643310547\n",
            "Epoch 21, Step 36, Loss: 0.00837660487741232\n",
            "Epoch 21, Step 37, Loss: 0.008213086985051632\n",
            "Epoch 21, Step 38, Loss: 0.007380555383861065\n",
            "Epoch 21, Step 39, Loss: 0.007043858524411917\n",
            "Epoch 21, Step 40, Loss: 0.007593767251819372\n",
            "Epoch 21, Step 41, Loss: 0.0062047578394412994\n",
            "Epoch 21, Step 42, Loss: 0.0057417177595198154\n",
            "Epoch 21, Step 43, Loss: 0.0054655661806464195\n",
            "Epoch 21, Step 44, Loss: 0.005608258303254843\n",
            "Epoch 21, Step 45, Loss: 0.006395920645445585\n",
            "Epoch 21, Step 46, Loss: 0.005844199098646641\n",
            "Epoch 21, Step 47, Loss: 0.006117306184023619\n",
            "Epoch 21, Step 48, Loss: 0.005597920157015324\n",
            "Epoch 21, Step 49, Loss: 0.0056479861959815025\n",
            "Epoch 21, Step 50, Loss: 0.005088151898235083\n",
            "Epoch 21, Step 51, Loss: 0.005496617406606674\n",
            "Epoch 21, Step 52, Loss: 0.005181869026273489\n",
            "Epoch 21, Step 53, Loss: 0.006662597414106131\n",
            "Epoch 21, Step 54, Loss: 0.005381094757467508\n",
            "Epoch 21, Step 55, Loss: 0.006057121325284243\n",
            "Epoch 21, Step 56, Loss: 0.006620560772716999\n",
            "Epoch 21, Step 57, Loss: 0.006213102489709854\n",
            "Epoch 21, Step 58, Loss: 0.006313718855381012\n",
            "Epoch 21, Step 59, Loss: 0.005794204771518707\n",
            "Epoch 21, Step 60, Loss: 0.006182679906487465\n",
            "Epoch 21, Step 61, Loss: 0.007749722804874182\n",
            "Epoch 21, Step 62, Loss: 0.005997376050800085\n",
            "Epoch 21, Step 63, Loss: 0.009716099128127098\n",
            "Epoch 21, Step 64, Loss: 0.005893870256841183\n",
            "Epoch 21, Step 65, Loss: 0.00599732855334878\n",
            "Epoch 21, Step 66, Loss: 0.005665338132530451\n",
            "Epoch 21, Step 67, Loss: 0.006190289277583361\n",
            "Epoch 21, Step 68, Loss: 0.0053534116595983505\n",
            "Epoch 21, Step 69, Loss: 0.005433677230030298\n",
            "Epoch 21, Step 70, Loss: 0.006157683674246073\n",
            "Epoch 21, Step 71, Loss: 0.006225206423550844\n",
            "Epoch 21, Step 72, Loss: 0.0050318073481321335\n",
            "Epoch 21, Step 73, Loss: 0.005167964845895767\n",
            "Epoch 21, Step 74, Loss: 0.005135627929121256\n",
            "Epoch 21, Step 75, Loss: 0.005174150224775076\n",
            "Epoch 21, Step 76, Loss: 0.005242760758846998\n",
            "Epoch 21, Step 77, Loss: 0.005721705965697765\n",
            "Epoch 21, Step 78, Loss: 0.004845888819545507\n",
            "Epoch 21, Step 79, Loss: 0.005259566009044647\n",
            "Epoch 21, Step 80, Loss: 0.0052841478027403355\n",
            "Epoch 21, Step 81, Loss: 0.007735912688076496\n",
            "Epoch 21, Step 82, Loss: 0.006571860518306494\n",
            "Epoch 21, Step 83, Loss: 0.006927283015102148\n",
            "Epoch 21, Step 84, Loss: 0.007369993720203638\n",
            "Epoch 21, Step 85, Loss: 0.00757638830691576\n",
            "Epoch 21, Step 86, Loss: 0.007446999195963144\n",
            "Train Metric MRRs: 0.3022266035844437\n",
            "Train Metric MAPs: 0.3386136605928095\n",
            "Validation Metric MRRs: 0.20975576287069989\n",
            "Validation Metric MAPs: 0.23171592655455558\n",
            "Epoch 22, Step 1, Loss: 0.017763838171958923\n",
            "Epoch 22, Step 2, Loss: 0.019170675426721573\n",
            "Epoch 22, Step 3, Loss: 0.015810774639248848\n",
            "Epoch 22, Step 4, Loss: 0.01390561182051897\n",
            "Epoch 22, Step 5, Loss: 0.009960825555026531\n",
            "Epoch 22, Step 6, Loss: 0.007694882806390524\n",
            "Epoch 22, Step 7, Loss: 0.0058568003587424755\n",
            "Epoch 22, Step 8, Loss: 0.005747123621404171\n",
            "Epoch 22, Step 9, Loss: 0.005367482081055641\n",
            "Epoch 22, Step 10, Loss: 0.0053076185286045074\n",
            "Epoch 22, Step 11, Loss: 0.0059378850273787975\n",
            "Epoch 22, Step 12, Loss: 0.005115305073559284\n",
            "Epoch 22, Step 13, Loss: 0.005092051345854998\n",
            "Epoch 22, Step 14, Loss: 0.0040118517354130745\n",
            "Epoch 22, Step 15, Loss: 0.0049963402561843395\n",
            "Epoch 22, Step 16, Loss: 0.0052848439663648605\n",
            "Epoch 22, Step 17, Loss: 0.006064344197511673\n",
            "Epoch 22, Step 18, Loss: 0.007557269651442766\n",
            "Epoch 22, Step 19, Loss: 0.009274189360439777\n",
            "Epoch 22, Step 20, Loss: 0.008670487441122532\n",
            "Epoch 22, Step 21, Loss: 0.00958569347858429\n",
            "Epoch 22, Step 22, Loss: 0.009715784341096878\n",
            "Epoch 22, Step 23, Loss: 0.010557794012129307\n",
            "Epoch 22, Step 24, Loss: 0.011344628408551216\n",
            "Epoch 22, Step 25, Loss: 0.011106142774224281\n",
            "Epoch 22, Step 26, Loss: 0.010805275291204453\n",
            "Epoch 22, Step 27, Loss: 0.009035496972501278\n",
            "Epoch 22, Step 28, Loss: 0.009625602513551712\n",
            "Epoch 22, Step 29, Loss: 0.009866864420473576\n",
            "Epoch 22, Step 30, Loss: 0.008170758374035358\n",
            "Epoch 22, Step 31, Loss: 0.01027203630656004\n",
            "Epoch 22, Step 32, Loss: 0.008970754221081734\n",
            "Epoch 22, Step 33, Loss: 0.009869239293038845\n",
            "Epoch 22, Step 34, Loss: 0.008831505663692951\n",
            "Epoch 22, Step 35, Loss: 0.008488770574331284\n",
            "Epoch 22, Step 36, Loss: 0.0083116814494133\n",
            "Epoch 22, Step 37, Loss: 0.008253407664597034\n",
            "Epoch 22, Step 38, Loss: 0.007407524157315493\n",
            "Epoch 22, Step 39, Loss: 0.0070691280998289585\n",
            "Epoch 22, Step 40, Loss: 0.0075615644454956055\n",
            "Epoch 22, Step 41, Loss: 0.0062135918997228146\n",
            "Epoch 22, Step 42, Loss: 0.005690422840416431\n",
            "Epoch 22, Step 43, Loss: 0.00546511122956872\n",
            "Epoch 22, Step 44, Loss: 0.005567236803472042\n",
            "Epoch 22, Step 45, Loss: 0.006381018552929163\n",
            "Epoch 22, Step 46, Loss: 0.005832082591950893\n",
            "Epoch 22, Step 47, Loss: 0.0061188675463199615\n",
            "Epoch 22, Step 48, Loss: 0.005664926487952471\n",
            "Epoch 22, Step 49, Loss: 0.005601090844720602\n",
            "Epoch 22, Step 50, Loss: 0.005117611028254032\n",
            "Epoch 22, Step 51, Loss: 0.005488336551934481\n",
            "Epoch 22, Step 52, Loss: 0.0052167740650475025\n",
            "Epoch 22, Step 53, Loss: 0.006721519865095615\n",
            "Epoch 22, Step 54, Loss: 0.005379651207476854\n",
            "Epoch 22, Step 55, Loss: 0.006089112721383572\n",
            "Epoch 22, Step 56, Loss: 0.006607151590287685\n",
            "Epoch 22, Step 57, Loss: 0.006164982449263334\n",
            "Epoch 22, Step 58, Loss: 0.006348766386508942\n",
            "Epoch 22, Step 59, Loss: 0.005757731385529041\n",
            "Epoch 22, Step 60, Loss: 0.006205130834132433\n",
            "Epoch 22, Step 61, Loss: 0.007729611359536648\n",
            "Epoch 22, Step 62, Loss: 0.0059682996943593025\n",
            "Epoch 22, Step 63, Loss: 0.009419876150786877\n",
            "Epoch 22, Step 64, Loss: 0.005859761964529753\n",
            "Epoch 22, Step 65, Loss: 0.005968248937278986\n",
            "Epoch 22, Step 66, Loss: 0.0055834208615124226\n",
            "Epoch 22, Step 67, Loss: 0.006123796105384827\n",
            "Epoch 22, Step 68, Loss: 0.005336966831237078\n",
            "Epoch 22, Step 69, Loss: 0.005421972833573818\n",
            "Epoch 22, Step 70, Loss: 0.006165795959532261\n",
            "Epoch 22, Step 71, Loss: 0.006289765704423189\n",
            "Epoch 22, Step 72, Loss: 0.005145112052559853\n",
            "Epoch 22, Step 73, Loss: 0.0051860869862139225\n",
            "Epoch 22, Step 74, Loss: 0.005128303077071905\n",
            "Epoch 22, Step 75, Loss: 0.005186998751014471\n",
            "Epoch 22, Step 76, Loss: 0.005152416415512562\n",
            "Epoch 22, Step 77, Loss: 0.005727423820644617\n",
            "Epoch 22, Step 78, Loss: 0.004849724005907774\n",
            "Epoch 22, Step 79, Loss: 0.0052160718478262424\n",
            "Epoch 22, Step 80, Loss: 0.005292041692882776\n",
            "Epoch 22, Step 81, Loss: 0.007810271345078945\n",
            "Epoch 22, Step 82, Loss: 0.0066750384867191315\n",
            "Epoch 22, Step 83, Loss: 0.006912492215633392\n",
            "Epoch 22, Step 84, Loss: 0.007373569533228874\n",
            "Epoch 22, Step 85, Loss: 0.0075296442955732346\n",
            "Epoch 22, Step 86, Loss: 0.007443444337695837\n",
            "Train Metric MRRs: 0.30198158685821896\n",
            "Train Metric MAPs: 0.3392058751618439\n",
            "Validation Metric MRRs: 0.21111446334988002\n",
            "Validation Metric MAPs: 0.2330181843620647\n",
            "Epoch 23, Step 1, Loss: 0.01746806502342224\n",
            "Epoch 23, Step 2, Loss: 0.01887761987745762\n",
            "Epoch 23, Step 3, Loss: 0.015786433592438698\n",
            "Epoch 23, Step 4, Loss: 0.013781111687421799\n",
            "Epoch 23, Step 5, Loss: 0.009866859763860703\n",
            "Epoch 23, Step 6, Loss: 0.007628175895661116\n",
            "Epoch 23, Step 7, Loss: 0.005813454743474722\n",
            "Epoch 23, Step 8, Loss: 0.005709836259484291\n",
            "Epoch 23, Step 9, Loss: 0.005288583226501942\n",
            "Epoch 23, Step 10, Loss: 0.005256256554275751\n",
            "Epoch 23, Step 11, Loss: 0.005895859096199274\n",
            "Epoch 23, Step 12, Loss: 0.0051914602518081665\n",
            "Epoch 23, Step 13, Loss: 0.004935063887387514\n",
            "Epoch 23, Step 14, Loss: 0.003954034764319658\n",
            "Epoch 23, Step 15, Loss: 0.0051555209793150425\n",
            "Epoch 23, Step 16, Loss: 0.005292708519846201\n",
            "Epoch 23, Step 17, Loss: 0.006040974520146847\n",
            "Epoch 23, Step 18, Loss: 0.007676351815462112\n",
            "Epoch 23, Step 19, Loss: 0.009172140620648861\n",
            "Epoch 23, Step 20, Loss: 0.008714353665709496\n",
            "Epoch 23, Step 21, Loss: 0.009658672846853733\n",
            "Epoch 23, Step 22, Loss: 0.009718257002532482\n",
            "Epoch 23, Step 23, Loss: 0.010181615129113197\n",
            "Epoch 23, Step 24, Loss: 0.011186939664185047\n",
            "Epoch 23, Step 25, Loss: 0.011171798221766949\n",
            "Epoch 23, Step 26, Loss: 0.010717005468904972\n",
            "Epoch 23, Step 27, Loss: 0.008963444270193577\n",
            "Epoch 23, Step 28, Loss: 0.009407048113644123\n",
            "Epoch 23, Step 29, Loss: 0.009861123748123646\n",
            "Epoch 23, Step 30, Loss: 0.0080959377810359\n",
            "Epoch 23, Step 31, Loss: 0.010270352475345135\n",
            "Epoch 23, Step 32, Loss: 0.009013424627482891\n",
            "Epoch 23, Step 33, Loss: 0.009841172955930233\n",
            "Epoch 23, Step 34, Loss: 0.008982270956039429\n",
            "Epoch 23, Step 35, Loss: 0.008521870709955692\n",
            "Epoch 23, Step 36, Loss: 0.00832765270024538\n",
            "Epoch 23, Step 37, Loss: 0.008062697015702724\n",
            "Epoch 23, Step 38, Loss: 0.007336957845836878\n",
            "Epoch 23, Step 39, Loss: 0.007098884787410498\n",
            "Epoch 23, Step 40, Loss: 0.007634750567376614\n",
            "Epoch 23, Step 41, Loss: 0.006155170034617186\n",
            "Epoch 23, Step 42, Loss: 0.005814575124531984\n",
            "Epoch 23, Step 43, Loss: 0.005467507056891918\n",
            "Epoch 23, Step 44, Loss: 0.005546885076910257\n",
            "Epoch 23, Step 45, Loss: 0.006375213619321585\n",
            "Epoch 23, Step 46, Loss: 0.005811040755361319\n",
            "Epoch 23, Step 47, Loss: 0.006097962148487568\n",
            "Epoch 23, Step 48, Loss: 0.0056054857559502125\n",
            "Epoch 23, Step 49, Loss: 0.005625114776194096\n",
            "Epoch 23, Step 50, Loss: 0.005122843198478222\n",
            "Epoch 23, Step 51, Loss: 0.005469188559800386\n",
            "Epoch 23, Step 52, Loss: 0.005197825375944376\n",
            "Epoch 23, Step 53, Loss: 0.006620192900300026\n",
            "Epoch 23, Step 54, Loss: 0.005333302542567253\n",
            "Epoch 23, Step 55, Loss: 0.006074309814721346\n",
            "Epoch 23, Step 56, Loss: 0.0066004423424601555\n",
            "Epoch 23, Step 57, Loss: 0.006216047331690788\n",
            "Epoch 23, Step 58, Loss: 0.006312132813036442\n",
            "Epoch 23, Step 59, Loss: 0.005771858152002096\n",
            "Epoch 23, Step 60, Loss: 0.006213860120624304\n",
            "Epoch 23, Step 61, Loss: 0.007812267169356346\n",
            "Epoch 23, Step 62, Loss: 0.005997218191623688\n",
            "Epoch 23, Step 63, Loss: 0.009254412725567818\n",
            "Epoch 23, Step 64, Loss: 0.0057897428050637245\n",
            "Epoch 23, Step 65, Loss: 0.005981186404824257\n",
            "Epoch 23, Step 66, Loss: 0.005611221771687269\n",
            "Epoch 23, Step 67, Loss: 0.00609777495265007\n",
            "Epoch 23, Step 68, Loss: 0.005345712881535292\n",
            "Epoch 23, Step 69, Loss: 0.005442694760859013\n",
            "Epoch 23, Step 70, Loss: 0.006117227952927351\n",
            "Epoch 23, Step 71, Loss: 0.006175453774631023\n",
            "Epoch 23, Step 72, Loss: 0.005065619945526123\n",
            "Epoch 23, Step 73, Loss: 0.005212489049881697\n",
            "Epoch 23, Step 74, Loss: 0.00512661412358284\n",
            "Epoch 23, Step 75, Loss: 0.0051376670598983765\n",
            "Epoch 23, Step 76, Loss: 0.005150250159204006\n",
            "Epoch 23, Step 77, Loss: 0.005721151828765869\n",
            "Epoch 23, Step 78, Loss: 0.0048236241564154625\n",
            "Epoch 23, Step 79, Loss: 0.005152398254722357\n",
            "Epoch 23, Step 80, Loss: 0.005309443920850754\n",
            "Epoch 23, Step 81, Loss: 0.007769478484988213\n",
            "Epoch 23, Step 82, Loss: 0.006608038209378719\n",
            "Epoch 23, Step 83, Loss: 0.006872480735182762\n",
            "Epoch 23, Step 84, Loss: 0.007320025935769081\n",
            "Epoch 23, Step 85, Loss: 0.0075322301127016544\n",
            "Epoch 23, Step 86, Loss: 0.00739471847191453\n",
            "Train Metric MRRs: 0.302979039462599\n",
            "Train Metric MAPs: 0.3411835137581369\n",
            "Validation Metric MRRs: 0.21159432450983828\n",
            "Validation Metric MAPs: 0.23285352364446923\n",
            "Epoch 24, Step 1, Loss: 0.01758905127644539\n",
            "Epoch 24, Step 2, Loss: 0.018989967182278633\n",
            "Epoch 24, Step 3, Loss: 0.015656305477023125\n",
            "Epoch 24, Step 4, Loss: 0.013686904683709145\n",
            "Epoch 24, Step 5, Loss: 0.009796908125281334\n",
            "Epoch 24, Step 6, Loss: 0.007645722478628159\n",
            "Epoch 24, Step 7, Loss: 0.00579407811164856\n",
            "Epoch 24, Step 8, Loss: 0.005780508741736412\n",
            "Epoch 24, Step 9, Loss: 0.005331875290721655\n",
            "Epoch 24, Step 10, Loss: 0.005446496419608593\n",
            "Epoch 24, Step 11, Loss: 0.005898443982005119\n",
            "Epoch 24, Step 12, Loss: 0.005122588947415352\n",
            "Epoch 24, Step 13, Loss: 0.004918101243674755\n",
            "Epoch 24, Step 14, Loss: 0.003865365870296955\n",
            "Epoch 24, Step 15, Loss: 0.004949173890054226\n",
            "Epoch 24, Step 16, Loss: 0.0052596163004636765\n",
            "Epoch 24, Step 17, Loss: 0.005996269639581442\n",
            "Epoch 24, Step 18, Loss: 0.0076794312335550785\n",
            "Epoch 24, Step 19, Loss: 0.009147840552031994\n",
            "Epoch 24, Step 20, Loss: 0.008927340619266033\n",
            "Epoch 24, Step 21, Loss: 0.00960206612944603\n",
            "Epoch 24, Step 22, Loss: 0.00974053330719471\n",
            "Epoch 24, Step 23, Loss: 0.01024133525788784\n",
            "Epoch 24, Step 24, Loss: 0.011048364453017712\n",
            "Epoch 24, Step 25, Loss: 0.011038742959499359\n",
            "Epoch 24, Step 26, Loss: 0.010753711685538292\n",
            "Epoch 24, Step 27, Loss: 0.008999355137348175\n",
            "Epoch 24, Step 28, Loss: 0.009436012245714664\n",
            "Epoch 24, Step 29, Loss: 0.009900077246129513\n",
            "Epoch 24, Step 30, Loss: 0.007994422689080238\n",
            "Epoch 24, Step 31, Loss: 0.01024577859789133\n",
            "Epoch 24, Step 32, Loss: 0.009019282646477222\n",
            "Epoch 24, Step 33, Loss: 0.009776655584573746\n",
            "Epoch 24, Step 34, Loss: 0.008922812528908253\n",
            "Epoch 24, Step 35, Loss: 0.008576145395636559\n",
            "Epoch 24, Step 36, Loss: 0.008287745527923107\n",
            "Epoch 24, Step 37, Loss: 0.008252648636698723\n",
            "Epoch 24, Step 38, Loss: 0.007412169128656387\n",
            "Epoch 24, Step 39, Loss: 0.007049357518553734\n",
            "Epoch 24, Step 40, Loss: 0.007658304180949926\n",
            "Epoch 24, Step 41, Loss: 0.006169220898300409\n",
            "Epoch 24, Step 42, Loss: 0.005789410788565874\n",
            "Epoch 24, Step 43, Loss: 0.0054628485813736916\n",
            "Epoch 24, Step 44, Loss: 0.005589282605797052\n",
            "Epoch 24, Step 45, Loss: 0.006333136465400457\n",
            "Epoch 24, Step 46, Loss: 0.005791087169200182\n",
            "Epoch 24, Step 47, Loss: 0.006044219713658094\n",
            "Epoch 24, Step 48, Loss: 0.005571859423071146\n",
            "Epoch 24, Step 49, Loss: 0.005604733247309923\n",
            "Epoch 24, Step 50, Loss: 0.005123162642121315\n",
            "Epoch 24, Step 51, Loss: 0.005512665491551161\n",
            "Epoch 24, Step 52, Loss: 0.005188337527215481\n",
            "Epoch 24, Step 53, Loss: 0.0066633280366659164\n",
            "Epoch 24, Step 54, Loss: 0.005331816151738167\n",
            "Epoch 24, Step 55, Loss: 0.006045847665518522\n",
            "Epoch 24, Step 56, Loss: 0.006606170441955328\n",
            "Epoch 24, Step 57, Loss: 0.0061653777956962585\n",
            "Epoch 24, Step 58, Loss: 0.006302264519035816\n",
            "Epoch 24, Step 59, Loss: 0.005722262896597385\n",
            "Epoch 24, Step 60, Loss: 0.006190104875713587\n",
            "Epoch 24, Step 61, Loss: 0.007745908107608557\n",
            "Epoch 24, Step 62, Loss: 0.005901901982724667\n",
            "Epoch 24, Step 63, Loss: 0.009266737848520279\n",
            "Epoch 24, Step 64, Loss: 0.005839203484356403\n",
            "Epoch 24, Step 65, Loss: 0.005894884001463652\n",
            "Epoch 24, Step 66, Loss: 0.005665360484272242\n",
            "Epoch 24, Step 67, Loss: 0.006167996674776077\n",
            "Epoch 24, Step 68, Loss: 0.005340305622667074\n",
            "Epoch 24, Step 69, Loss: 0.005527936853468418\n",
            "Epoch 24, Step 70, Loss: 0.0061698067001998425\n",
            "Epoch 24, Step 71, Loss: 0.006220954470336437\n",
            "Epoch 24, Step 72, Loss: 0.005012438166886568\n",
            "Epoch 24, Step 73, Loss: 0.005118705332279205\n",
            "Epoch 24, Step 74, Loss: 0.005146726965904236\n",
            "Epoch 24, Step 75, Loss: 0.0051304735243320465\n",
            "Epoch 24, Step 76, Loss: 0.005151629913598299\n",
            "Epoch 24, Step 77, Loss: 0.0057502565905451775\n",
            "Epoch 24, Step 78, Loss: 0.004832616541534662\n",
            "Epoch 24, Step 79, Loss: 0.005206527654081583\n",
            "Epoch 24, Step 80, Loss: 0.005312912166118622\n",
            "Epoch 24, Step 81, Loss: 0.007664099335670471\n",
            "Epoch 24, Step 82, Loss: 0.006614011246711016\n",
            "Epoch 24, Step 83, Loss: 0.0069214291870594025\n",
            "Epoch 24, Step 84, Loss: 0.007415702100843191\n",
            "Epoch 24, Step 85, Loss: 0.007576323114335537\n",
            "Epoch 24, Step 86, Loss: 0.007429203949868679\n",
            "Train Metric MRRs: 0.303233258465803\n",
            "Train Metric MAPs: 0.33962051399228205\n",
            "Validation Metric MRRs: 0.20972768038839357\n",
            "Validation Metric MAPs: 0.23094979702051688\n",
            "Epoch 25, Step 1, Loss: 0.019455399364233017\n",
            "Epoch 25, Step 2, Loss: 0.019109878689050674\n",
            "Epoch 25, Step 3, Loss: 0.015795523300766945\n",
            "Epoch 25, Step 4, Loss: 0.013653065077960491\n",
            "Epoch 25, Step 5, Loss: 0.009763477370142937\n",
            "Epoch 25, Step 6, Loss: 0.007551962044090033\n",
            "Epoch 25, Step 7, Loss: 0.005823106039315462\n",
            "Epoch 25, Step 8, Loss: 0.005704871378839016\n",
            "Epoch 25, Step 9, Loss: 0.005251300521194935\n",
            "Epoch 25, Step 10, Loss: 0.0053060827776789665\n",
            "Epoch 25, Step 11, Loss: 0.005957243964076042\n",
            "Epoch 25, Step 12, Loss: 0.005243855994194746\n",
            "Epoch 25, Step 13, Loss: 0.004848310723900795\n",
            "Epoch 25, Step 14, Loss: 0.004021945875138044\n",
            "Epoch 25, Step 15, Loss: 0.005543425213545561\n",
            "Epoch 25, Step 16, Loss: 0.005542485974729061\n",
            "Epoch 25, Step 17, Loss: 0.0060919662937521935\n",
            "Epoch 25, Step 18, Loss: 0.007552100345492363\n",
            "Epoch 25, Step 19, Loss: 0.00915276538580656\n",
            "Epoch 25, Step 20, Loss: 0.008518106304109097\n",
            "Epoch 25, Step 21, Loss: 0.009770454838871956\n",
            "Epoch 25, Step 22, Loss: 0.00999270286411047\n",
            "Epoch 25, Step 23, Loss: 0.010253421030938625\n",
            "Epoch 25, Step 24, Loss: 0.011219030246138573\n",
            "Epoch 25, Step 25, Loss: 0.011076074093580246\n",
            "Epoch 25, Step 26, Loss: 0.010691610164940357\n",
            "Epoch 25, Step 27, Loss: 0.009012280032038689\n",
            "Epoch 25, Step 28, Loss: 0.009470365010201931\n",
            "Epoch 25, Step 29, Loss: 0.009941695258021355\n",
            "Epoch 25, Step 30, Loss: 0.008097516372799873\n",
            "Epoch 25, Step 31, Loss: 0.01028739009052515\n",
            "Epoch 25, Step 32, Loss: 0.009031043387949467\n",
            "Epoch 25, Step 33, Loss: 0.009828317910432816\n",
            "Epoch 25, Step 34, Loss: 0.008841583505272865\n",
            "Epoch 25, Step 35, Loss: 0.008463462814688683\n",
            "Epoch 25, Step 36, Loss: 0.008363359607756138\n",
            "Epoch 25, Step 37, Loss: 0.008331378921866417\n",
            "Epoch 25, Step 38, Loss: 0.007359896786510944\n",
            "Epoch 25, Step 39, Loss: 0.007046717684715986\n",
            "Epoch 25, Step 40, Loss: 0.007438992615789175\n",
            "Epoch 25, Step 41, Loss: 0.006171927321702242\n",
            "Epoch 25, Step 42, Loss: 0.00574676226824522\n",
            "Epoch 25, Step 43, Loss: 0.005563032813370228\n",
            "Epoch 25, Step 44, Loss: 0.0056349546648561954\n",
            "Epoch 25, Step 45, Loss: 0.006475222762674093\n",
            "Epoch 25, Step 46, Loss: 0.00585673563182354\n",
            "Epoch 25, Step 47, Loss: 0.0061052534729242325\n",
            "Epoch 25, Step 48, Loss: 0.0055422475561499596\n",
            "Epoch 25, Step 49, Loss: 0.005569650791585445\n",
            "Epoch 25, Step 50, Loss: 0.005092059262096882\n",
            "Epoch 25, Step 51, Loss: 0.00559155223891139\n",
            "Epoch 25, Step 52, Loss: 0.0051920670084655285\n",
            "Epoch 25, Step 53, Loss: 0.00658933911472559\n",
            "Epoch 25, Step 54, Loss: 0.0053283339366316795\n",
            "Epoch 25, Step 55, Loss: 0.006009791046380997\n",
            "Epoch 25, Step 56, Loss: 0.006555346772074699\n",
            "Epoch 25, Step 57, Loss: 0.006229082588106394\n",
            "Epoch 25, Step 58, Loss: 0.006280691362917423\n",
            "Epoch 25, Step 59, Loss: 0.005759066436439753\n",
            "Epoch 25, Step 60, Loss: 0.006149599328637123\n",
            "Epoch 25, Step 61, Loss: 0.0076635959558188915\n",
            "Epoch 25, Step 62, Loss: 0.006002597976475954\n",
            "Epoch 25, Step 63, Loss: 0.00930482055991888\n",
            "Epoch 25, Step 64, Loss: 0.005945918150246143\n",
            "Epoch 25, Step 65, Loss: 0.005883253179490566\n",
            "Epoch 25, Step 66, Loss: 0.00559653714299202\n",
            "Epoch 25, Step 67, Loss: 0.0061391545459628105\n",
            "Epoch 25, Step 68, Loss: 0.005292188376188278\n",
            "Epoch 25, Step 69, Loss: 0.005462944507598877\n",
            "Epoch 25, Step 70, Loss: 0.006133942399173975\n",
            "Epoch 25, Step 71, Loss: 0.00621878681704402\n",
            "Epoch 25, Step 72, Loss: 0.005132149439305067\n",
            "Epoch 25, Step 73, Loss: 0.005110126920044422\n",
            "Epoch 25, Step 74, Loss: 0.0051157656125724316\n",
            "Epoch 25, Step 75, Loss: 0.005149523261934519\n",
            "Epoch 25, Step 76, Loss: 0.005145491566509008\n",
            "Epoch 25, Step 77, Loss: 0.0056596845388412476\n",
            "Epoch 25, Step 78, Loss: 0.004833870101720095\n",
            "Epoch 25, Step 79, Loss: 0.005185345653444529\n",
            "Epoch 25, Step 80, Loss: 0.005363012198358774\n",
            "Epoch 25, Step 81, Loss: 0.007740262430161238\n",
            "Epoch 25, Step 82, Loss: 0.006692248396575451\n",
            "Epoch 25, Step 83, Loss: 0.006891130935400724\n",
            "Epoch 25, Step 84, Loss: 0.0072823455557227135\n",
            "Epoch 25, Step 85, Loss: 0.007493414916098118\n",
            "Epoch 25, Step 86, Loss: 0.007375603541731834\n",
            "Train Metric MRRs: 0.30141542436569635\n",
            "Train Metric MAPs: 0.33862608087909035\n",
            "Validation Metric MRRs: 0.21218612629230685\n",
            "Validation Metric MAPs: 0.23113549363637947\n",
            "Epoch 26, Step 1, Loss: 0.018527744337916374\n",
            "Epoch 26, Step 2, Loss: 0.018863750621676445\n",
            "Epoch 26, Step 3, Loss: 0.015716778114438057\n",
            "Epoch 26, Step 4, Loss: 0.013746421784162521\n",
            "Epoch 26, Step 5, Loss: 0.009779779240489006\n",
            "Epoch 26, Step 6, Loss: 0.007648608181625605\n",
            "Epoch 26, Step 7, Loss: 0.005820039659738541\n",
            "Epoch 26, Step 8, Loss: 0.005771738011389971\n",
            "Epoch 26, Step 9, Loss: 0.0052651530131697655\n",
            "Epoch 26, Step 10, Loss: 0.005387204699218273\n",
            "Epoch 26, Step 11, Loss: 0.0060194144025444984\n",
            "Epoch 26, Step 12, Loss: 0.00526044238358736\n",
            "Epoch 26, Step 13, Loss: 0.004966028034687042\n",
            "Epoch 26, Step 14, Loss: 0.003898671129718423\n",
            "Epoch 26, Step 15, Loss: 0.004935167729854584\n",
            "Epoch 26, Step 16, Loss: 0.005289027001708746\n",
            "Epoch 26, Step 17, Loss: 0.005957985296845436\n",
            "Epoch 26, Step 18, Loss: 0.007588083855807781\n",
            "Epoch 26, Step 19, Loss: 0.009330702014267445\n",
            "Epoch 26, Step 20, Loss: 0.008699275553226471\n",
            "Epoch 26, Step 21, Loss: 0.009655611589550972\n",
            "Epoch 26, Step 22, Loss: 0.009816854260861874\n",
            "Epoch 26, Step 23, Loss: 0.010327192954719067\n",
            "Epoch 26, Step 24, Loss: 0.01107909344136715\n",
            "Epoch 26, Step 25, Loss: 0.01091310940682888\n",
            "Epoch 26, Step 26, Loss: 0.010579701513051987\n",
            "Epoch 26, Step 27, Loss: 0.008950138464570045\n",
            "Epoch 26, Step 28, Loss: 0.0094398632645607\n",
            "Epoch 26, Step 29, Loss: 0.009812409058213234\n",
            "Epoch 26, Step 30, Loss: 0.008094768971204758\n",
            "Epoch 26, Step 31, Loss: 0.010193304158747196\n",
            "Epoch 26, Step 32, Loss: 0.008959325961768627\n",
            "Epoch 26, Step 33, Loss: 0.009735064581036568\n",
            "Epoch 26, Step 34, Loss: 0.008812476880848408\n",
            "Epoch 26, Step 35, Loss: 0.008506210520863533\n",
            "Epoch 26, Step 36, Loss: 0.008258834481239319\n",
            "Epoch 26, Step 37, Loss: 0.008260301314294338\n",
            "Epoch 26, Step 38, Loss: 0.007455349434167147\n",
            "Epoch 26, Step 39, Loss: 0.00705765699967742\n",
            "Epoch 26, Step 40, Loss: 0.007516120094805956\n",
            "Epoch 26, Step 41, Loss: 0.006092393770813942\n",
            "Epoch 26, Step 42, Loss: 0.005734897218644619\n",
            "Epoch 26, Step 43, Loss: 0.005425672512501478\n",
            "Epoch 26, Step 44, Loss: 0.005581868812441826\n",
            "Epoch 26, Step 45, Loss: 0.006374450400471687\n",
            "Epoch 26, Step 46, Loss: 0.005800212733447552\n",
            "Epoch 26, Step 47, Loss: 0.00611322233453393\n",
            "Epoch 26, Step 48, Loss: 0.005536749958992004\n",
            "Epoch 26, Step 49, Loss: 0.005612845066934824\n",
            "Epoch 26, Step 50, Loss: 0.005091290455311537\n",
            "Epoch 26, Step 51, Loss: 0.005564642604440451\n",
            "Epoch 26, Step 52, Loss: 0.005199651699513197\n",
            "Epoch 26, Step 53, Loss: 0.0066751777194440365\n",
            "Epoch 26, Step 54, Loss: 0.005312750115990639\n",
            "Epoch 26, Step 55, Loss: 0.005999500397592783\n",
            "Epoch 26, Step 56, Loss: 0.006654185708612204\n",
            "Epoch 26, Step 57, Loss: 0.006177631206810474\n",
            "Epoch 26, Step 58, Loss: 0.00630956469103694\n",
            "Epoch 26, Step 59, Loss: 0.005695783067494631\n",
            "Epoch 26, Step 60, Loss: 0.006140454672276974\n",
            "Epoch 26, Step 61, Loss: 0.007717786822468042\n",
            "Epoch 26, Step 62, Loss: 0.005962713621556759\n",
            "Epoch 26, Step 63, Loss: 0.009148400276899338\n",
            "Epoch 26, Step 64, Loss: 0.005753247067332268\n",
            "Epoch 26, Step 65, Loss: 0.0058838254772126675\n",
            "Epoch 26, Step 66, Loss: 0.005536208860576153\n",
            "Epoch 26, Step 67, Loss: 0.0060684215277433395\n",
            "Epoch 26, Step 68, Loss: 0.005186687223613262\n",
            "Epoch 26, Step 69, Loss: 0.005431049969047308\n",
            "Epoch 26, Step 70, Loss: 0.006085372529923916\n",
            "Epoch 26, Step 71, Loss: 0.006252134684473276\n",
            "Epoch 26, Step 72, Loss: 0.00512824347242713\n",
            "Epoch 26, Step 73, Loss: 0.005173234734684229\n",
            "Epoch 26, Step 74, Loss: 0.005161677952855825\n",
            "Epoch 26, Step 75, Loss: 0.005166483111679554\n",
            "Epoch 26, Step 76, Loss: 0.005089967977255583\n",
            "Epoch 26, Step 77, Loss: 0.005624907556921244\n",
            "Epoch 26, Step 78, Loss: 0.0048273298889398575\n",
            "Epoch 26, Step 79, Loss: 0.005177002400159836\n",
            "Epoch 26, Step 80, Loss: 0.005425484385341406\n",
            "Epoch 26, Step 81, Loss: 0.007743305526673794\n",
            "Epoch 26, Step 82, Loss: 0.006751206703484058\n",
            "Epoch 26, Step 83, Loss: 0.0069523765705525875\n",
            "Epoch 26, Step 84, Loss: 0.007264722138643265\n",
            "Epoch 26, Step 85, Loss: 0.00752805732190609\n",
            "Epoch 26, Step 86, Loss: 0.007361061871051788\n",
            "Train Metric MRRs: 0.3034596229914614\n",
            "Train Metric MAPs: 0.34273558304600704\n",
            "Validation Metric MRRs: 0.2098540168382629\n",
            "Validation Metric MAPs: 0.23318950110901623\n",
            "Epoch 27, Step 1, Loss: 0.017636451870203018\n",
            "Epoch 27, Step 2, Loss: 0.018699171021580696\n",
            "Epoch 27, Step 3, Loss: 0.01562625542283058\n",
            "Epoch 27, Step 4, Loss: 0.013579746708273888\n",
            "Epoch 27, Step 5, Loss: 0.009710702113807201\n",
            "Epoch 27, Step 6, Loss: 0.007534804288297892\n",
            "Epoch 27, Step 7, Loss: 0.005793982185423374\n",
            "Epoch 27, Step 8, Loss: 0.0057027824223041534\n",
            "Epoch 27, Step 9, Loss: 0.005195492412894964\n",
            "Epoch 27, Step 10, Loss: 0.005236066412180662\n",
            "Epoch 27, Step 11, Loss: 0.0059661902487277985\n",
            "Epoch 27, Step 12, Loss: 0.0052051483653485775\n",
            "Epoch 27, Step 13, Loss: 0.004770941566675901\n",
            "Epoch 27, Step 14, Loss: 0.003822095226496458\n",
            "Epoch 27, Step 15, Loss: 0.005024243611842394\n",
            "Epoch 27, Step 16, Loss: 0.005196055863052607\n",
            "Epoch 27, Step 17, Loss: 0.005991565529257059\n",
            "Epoch 27, Step 18, Loss: 0.007336328737437725\n",
            "Epoch 27, Step 19, Loss: 0.008892481215298176\n",
            "Epoch 27, Step 20, Loss: 0.008863664232194424\n",
            "Epoch 27, Step 21, Loss: 0.00961401779204607\n",
            "Epoch 27, Step 22, Loss: 0.00993768684566021\n",
            "Epoch 27, Step 23, Loss: 0.010143578983843327\n",
            "Epoch 27, Step 24, Loss: 0.010917093604803085\n",
            "Epoch 27, Step 25, Loss: 0.010982363484799862\n",
            "Epoch 27, Step 26, Loss: 0.010746283456683159\n",
            "Epoch 27, Step 27, Loss: 0.009282359853386879\n",
            "Epoch 27, Step 28, Loss: 0.009389559738337994\n",
            "Epoch 27, Step 29, Loss: 0.009806831367313862\n",
            "Epoch 27, Step 30, Loss: 0.008042829111218452\n",
            "Epoch 27, Step 31, Loss: 0.01024989876896143\n",
            "Epoch 27, Step 32, Loss: 0.00908915139734745\n",
            "Epoch 27, Step 33, Loss: 0.009711640886962414\n",
            "Epoch 27, Step 34, Loss: 0.008902163244783878\n",
            "Epoch 27, Step 35, Loss: 0.008492120541632175\n",
            "Epoch 27, Step 36, Loss: 0.008161579258739948\n",
            "Epoch 27, Step 37, Loss: 0.008097830228507519\n",
            "Epoch 27, Step 38, Loss: 0.007365047931671143\n",
            "Epoch 27, Step 39, Loss: 0.007052379660308361\n",
            "Epoch 27, Step 40, Loss: 0.007488758768886328\n",
            "Epoch 27, Step 41, Loss: 0.0060705519281327724\n",
            "Epoch 27, Step 42, Loss: 0.005789586808532476\n",
            "Epoch 27, Step 43, Loss: 0.005481113214045763\n",
            "Epoch 27, Step 44, Loss: 0.005551795940846205\n",
            "Epoch 27, Step 45, Loss: 0.00632527656853199\n",
            "Epoch 27, Step 46, Loss: 0.005738904234021902\n",
            "Epoch 27, Step 47, Loss: 0.006055104546248913\n",
            "Epoch 27, Step 48, Loss: 0.005588398315012455\n",
            "Epoch 27, Step 49, Loss: 0.005598927848041058\n",
            "Epoch 27, Step 50, Loss: 0.005096255335956812\n",
            "Epoch 27, Step 51, Loss: 0.005573499947786331\n",
            "Epoch 27, Step 52, Loss: 0.005200401414185762\n",
            "Epoch 27, Step 53, Loss: 0.006537044420838356\n",
            "Epoch 27, Step 54, Loss: 0.005324120633304119\n",
            "Epoch 27, Step 55, Loss: 0.006063466891646385\n",
            "Epoch 27, Step 56, Loss: 0.006586679257452488\n",
            "Epoch 27, Step 57, Loss: 0.006182363256812096\n",
            "Epoch 27, Step 58, Loss: 0.006248611956834793\n",
            "Epoch 27, Step 59, Loss: 0.005694308318197727\n",
            "Epoch 27, Step 60, Loss: 0.006101776380091906\n",
            "Epoch 27, Step 61, Loss: 0.007521188817918301\n",
            "Epoch 27, Step 62, Loss: 0.00592397153377533\n",
            "Epoch 27, Step 63, Loss: 0.008898183703422546\n",
            "Epoch 27, Step 64, Loss: 0.005663325544446707\n",
            "Epoch 27, Step 65, Loss: 0.005841708742082119\n",
            "Epoch 27, Step 66, Loss: 0.0055329264141619205\n",
            "Epoch 27, Step 67, Loss: 0.0060233562253415585\n",
            "Epoch 27, Step 68, Loss: 0.005210851784795523\n",
            "Epoch 27, Step 69, Loss: 0.005433700047433376\n",
            "Epoch 27, Step 70, Loss: 0.006080467253923416\n",
            "Epoch 27, Step 71, Loss: 0.006213363725692034\n",
            "Epoch 27, Step 72, Loss: 0.004990254063159227\n",
            "Epoch 27, Step 73, Loss: 0.005042458884418011\n",
            "Epoch 27, Step 74, Loss: 0.005132858641445637\n",
            "Epoch 27, Step 75, Loss: 0.005082970950752497\n",
            "Epoch 27, Step 76, Loss: 0.0050625549629330635\n",
            "Epoch 27, Step 77, Loss: 0.00559525191783905\n",
            "Epoch 27, Step 78, Loss: 0.004791775718331337\n",
            "Epoch 27, Step 79, Loss: 0.005180263426154852\n",
            "Epoch 27, Step 80, Loss: 0.005293479189276695\n",
            "Epoch 27, Step 81, Loss: 0.0076823048293590546\n",
            "Epoch 27, Step 82, Loss: 0.006593676749616861\n",
            "Epoch 27, Step 83, Loss: 0.006886570248752832\n",
            "Epoch 27, Step 84, Loss: 0.007180672604590654\n",
            "Epoch 27, Step 85, Loss: 0.00751701183617115\n",
            "Epoch 27, Step 86, Loss: 0.007410395424813032\n",
            "Train Metric MRRs: 0.30506013674338367\n",
            "Train Metric MAPs: 0.34498324824112264\n",
            "Validation Metric MRRs: 0.21091083103402747\n",
            "Validation Metric MAPs: 0.2348878459410383\n",
            "Epoch 28, Step 1, Loss: 0.01713603176176548\n",
            "Epoch 28, Step 2, Loss: 0.018392086029052734\n",
            "Epoch 28, Step 3, Loss: 0.015427830629050732\n",
            "Epoch 28, Step 4, Loss: 0.013676275499165058\n",
            "Epoch 28, Step 5, Loss: 0.009759878739714622\n",
            "Epoch 28, Step 6, Loss: 0.0076269833371043205\n",
            "Epoch 28, Step 7, Loss: 0.00583081366494298\n",
            "Epoch 28, Step 8, Loss: 0.005814606789499521\n",
            "Epoch 28, Step 9, Loss: 0.005163328256458044\n",
            "Epoch 28, Step 10, Loss: 0.005258629564195871\n",
            "Epoch 28, Step 11, Loss: 0.005971166305243969\n",
            "Epoch 28, Step 12, Loss: 0.0052416264079511166\n",
            "Epoch 28, Step 13, Loss: 0.00480450177565217\n",
            "Epoch 28, Step 14, Loss: 0.003834769129753113\n",
            "Epoch 28, Step 15, Loss: 0.004933645483106375\n",
            "Epoch 28, Step 16, Loss: 0.005224676802754402\n",
            "Epoch 28, Step 17, Loss: 0.005933609791100025\n",
            "Epoch 28, Step 18, Loss: 0.007393469102680683\n",
            "Epoch 28, Step 19, Loss: 0.008763156831264496\n",
            "Epoch 28, Step 20, Loss: 0.008714701980352402\n",
            "Epoch 28, Step 21, Loss: 0.0096617192029953\n",
            "Epoch 28, Step 22, Loss: 0.00965516734868288\n",
            "Epoch 28, Step 23, Loss: 0.010071313008666039\n",
            "Epoch 28, Step 24, Loss: 0.010916790924966335\n",
            "Epoch 28, Step 25, Loss: 0.01070447638630867\n",
            "Epoch 28, Step 26, Loss: 0.01028982549905777\n",
            "Epoch 28, Step 27, Loss: 0.008844280615448952\n",
            "Epoch 28, Step 28, Loss: 0.009307253174483776\n",
            "Epoch 28, Step 29, Loss: 0.009993225336074829\n",
            "Epoch 28, Step 30, Loss: 0.008192868903279305\n",
            "Epoch 28, Step 31, Loss: 0.010374643839895725\n",
            "Epoch 28, Step 32, Loss: 0.008989272639155388\n",
            "Epoch 28, Step 33, Loss: 0.009718750603497028\n",
            "Epoch 28, Step 34, Loss: 0.008827568031847477\n",
            "Epoch 28, Step 35, Loss: 0.008726362138986588\n",
            "Epoch 28, Step 36, Loss: 0.008413867093622684\n",
            "Epoch 28, Step 37, Loss: 0.008480081334710121\n",
            "Epoch 28, Step 38, Loss: 0.007434324827045202\n",
            "Epoch 28, Step 39, Loss: 0.0069697885774075985\n",
            "Epoch 28, Step 40, Loss: 0.0075090560130774975\n",
            "Epoch 28, Step 41, Loss: 0.006067956332117319\n",
            "Epoch 28, Step 42, Loss: 0.005969498306512833\n",
            "Epoch 28, Step 43, Loss: 0.005593252368271351\n",
            "Epoch 28, Step 44, Loss: 0.005787375383079052\n",
            "Epoch 28, Step 45, Loss: 0.0064366660080850124\n",
            "Epoch 28, Step 46, Loss: 0.00584343820810318\n",
            "Epoch 28, Step 47, Loss: 0.006096536759287119\n",
            "Epoch 28, Step 48, Loss: 0.005597307812422514\n",
            "Epoch 28, Step 49, Loss: 0.005650534760206938\n",
            "Epoch 28, Step 50, Loss: 0.00510009890422225\n",
            "Epoch 28, Step 51, Loss: 0.005575079936534166\n",
            "Epoch 28, Step 52, Loss: 0.005213354714214802\n",
            "Epoch 28, Step 53, Loss: 0.006453234702348709\n",
            "Epoch 28, Step 54, Loss: 0.005374590400606394\n",
            "Epoch 28, Step 55, Loss: 0.006010616663843393\n",
            "Epoch 28, Step 56, Loss: 0.006638446357101202\n",
            "Epoch 28, Step 57, Loss: 0.0061540910974144936\n",
            "Epoch 28, Step 58, Loss: 0.006280215922743082\n",
            "Epoch 28, Step 59, Loss: 0.005726068280637264\n",
            "Epoch 28, Step 60, Loss: 0.006100754253566265\n",
            "Epoch 28, Step 61, Loss: 0.007553822826594114\n",
            "Epoch 28, Step 62, Loss: 0.005933684762567282\n",
            "Epoch 28, Step 63, Loss: 0.009191365912556648\n",
            "Epoch 28, Step 64, Loss: 0.005769903305917978\n",
            "Epoch 28, Step 65, Loss: 0.005785974208265543\n",
            "Epoch 28, Step 66, Loss: 0.005514432676136494\n",
            "Epoch 28, Step 67, Loss: 0.006028828211128712\n",
            "Epoch 28, Step 68, Loss: 0.005228432361036539\n",
            "Epoch 28, Step 69, Loss: 0.005420996807515621\n",
            "Epoch 28, Step 70, Loss: 0.0061193895526230335\n",
            "Epoch 28, Step 71, Loss: 0.006282786838710308\n",
            "Epoch 28, Step 72, Loss: 0.0050995792262256145\n",
            "Epoch 28, Step 73, Loss: 0.005103180184960365\n",
            "Epoch 28, Step 74, Loss: 0.0051638404838740826\n",
            "Epoch 28, Step 75, Loss: 0.005119078326970339\n",
            "Epoch 28, Step 76, Loss: 0.005147975403815508\n",
            "Epoch 28, Step 77, Loss: 0.0055987159721553326\n",
            "Epoch 28, Step 78, Loss: 0.004865649621933699\n",
            "Epoch 28, Step 79, Loss: 0.005162481218576431\n",
            "Epoch 28, Step 80, Loss: 0.005365543533116579\n",
            "Epoch 28, Step 81, Loss: 0.007733489386737347\n",
            "Epoch 28, Step 82, Loss: 0.006684232037514448\n",
            "Epoch 28, Step 83, Loss: 0.0069960025139153\n",
            "Epoch 28, Step 84, Loss: 0.007375373505055904\n",
            "Epoch 28, Step 85, Loss: 0.007527939975261688\n",
            "Epoch 28, Step 86, Loss: 0.007386499084532261\n",
            "Train Metric MRRs: 0.30725207138925986\n",
            "Train Metric MAPs: 0.34725148265707145\n",
            "Validation Metric MRRs: 0.20995695122784455\n",
            "Validation Metric MAPs: 0.23377321377486499\n",
            "Early stopping triggered!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.restore_best_checkpoint()\n",
        "trainer.validate('Validation')\n",
        "trainer.validate('Test')"
      ],
      "metadata": {
        "id": "brUW-S9GWkh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b491382-191b-409d-8c74-7f90a3110db9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Metric MRRs: 0.2153835320584412\n",
            "Validation Metric MAPs: 0.23239875567549842\n",
            "Test Metric MRRs: 0.26920571903257345\n",
            "Test Metric MAPs: 0.2872764078421189\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.26920571903257345, 0.2872764078421189)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}