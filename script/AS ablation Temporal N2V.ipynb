{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19020,"status":"ok","timestamp":1693381361528,"user":{"displayName":"Quandary zhang","userId":"14333747180371872055"},"user_tz":-60},"id":"I7IVQVEDWEWB","outputId":"6fba894d-76b4-4b17-97e7-5963fa666d42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/mlds\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/mlds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ElDsah6AWVwk"},"outputs":[],"source":["import data_auto_sys as aus\n","import tasker_link_prediction as t_lp\n","from splitter import splitter\n","from models import Temporal_GCN\n","from trainer import Trainer\n","import yaml\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2341,"status":"ok","timestamp":1693381374315,"user":{"displayName":"Quandary zhang","userId":"14333747180371872055"},"user_tz":-60},"id":"doo1WcuyWZAC","outputId":"cd5bd692-874a-4833-ec3b-83ba20ad7176"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'adam_config': {'adam_beta_1': 0.9,\n","  'adam_beta_2': 0.999,\n","  'adam_epsilon': 1e-07,\n","  'adam_learning_rate': 0.005},\n"," 'model_path': 'models/AS_Ablation_N2V_Temporal/',\n"," 'classifier_hidden_size': 20,\n"," 'ffn_fusion_size': 10,\n"," 'ffn_hidden_size': 10,\n"," 'gcn_fusion_size': 20,\n"," 'spatial_hidden_size': 20,\n"," 'spatial_input_dim': 128,\n"," 'temporal_hidden_size': 20,\n"," 'temporal_input_dim': 200,\n"," 'num_epochs': 1000,\n"," 'patience': 10,\n"," 'major_threshold': None,\n"," 'train_proportion': 0.7,\n"," 'dev_proportion': 0.1,\n"," 'data_path': 'data/AS-733/',\n"," 'prep_data_path': 'prep_data/AS733_neg/'}"]},"metadata":{},"execution_count":3}],"source":["prep = False\n","\n","with open('config/config_AS_ablation_temporal_n2v.yaml', 'r') as file:\n","    config = yaml.safe_load(file)\n","\n","model_path = config['model_path']\n","\n","if not os.path.exists(model_path):\n","    os.mkdir(model_path)\n","    os.mkdir(model_path + \"best_checkpoints/\")\n","    os.mkdir(model_path + \"latest_checkpoints/\")\n","\n","with open(model_path + 'config.yaml', 'w') as file:\n","    yaml.dump(config, file)\n","\n","config"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46540,"status":"ok","timestamp":1693381420854,"user":{"displayName":"Quandary zhang","userId":"14333747180371872055"},"user_tz":-60},"id":"unCQbLC8Wb0S","outputId":"43039e43-aa0c-40fc-ca81-4de0bc5efccb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset splits sizes:  train 60 dev 10 test 20\n"]}],"source":["data = aus.Autonomous_System_Dataset(config['data_path'])\n","tasker = t_lp.Link_Pred_Tasker(data, path=config['prep_data_path'], prep=prep,\n","                               embs_dim=config['spatial_input_dim'], temp_dim=int(config['temporal_input_dim']/10),\n","                               major_threshold=config['major_threshold'], smart_neg_sampling=True, neg_sample=1000)\n","\n","splitter_as = splitter(tasker, train_proportion = config['train_proportion'], dev_proportion = config['dev_proportion'])\n","\n","model= Temporal_GCN(temporal_input_dim=config['temporal_input_dim'],\n","                 temporal_hidden_size=config['temporal_hidden_size'],\n","                 classifier_hidden_size=config['classifier_hidden_size'],\n","                 gcn_fusion_size=config['gcn_fusion_size'])\n","\n","trainer = Trainer(model=model, splitter=splitter_as, model_path=model_path, adam_config=config['adam_config'], patience=config['patience'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wuhZRFQVognD","executionInfo":{"status":"ok","timestamp":1693390806896,"user_tz":-60,"elapsed":9384376,"user":{"displayName":"Quandary zhang","userId":"14333747180371872055"}},"outputId":"588a91ec-c267-4aab-8cd2-54c7b8d55ab4"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7ae05a2835b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7ae05a2835b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 44, Step 1, Loss: 0.002372744493186474\n","Epoch 44, Step 2, Loss: 0.0023271767422556877\n","Epoch 44, Step 3, Loss: 0.0023334056604653597\n","Epoch 44, Step 4, Loss: 0.0023941288236528635\n","Epoch 44, Step 5, Loss: 0.0024161171168088913\n","Epoch 44, Step 6, Loss: 0.002338219666853547\n","Epoch 44, Step 7, Loss: 0.0022309082560241222\n","Epoch 44, Step 8, Loss: 0.002268332289531827\n","Epoch 44, Step 9, Loss: 0.0022418340668082237\n","Epoch 44, Step 10, Loss: 0.0023802067153155804\n","Epoch 44, Step 11, Loss: 0.0022850711829960346\n","Epoch 44, Step 12, Loss: 0.00221804715692997\n","Epoch 44, Step 13, Loss: 0.0021991734392941\n","Epoch 44, Step 14, Loss: 0.002169219544157386\n","Epoch 44, Step 15, Loss: 0.002205796539783478\n","Epoch 44, Step 16, Loss: 0.002335520228371024\n","Epoch 44, Step 17, Loss: 0.002327550668269396\n","Epoch 44, Step 18, Loss: 0.0027176185976713896\n","Epoch 44, Step 19, Loss: 0.002658173907548189\n","Epoch 44, Step 20, Loss: 0.0026219459250569344\n","Epoch 44, Step 21, Loss: 0.0025568734854459763\n","Epoch 44, Step 22, Loss: 0.002552023157477379\n","Epoch 44, Step 23, Loss: 0.0025179609656333923\n","Epoch 44, Step 24, Loss: 0.0024791196919977665\n","Epoch 44, Step 25, Loss: 0.0023567434400320053\n","Epoch 44, Step 26, Loss: 0.002407615538686514\n","Epoch 44, Step 27, Loss: 0.0024505537003278732\n","Epoch 44, Step 28, Loss: 0.00228821556083858\n","Epoch 44, Step 29, Loss: 0.002306458307430148\n","Epoch 44, Step 30, Loss: 0.002213089494034648\n","Epoch 44, Step 31, Loss: 0.002280809683725238\n","Epoch 44, Step 32, Loss: 0.0023147964384406805\n","Epoch 44, Step 33, Loss: 0.0023483172990381718\n","Epoch 44, Step 34, Loss: 0.002349664457142353\n","Epoch 44, Step 35, Loss: 0.002300129970535636\n","Epoch 44, Step 36, Loss: 0.0022358852438628674\n","Epoch 44, Step 37, Loss: 0.002175462432205677\n","Epoch 44, Step 38, Loss: 0.00223319954238832\n","Epoch 44, Step 39, Loss: 0.0022184792906045914\n","Epoch 44, Step 40, Loss: 0.0022341369185596704\n","Epoch 44, Step 41, Loss: 0.0021745439153164625\n","Epoch 44, Step 42, Loss: 0.002235355554148555\n","Epoch 44, Step 43, Loss: 0.002269149525091052\n","Epoch 44, Step 44, Loss: 0.0021518603898584843\n","Epoch 44, Step 45, Loss: 0.0022232639603316784\n","Epoch 44, Step 46, Loss: 0.0022540383506566286\n","Epoch 44, Step 47, Loss: 0.0023153715301305056\n","Epoch 44, Step 48, Loss: 0.002212247345596552\n","Epoch 44, Step 49, Loss: 0.002236751141026616\n","Epoch 44, Step 50, Loss: 0.0021747597493231297\n","Epoch 44, Step 51, Loss: 0.002160174073651433\n","Epoch 44, Step 52, Loss: 0.002364565385505557\n","Epoch 44, Step 53, Loss: 0.0022699895780533552\n","Epoch 44, Step 54, Loss: 0.0022346945479512215\n","Epoch 44, Step 55, Loss: 0.002206510864198208\n","Epoch 44, Step 56, Loss: 0.0021948220673948526\n","Epoch 44, Step 57, Loss: 0.0022860579192638397\n","Epoch 44, Step 58, Loss: 0.002342923078685999\n","Epoch 44, Step 59, Loss: 0.0023395719472318888\n","Epoch 44, Step 60, Loss: 0.0024242016952484846\n","Train Metric MRRs: 0.6731364971166552\n","Train Metric MAPs: 0.3853959971529846\n","Validation Metric MRRs: 0.6386047041495778\n","Validation Metric MAPs: 0.3084945396081421\n","Epoch 45, Step 1, Loss: 0.002380352234467864\n","Epoch 45, Step 2, Loss: 0.002333001932129264\n","Epoch 45, Step 3, Loss: 0.002323756692931056\n","Epoch 45, Step 4, Loss: 0.002376720542088151\n","Epoch 45, Step 5, Loss: 0.0024249451234936714\n","Epoch 45, Step 6, Loss: 0.002359961625188589\n","Epoch 45, Step 7, Loss: 0.0022499514743685722\n","Epoch 45, Step 8, Loss: 0.002269115997478366\n","Epoch 45, Step 9, Loss: 0.002222940092906356\n","Epoch 45, Step 10, Loss: 0.002343773376196623\n","Epoch 45, Step 11, Loss: 0.0022981599904596806\n","Epoch 45, Step 12, Loss: 0.0022230239119380713\n","Epoch 45, Step 13, Loss: 0.0021884068846702576\n","Epoch 45, Step 14, Loss: 0.002147129038348794\n","Epoch 45, Step 15, Loss: 0.002164136851206422\n","Epoch 45, Step 16, Loss: 0.002319113817065954\n","Epoch 45, Step 17, Loss: 0.0023530437611043453\n","Epoch 45, Step 18, Loss: 0.0027584617491811514\n","Epoch 45, Step 19, Loss: 0.0026631082873791456\n","Epoch 45, Step 20, Loss: 0.0025828080251812935\n","Epoch 45, Step 21, Loss: 0.002492290921509266\n","Epoch 45, Step 22, Loss: 0.002518035238608718\n","Epoch 45, Step 23, Loss: 0.002520211273804307\n","Epoch 45, Step 24, Loss: 0.0024919447023421526\n","Epoch 45, Step 25, Loss: 0.0023560887202620506\n","Epoch 45, Step 26, Loss: 0.0023318524472415447\n","Epoch 45, Step 27, Loss: 0.002358340658247471\n","Epoch 45, Step 28, Loss: 0.00227787671610713\n","Epoch 45, Step 29, Loss: 0.0023060005623847246\n","Epoch 45, Step 30, Loss: 0.0022278863471001387\n","Epoch 45, Step 31, Loss: 0.0022884479258209467\n","Epoch 45, Step 32, Loss: 0.0022629532031714916\n","Epoch 45, Step 33, Loss: 0.0023145400919020176\n","Epoch 45, Step 34, Loss: 0.0023192039225250483\n","Epoch 45, Step 35, Loss: 0.002269642660394311\n","Epoch 45, Step 36, Loss: 0.002208094811066985\n","Epoch 45, Step 37, Loss: 0.0021647473331540823\n","Epoch 45, Step 38, Loss: 0.0022040202748030424\n","Epoch 45, Step 39, Loss: 0.002187910955399275\n","Epoch 45, Step 40, Loss: 0.002194032073020935\n","Epoch 45, Step 41, Loss: 0.0021560166496783495\n","Epoch 45, Step 42, Loss: 0.002178353723138571\n","Epoch 45, Step 43, Loss: 0.0022317885886877775\n","Epoch 45, Step 44, Loss: 0.0021607265807688236\n","Epoch 45, Step 45, Loss: 0.002221622969955206\n","Epoch 45, Step 46, Loss: 0.002236010739579797\n","Epoch 45, Step 47, Loss: 0.002243370283395052\n","Epoch 45, Step 48, Loss: 0.0021801136899739504\n","Epoch 45, Step 49, Loss: 0.00224767392501235\n","Epoch 45, Step 50, Loss: 0.0021835078950971365\n","Epoch 45, Step 51, Loss: 0.002144040074199438\n","Epoch 45, Step 52, Loss: 0.00230969930998981\n","Epoch 45, Step 53, Loss: 0.0022213151678442955\n","Epoch 45, Step 54, Loss: 0.0022150487639009953\n","Epoch 45, Step 55, Loss: 0.002214235020801425\n","Epoch 45, Step 56, Loss: 0.0022062889765948057\n","Epoch 45, Step 57, Loss: 0.0022659292444586754\n","Epoch 45, Step 58, Loss: 0.0023444651160389185\n","Epoch 45, Step 59, Loss: 0.0023498411756008863\n","Epoch 45, Step 60, Loss: 0.0024333414621651173\n","Train Metric MRRs: 0.6742346930556384\n","Train Metric MAPs: 0.3786500995249128\n","Validation Metric MRRs: 0.6425718421304989\n","Validation Metric MAPs: 0.3112209388470526\n","Epoch 46, Step 1, Loss: 0.00238189660012722\n","Epoch 46, Step 2, Loss: 0.002329176524654031\n","Epoch 46, Step 3, Loss: 0.002321958541870117\n","Epoch 46, Step 4, Loss: 0.0023730408865958452\n","Epoch 46, Step 5, Loss: 0.002402479527518153\n","Epoch 46, Step 6, Loss: 0.0023360582999885082\n","Epoch 46, Step 7, Loss: 0.0022621406242251396\n","Epoch 46, Step 8, Loss: 0.0022657178342342377\n","Epoch 46, Step 9, Loss: 0.002216469496488571\n","Epoch 46, Step 10, Loss: 0.0023367502726614475\n","Epoch 46, Step 11, Loss: 0.0023068678565323353\n","Epoch 46, Step 12, Loss: 0.002228162484243512\n","Epoch 46, Step 13, Loss: 0.0022295538801699877\n","Epoch 46, Step 14, Loss: 0.002167046070098877\n","Epoch 46, Step 15, Loss: 0.0021534443367272615\n","Epoch 46, Step 16, Loss: 0.0023029895965009928\n","Epoch 46, Step 17, Loss: 0.002354438416659832\n","Epoch 46, Step 18, Loss: 0.0027705193497240543\n","Epoch 46, Step 19, Loss: 0.0026852183509618044\n","Epoch 46, Step 20, Loss: 0.0025803495664149523\n","Epoch 46, Step 21, Loss: 0.0024579770397394896\n","Epoch 46, Step 22, Loss: 0.002464521210640669\n","Epoch 46, Step 23, Loss: 0.0025199975352734327\n","Epoch 46, Step 24, Loss: 0.0024745017290115356\n","Epoch 46, Step 25, Loss: 0.002351357601583004\n","Epoch 46, Step 26, Loss: 0.002300300169736147\n","Epoch 46, Step 27, Loss: 0.0023021046072244644\n","Epoch 46, Step 28, Loss: 0.0022521994542330503\n","Epoch 46, Step 29, Loss: 0.002315741963684559\n","Epoch 46, Step 30, Loss: 0.0022423674818128347\n","Epoch 46, Step 31, Loss: 0.002291157841682434\n","Epoch 46, Step 32, Loss: 0.002223583171144128\n","Epoch 46, Step 33, Loss: 0.0022742580622434616\n","Epoch 46, Step 34, Loss: 0.0022805172484368086\n","Epoch 46, Step 35, Loss: 0.002243454335257411\n","Epoch 46, Step 36, Loss: 0.0022479756735265255\n","Epoch 46, Step 37, Loss: 0.002152026863768697\n","Epoch 46, Step 38, Loss: 0.0021848396863788366\n","Epoch 46, Step 39, Loss: 0.002172333188354969\n","Epoch 46, Step 40, Loss: 0.002228350378572941\n","Epoch 46, Step 41, Loss: 0.00220889993943274\n","Epoch 46, Step 42, Loss: 0.002194694010540843\n","Epoch 46, Step 43, Loss: 0.00223736884072423\n","Epoch 46, Step 44, Loss: 0.0021619845647364855\n","Epoch 46, Step 45, Loss: 0.0021788745652884245\n","Epoch 46, Step 46, Loss: 0.0022575873881578445\n","Epoch 46, Step 47, Loss: 0.0022590486332774162\n","Epoch 46, Step 48, Loss: 0.0021803074050694704\n","Epoch 46, Step 49, Loss: 0.002232101047411561\n","Epoch 46, Step 50, Loss: 0.002155305352061987\n","Epoch 46, Step 51, Loss: 0.0021398935932666063\n","Epoch 46, Step 52, Loss: 0.002287433482706547\n","Epoch 46, Step 53, Loss: 0.0022380477748811245\n","Epoch 46, Step 54, Loss: 0.00222612451761961\n","Epoch 46, Step 55, Loss: 0.0022412342950701714\n","Epoch 46, Step 56, Loss: 0.002232405822724104\n","Epoch 46, Step 57, Loss: 0.0022574299946427345\n","Epoch 46, Step 58, Loss: 0.0023491629399359226\n","Epoch 46, Step 59, Loss: 0.0023574205115437508\n","Epoch 46, Step 60, Loss: 0.0024478912819176912\n","Train Metric MRRs: 0.6751237246807416\n","Train Metric MAPs: 0.3759143072195383\n","Validation Metric MRRs: 0.6428327568151947\n","Validation Metric MAPs: 0.30361821602650974\n","Epoch 47, Step 1, Loss: 0.002363620325922966\n","Epoch 47, Step 2, Loss: 0.0022968791890889406\n","Epoch 47, Step 3, Loss: 0.002276951214298606\n","Epoch 47, Step 4, Loss: 0.002377222292125225\n","Epoch 47, Step 5, Loss: 0.002404057653620839\n","Epoch 47, Step 6, Loss: 0.002342376159504056\n","Epoch 47, Step 7, Loss: 0.002291356446221471\n","Epoch 47, Step 8, Loss: 0.002295729471370578\n","Epoch 47, Step 9, Loss: 0.002224407158792019\n","Epoch 47, Step 10, Loss: 0.002310176845639944\n","Epoch 47, Step 11, Loss: 0.0022706224117428064\n","Epoch 47, Step 12, Loss: 0.002231740392744541\n","Epoch 47, Step 13, Loss: 0.002259321976453066\n","Epoch 47, Step 14, Loss: 0.00218065083026886\n","Epoch 47, Step 15, Loss: 0.00217660260386765\n","Epoch 47, Step 16, Loss: 0.0022423500195145607\n","Epoch 47, Step 17, Loss: 0.0023305851500481367\n","Epoch 47, Step 18, Loss: 0.0028098030015826225\n","Epoch 47, Step 19, Loss: 0.0027235967572778463\n","Epoch 47, Step 20, Loss: 0.002640916034579277\n","Epoch 47, Step 21, Loss: 0.0024669645354151726\n","Epoch 47, Step 22, Loss: 0.002414983930066228\n","Epoch 47, Step 23, Loss: 0.0025115569587796926\n","Epoch 47, Step 24, Loss: 0.002507791155949235\n","Epoch 47, Step 25, Loss: 0.0024395433720201254\n","Epoch 47, Step 26, Loss: 0.002331090858206153\n","Epoch 47, Step 27, Loss: 0.0022641837131232023\n","Epoch 47, Step 28, Loss: 0.0022019867319613695\n","Epoch 47, Step 29, Loss: 0.0022665411233901978\n","Epoch 47, Step 30, Loss: 0.0022302318830043077\n","Epoch 47, Step 31, Loss: 0.0023151966743171215\n","Epoch 47, Step 32, Loss: 0.002265126444399357\n","Epoch 47, Step 33, Loss: 0.0022871517576277256\n","Epoch 47, Step 34, Loss: 0.002271762816235423\n","Epoch 47, Step 35, Loss: 0.0022426231298595667\n","Epoch 47, Step 36, Loss: 0.002238678513094783\n","Epoch 47, Step 37, Loss: 0.002207853365689516\n","Epoch 47, Step 38, Loss: 0.002230638638138771\n","Epoch 47, Step 39, Loss: 0.0021793281193822622\n","Epoch 47, Step 40, Loss: 0.002180264564231038\n","Epoch 47, Step 41, Loss: 0.0022270455956459045\n","Epoch 47, Step 42, Loss: 0.0022455169819295406\n","Epoch 47, Step 43, Loss: 0.0022335348185151815\n","Epoch 47, Step 44, Loss: 0.0021500454749912024\n","Epoch 47, Step 45, Loss: 0.002160100731998682\n","Epoch 47, Step 46, Loss: 0.0022145567927509546\n","Epoch 47, Step 47, Loss: 0.0022493775468319654\n","Epoch 47, Step 48, Loss: 0.002176715526729822\n","Epoch 47, Step 49, Loss: 0.002256481908261776\n","Epoch 47, Step 50, Loss: 0.0021949056535959244\n","Epoch 47, Step 51, Loss: 0.002098409691825509\n","Epoch 47, Step 52, Loss: 0.0022312321234494448\n","Epoch 47, Step 53, Loss: 0.002244447823613882\n","Epoch 47, Step 54, Loss: 0.002265093382447958\n","Epoch 47, Step 55, Loss: 0.0022605769336223602\n","Epoch 47, Step 56, Loss: 0.002234836108982563\n","Epoch 47, Step 57, Loss: 0.0022309902124106884\n","Epoch 47, Step 58, Loss: 0.0023084681015461683\n","Epoch 47, Step 59, Loss: 0.002380350371822715\n","Epoch 47, Step 60, Loss: 0.002506412798538804\n","Train Metric MRRs: 0.6762157309067466\n","Train Metric MAPs: 0.37289232928864063\n","Validation Metric MRRs: 0.6432224653968717\n","Validation Metric MAPs: 0.2631265144846095\n","Epoch 48, Step 1, Loss: 0.002496577799320221\n","Epoch 48, Step 2, Loss: 0.002321698470041156\n","Epoch 48, Step 3, Loss: 0.002258245600387454\n","Epoch 48, Step 4, Loss: 0.002354346914216876\n","Epoch 48, Step 5, Loss: 0.002408257918432355\n","Epoch 48, Step 6, Loss: 0.002396980067715049\n","Epoch 48, Step 7, Loss: 0.002341978484764695\n","Epoch 48, Step 8, Loss: 0.0022965858224779367\n","Epoch 48, Step 9, Loss: 0.0022144883405417204\n","Epoch 48, Step 10, Loss: 0.0022796073462814093\n","Epoch 48, Step 11, Loss: 0.0022625690326094627\n","Epoch 48, Step 12, Loss: 0.002253870479762554\n","Epoch 48, Step 13, Loss: 0.002289083320647478\n","Epoch 48, Step 14, Loss: 0.0021976602729409933\n","Epoch 48, Step 15, Loss: 0.0021809737663716078\n","Epoch 48, Step 16, Loss: 0.0022337466944009066\n","Epoch 48, Step 17, Loss: 0.0022952056024223566\n","Epoch 48, Step 18, Loss: 0.0027415987569838762\n","Epoch 48, Step 19, Loss: 0.0027178972959518433\n","Epoch 48, Step 20, Loss: 0.002646390348672867\n","Epoch 48, Step 21, Loss: 0.0025104163214564323\n","Epoch 48, Step 22, Loss: 0.0024148966185748577\n","Epoch 48, Step 23, Loss: 0.0024908515624701977\n","Epoch 48, Step 24, Loss: 0.002440081909298897\n","Epoch 48, Step 25, Loss: 0.0024391114711761475\n","Epoch 48, Step 26, Loss: 0.0023646135814487934\n","Epoch 48, Step 27, Loss: 0.002255302155390382\n","Epoch 48, Step 28, Loss: 0.0021906793117523193\n","Epoch 48, Step 29, Loss: 0.0022525598760694265\n","Epoch 48, Step 30, Loss: 0.0022014323621988297\n","Epoch 48, Step 31, Loss: 0.002327886875718832\n","Epoch 48, Step 32, Loss: 0.002218744484707713\n","Epoch 48, Step 33, Loss: 0.0022578046191483736\n","Epoch 48, Step 34, Loss: 0.0022498928010463715\n","Epoch 48, Step 35, Loss: 0.002195190405473113\n","Epoch 48, Step 36, Loss: 0.0022093476727604866\n","Epoch 48, Step 37, Loss: 0.0022215561475604773\n","Epoch 48, Step 38, Loss: 0.00225094729103148\n","Epoch 48, Step 39, Loss: 0.0021691264118999243\n","Epoch 48, Step 40, Loss: 0.002189488848671317\n","Epoch 48, Step 41, Loss: 0.00222613918595016\n","Epoch 48, Step 42, Loss: 0.0022876362781971693\n","Epoch 48, Step 43, Loss: 0.0023297134321182966\n","Epoch 48, Step 44, Loss: 0.0021896380931138992\n","Epoch 48, Step 45, Loss: 0.002189949620515108\n","Epoch 48, Step 46, Loss: 0.0022475977893918753\n","Epoch 48, Step 47, Loss: 0.002298068255186081\n","Epoch 48, Step 48, Loss: 0.0022604218684136868\n","Epoch 48, Step 49, Loss: 0.002343337284401059\n","Epoch 48, Step 50, Loss: 0.0022390137892216444\n","Epoch 48, Step 51, Loss: 0.0021456279791891575\n","Epoch 48, Step 52, Loss: 0.0022473654244095087\n","Epoch 48, Step 53, Loss: 0.002219923073425889\n","Epoch 48, Step 54, Loss: 0.002307961229234934\n","Epoch 48, Step 55, Loss: 0.0023241289891302586\n","Epoch 48, Step 56, Loss: 0.0022986221592873335\n","Epoch 48, Step 57, Loss: 0.0022826821077615023\n","Epoch 48, Step 58, Loss: 0.0023067838046699762\n","Epoch 48, Step 59, Loss: 0.0024123205803334713\n","Epoch 48, Step 60, Loss: 0.0025316993705928326\n","Train Metric MRRs: 0.6750916076565576\n","Train Metric MAPs: 0.3637164155488298\n","Validation Metric MRRs: 0.6400737076954081\n","Validation Metric MAPs: 0.24709855154112853\n","Epoch 49, Step 1, Loss: 0.0024950923398137093\n","Epoch 49, Step 2, Loss: 0.0023774728178977966\n","Epoch 49, Step 3, Loss: 0.002252989448606968\n","Epoch 49, Step 4, Loss: 0.0023053179029375315\n","Epoch 49, Step 5, Loss: 0.002376232761889696\n","Epoch 49, Step 6, Loss: 0.0023787471000105143\n","Epoch 49, Step 7, Loss: 0.002378503791987896\n","Epoch 49, Step 8, Loss: 0.002358351368457079\n","Epoch 49, Step 9, Loss: 0.002227968070656061\n","Epoch 49, Step 10, Loss: 0.0022919843904674053\n","Epoch 49, Step 11, Loss: 0.0022469160612672567\n","Epoch 49, Step 12, Loss: 0.002244106959551573\n","Epoch 49, Step 13, Loss: 0.002363337902352214\n","Epoch 49, Step 14, Loss: 0.0022718184627592564\n","Epoch 49, Step 15, Loss: 0.002232546918094158\n","Epoch 49, Step 16, Loss: 0.0022781454026699066\n","Epoch 49, Step 17, Loss: 0.0022918342147022486\n","Epoch 49, Step 18, Loss: 0.002689323155209422\n","Epoch 49, Step 19, Loss: 0.0026690762024372816\n","Epoch 49, Step 20, Loss: 0.0025830850936472416\n","Epoch 49, Step 21, Loss: 0.0025031499098986387\n","Epoch 49, Step 22, Loss: 0.0024071838706731796\n","Epoch 49, Step 23, Loss: 0.002462430391460657\n","Epoch 49, Step 24, Loss: 0.0024054045788943768\n","Epoch 49, Step 25, Loss: 0.0023922172840684652\n","Epoch 49, Step 26, Loss: 0.0023707877844572067\n","Epoch 49, Step 27, Loss: 0.0022940454073250294\n","Epoch 49, Step 28, Loss: 0.002186133759096265\n","Epoch 49, Step 29, Loss: 0.002210369799286127\n","Epoch 49, Step 30, Loss: 0.002217697910964489\n","Epoch 49, Step 31, Loss: 0.0022793523967266083\n","Epoch 49, Step 32, Loss: 0.0022593524772673845\n","Epoch 49, Step 33, Loss: 0.0022713779471814632\n","Epoch 49, Step 34, Loss: 0.002211875282227993\n","Epoch 49, Step 35, Loss: 0.0021895102690905333\n","Epoch 49, Step 36, Loss: 0.0022228281013667583\n","Epoch 49, Step 37, Loss: 0.002232479164376855\n","Epoch 49, Step 38, Loss: 0.002251105383038521\n","Epoch 49, Step 39, Loss: 0.002183710690587759\n","Epoch 49, Step 40, Loss: 0.0022017827723175287\n","Epoch 49, Step 41, Loss: 0.002111119916662574\n","Epoch 49, Step 42, Loss: 0.0021862336434423923\n","Epoch 49, Step 43, Loss: 0.002338445046916604\n","Epoch 49, Step 44, Loss: 0.0022445048671215773\n","Epoch 49, Step 45, Loss: 0.0022355171386152506\n","Epoch 49, Step 46, Loss: 0.0022228786256164312\n","Epoch 49, Step 47, Loss: 0.002166320336982608\n","Epoch 49, Step 48, Loss: 0.002242916962131858\n","Epoch 49, Step 49, Loss: 0.0024493830278515816\n","Epoch 49, Step 50, Loss: 0.002392563968896866\n","Epoch 49, Step 51, Loss: 0.0021853770595043898\n","Epoch 49, Step 52, Loss: 0.0022358782589435577\n","Epoch 49, Step 53, Loss: 0.002158536808565259\n","Epoch 49, Step 54, Loss: 0.002338275546208024\n","Epoch 49, Step 55, Loss: 0.0024022278375923634\n","Epoch 49, Step 56, Loss: 0.0023930033203214407\n","Epoch 49, Step 57, Loss: 0.0023506509605795145\n","Epoch 49, Step 58, Loss: 0.002254477236419916\n","Epoch 49, Step 59, Loss: 0.002316795289516449\n","Epoch 49, Step 60, Loss: 0.0025244716089218855\n","Train Metric MRRs: 0.6754670039030138\n","Train Metric MAPs: 0.36753639295929186\n","Validation Metric MRRs: 0.6358697354475997\n","Validation Metric MAPs: 0.2354125180340476\n","Epoch 50, Step 1, Loss: 0.0026891690213233232\n","Epoch 50, Step 2, Loss: 0.002604063833132386\n","Epoch 50, Step 3, Loss: 0.002370760776102543\n","Epoch 50, Step 4, Loss: 0.0022756692487746477\n","Epoch 50, Step 5, Loss: 0.0023411933798342943\n","Epoch 50, Step 6, Loss: 0.002471320563927293\n","Epoch 50, Step 7, Loss: 0.0025227025616914034\n","Epoch 50, Step 8, Loss: 0.0025555253960192204\n","Epoch 50, Step 9, Loss: 0.0023307010997086763\n","Epoch 50, Step 10, Loss: 0.0023539657704532146\n","Epoch 50, Step 11, Loss: 0.002265194896608591\n","Epoch 50, Step 12, Loss: 0.002296799561008811\n","Epoch 50, Step 13, Loss: 0.002450818195939064\n","Epoch 50, Step 14, Loss: 0.0023545664735138416\n","Epoch 50, Step 15, Loss: 0.0023604072630405426\n","Epoch 50, Step 16, Loss: 0.002334864577278495\n","Epoch 50, Step 17, Loss: 0.002322370884940028\n","Epoch 50, Step 18, Loss: 0.0026928961742669344\n","Epoch 50, Step 19, Loss: 0.002666344866156578\n","Epoch 50, Step 20, Loss: 0.0026556486263871193\n","Epoch 50, Step 21, Loss: 0.0026062706019729376\n","Epoch 50, Step 22, Loss: 0.0025044141802936792\n","Epoch 50, Step 23, Loss: 0.002512169536203146\n","Epoch 50, Step 24, Loss: 0.002376348478719592\n","Epoch 50, Step 25, Loss: 0.0024179762694984674\n","Epoch 50, Step 26, Loss: 0.002422000514343381\n","Epoch 50, Step 27, Loss: 0.0023687041830271482\n","Epoch 50, Step 28, Loss: 0.0022638621740043163\n","Epoch 50, Step 29, Loss: 0.0022440082393586636\n","Epoch 50, Step 30, Loss: 0.0022360458970069885\n","Epoch 50, Step 31, Loss: 0.002283118898048997\n","Epoch 50, Step 32, Loss: 0.0022616470232605934\n","Epoch 50, Step 33, Loss: 0.002262315945699811\n","Epoch 50, Step 34, Loss: 0.0022881620097905397\n","Epoch 50, Step 35, Loss: 0.002228282857686281\n","Epoch 50, Step 36, Loss: 0.0022148785647004843\n","Epoch 50, Step 37, Loss: 0.002220368245616555\n","Epoch 50, Step 38, Loss: 0.0023708511143922806\n","Epoch 50, Step 39, Loss: 0.002320490777492523\n","Epoch 50, Step 40, Loss: 0.002345995046198368\n","Epoch 50, Step 41, Loss: 0.002229694277048111\n","Epoch 50, Step 42, Loss: 0.0022613233886659145\n","Epoch 50, Step 43, Loss: 0.002317811129614711\n","Epoch 50, Step 44, Loss: 0.0022878863383084536\n","Epoch 50, Step 45, Loss: 0.0024051766376942396\n","Epoch 50, Step 46, Loss: 0.002341227140277624\n","Epoch 50, Step 47, Loss: 0.002240244997665286\n","Epoch 50, Step 48, Loss: 0.0022040512412786484\n","Epoch 50, Step 49, Loss: 0.0023235080298036337\n","Epoch 50, Step 50, Loss: 0.0025101490318775177\n","Epoch 50, Step 51, Loss: 0.0024704306852072477\n","Epoch 50, Step 52, Loss: 0.002394512528553605\n","Epoch 50, Step 53, Loss: 0.002193534979596734\n","Epoch 50, Step 54, Loss: 0.002274326980113983\n","Epoch 50, Step 55, Loss: 0.002426827559247613\n","Epoch 50, Step 56, Loss: 0.002530500292778015\n","Epoch 50, Step 57, Loss: 0.0027014599181711674\n","Epoch 50, Step 58, Loss: 0.0025291014462709427\n","Epoch 50, Step 59, Loss: 0.002358619589358568\n","Epoch 50, Step 60, Loss: 0.0024122116155922413\n","Train Metric MRRs: 0.6713572717704799\n","Train Metric MAPs: 0.3699524790145556\n","Validation Metric MRRs: 0.6350561353360359\n","Validation Metric MAPs: 0.22726505702772032\n","Epoch 51, Step 1, Loss: 0.0026237349957227707\n","Epoch 51, Step 2, Loss: 0.0027534193359315395\n","Epoch 51, Step 3, Loss: 0.0028165653347969055\n","Epoch 51, Step 4, Loss: 0.002655356889590621\n","Epoch 51, Step 5, Loss: 0.0024091254454106092\n","Epoch 51, Step 6, Loss: 0.0023118057288229465\n","Epoch 51, Step 7, Loss: 0.0025552241131663322\n","Epoch 51, Step 8, Loss: 0.0027530137449502945\n","Epoch 51, Step 9, Loss: 0.002711657900363207\n","Epoch 51, Step 10, Loss: 0.0028244212735444307\n","Epoch 51, Step 11, Loss: 0.0025046623777598143\n","Epoch 51, Step 12, Loss: 0.00231335312128067\n","Epoch 51, Step 13, Loss: 0.002432068344205618\n","Epoch 51, Step 14, Loss: 0.002693047747015953\n","Epoch 51, Step 15, Loss: 0.0025862061884254217\n","Epoch 51, Step 16, Loss: 0.002630162751302123\n","Epoch 51, Step 17, Loss: 0.0024718448985368013\n","Epoch 51, Step 18, Loss: 0.002748870523646474\n","Epoch 51, Step 19, Loss: 0.0027875069063156843\n","Epoch 51, Step 20, Loss: 0.0027681547217071056\n","Epoch 51, Step 21, Loss: 0.0028306026943027973\n","Epoch 51, Step 22, Loss: 0.0027749466244131327\n","Epoch 51, Step 23, Loss: 0.0027000189293175936\n","Epoch 51, Step 24, Loss: 0.0025031620170921087\n","Epoch 51, Step 25, Loss: 0.0024556941352784634\n","Epoch 51, Step 26, Loss: 0.002508788136765361\n","Epoch 51, Step 27, Loss: 0.0024933202657848597\n","Epoch 51, Step 28, Loss: 0.002359362319111824\n","Epoch 51, Step 29, Loss: 0.0023956948425620794\n","Epoch 51, Step 30, Loss: 0.002360748127102852\n","Epoch 51, Step 31, Loss: 0.002362645696848631\n","Epoch 51, Step 32, Loss: 0.0022935138549655676\n","Epoch 51, Step 33, Loss: 0.002352012088522315\n","Epoch 51, Step 34, Loss: 0.002319866558536887\n","Epoch 51, Step 35, Loss: 0.00226033478975296\n","Epoch 51, Step 36, Loss: 0.002291438402608037\n","Epoch 51, Step 37, Loss: 0.0022466189693659544\n","Epoch 51, Step 38, Loss: 0.002440594369545579\n","Epoch 51, Step 39, Loss: 0.002277543768286705\n","Epoch 51, Step 40, Loss: 0.0023717929143458605\n","Epoch 51, Step 41, Loss: 0.0022523263469338417\n","Epoch 51, Step 42, Loss: 0.002290674950927496\n","Epoch 51, Step 43, Loss: 0.0024048020131886005\n","Epoch 51, Step 44, Loss: 0.002259349450469017\n","Epoch 51, Step 45, Loss: 0.0024513762909919024\n","Epoch 51, Step 46, Loss: 0.0024183988571166992\n","Epoch 51, Step 47, Loss: 0.002364500891417265\n","Epoch 51, Step 48, Loss: 0.0022887291852384806\n","Epoch 51, Step 49, Loss: 0.002220842521637678\n","Epoch 51, Step 50, Loss: 0.002311396412551403\n","Epoch 51, Step 51, Loss: 0.0024244324304163456\n","Epoch 51, Step 52, Loss: 0.002419618656858802\n","Epoch 51, Step 53, Loss: 0.002319138264283538\n","Epoch 51, Step 54, Loss: 0.00225090398453176\n","Epoch 51, Step 55, Loss: 0.002263443311676383\n","Epoch 51, Step 56, Loss: 0.002391100162640214\n","Epoch 51, Step 57, Loss: 0.0026202069129794836\n","Epoch 51, Step 58, Loss: 0.0026418741326779127\n","Epoch 51, Step 59, Loss: 0.0025995683390647173\n","Epoch 51, Step 60, Loss: 0.0025400803424417973\n","Train Metric MRRs: 0.6654590760898447\n","Train Metric MAPs: 0.35951933944666586\n","Validation Metric MRRs: 0.6184827907775019\n","Validation Metric MAPs: 0.22001556782883866\n","Epoch 52, Step 1, Loss: 0.0024971473030745983\n","Epoch 52, Step 2, Loss: 0.0024937966372817755\n","Epoch 52, Step 3, Loss: 0.0027633749414235353\n","Epoch 52, Step 4, Loss: 0.0029609519988298416\n","Epoch 52, Step 5, Loss: 0.0028107729740440845\n","Epoch 52, Step 6, Loss: 0.0023929267190396786\n","Epoch 52, Step 7, Loss: 0.0023392592556774616\n","Epoch 52, Step 8, Loss: 0.0025430021341890097\n","Epoch 52, Step 9, Loss: 0.0028049196116626263\n","Epoch 52, Step 10, Loss: 0.0032126607839018106\n","Epoch 52, Step 11, Loss: 0.0029247894417494535\n","Epoch 52, Step 12, Loss: 0.0025471446570008993\n","Epoch 52, Step 13, Loss: 0.0023297241423279047\n","Epoch 52, Step 14, Loss: 0.002346448367461562\n","Epoch 52, Step 15, Loss: 0.0026185864116996527\n","Epoch 52, Step 16, Loss: 0.0030952347442507744\n","Epoch 52, Step 17, Loss: 0.0031483853235840797\n","Epoch 52, Step 18, Loss: 0.0030602903570979834\n","Epoch 52, Step 19, Loss: 0.0027022657450288534\n","Epoch 52, Step 20, Loss: 0.00260898657143116\n","Epoch 52, Step 21, Loss: 0.0028283370193094015\n","Epoch 52, Step 22, Loss: 0.003086060518398881\n","Epoch 52, Step 23, Loss: 0.0031154665630310774\n","Epoch 52, Step 24, Loss: 0.0028261265251785517\n","Epoch 52, Step 25, Loss: 0.002539019798859954\n","Epoch 52, Step 26, Loss: 0.002510934369638562\n","Epoch 52, Step 27, Loss: 0.002652278635650873\n","Epoch 52, Step 28, Loss: 0.00263043656013906\n","Epoch 52, Step 29, Loss: 0.0025694849900901318\n","Epoch 52, Step 30, Loss: 0.0024514412507414818\n","Epoch 52, Step 31, Loss: 0.00237598130479455\n","Epoch 52, Step 32, Loss: 0.0023332382552325726\n","Epoch 52, Step 33, Loss: 0.002423918340355158\n","Epoch 52, Step 34, Loss: 0.002498212270438671\n","Epoch 52, Step 35, Loss: 0.0023744991049170494\n","Epoch 52, Step 36, Loss: 0.0024173578713089228\n","Epoch 52, Step 37, Loss: 0.0023395970929414034\n","Epoch 52, Step 38, Loss: 0.0023106345906853676\n","Epoch 52, Step 39, Loss: 0.002288989955559373\n","Epoch 52, Step 40, Loss: 0.0023221697192639112\n","Epoch 52, Step 41, Loss: 0.0024234233424067497\n","Epoch 52, Step 42, Loss: 0.0024659268092364073\n","Epoch 52, Step 43, Loss: 0.002499907510355115\n","Epoch 52, Step 44, Loss: 0.0023026778362691402\n","Epoch 52, Step 45, Loss: 0.0023968424648046494\n","Epoch 52, Step 46, Loss: 0.0024178281892091036\n","Epoch 52, Step 47, Loss: 0.0024873027577996254\n","Epoch 52, Step 48, Loss: 0.0025275140069425106\n","Epoch 52, Step 49, Loss: 0.002433349611237645\n","Epoch 52, Step 50, Loss: 0.002342307474464178\n","Epoch 52, Step 51, Loss: 0.002294434467330575\n","Epoch 52, Step 52, Loss: 0.0023954431526362896\n","Epoch 52, Step 53, Loss: 0.0025354186072945595\n","Epoch 52, Step 54, Loss: 0.0026026859413832426\n","Epoch 52, Step 55, Loss: 0.002464275574311614\n","Epoch 52, Step 56, Loss: 0.0024177636951208115\n","Epoch 52, Step 57, Loss: 0.0024376779329031706\n","Epoch 52, Step 58, Loss: 0.002728488529101014\n","Epoch 52, Step 59, Loss: 0.003100445494055748\n","Epoch 52, Step 60, Loss: 0.0029047844000160694\n","Train Metric MRRs: 0.6616902227045363\n","Train Metric MAPs: 0.3648675767663831\n","Validation Metric MRRs: 0.6269252619608382\n","Validation Metric MAPs: 0.24320386926167017\n","Epoch 53, Step 1, Loss: 0.0027985763736069202\n","Epoch 53, Step 2, Loss: 0.0025430978275835514\n","Epoch 53, Step 3, Loss: 0.002535509876906872\n","Epoch 53, Step 4, Loss: 0.0029982198029756546\n","Epoch 53, Step 5, Loss: 0.0032722551841288805\n","Epoch 53, Step 6, Loss: 0.002925475127995014\n","Epoch 53, Step 7, Loss: 0.0025615862105041742\n","Epoch 53, Step 8, Loss: 0.0024548617657274008\n","Epoch 53, Step 9, Loss: 0.0025362500455230474\n","Epoch 53, Step 10, Loss: 0.0029865482356399298\n","Epoch 53, Step 11, Loss: 0.0032061978708952665\n","Epoch 53, Step 12, Loss: 0.003313034540042281\n","Epoch 53, Step 13, Loss: 0.0031490467954427004\n","Epoch 53, Step 14, Loss: 0.0026205964386463165\n","Epoch 53, Step 15, Loss: 0.002316187135875225\n","Epoch 53, Step 16, Loss: 0.0024737331550568342\n","Epoch 53, Step 17, Loss: 0.0030315446201711893\n","Epoch 53, Step 18, Loss: 0.003790421411395073\n","Epoch 53, Step 19, Loss: 0.0036152792163193226\n","Epoch 53, Step 20, Loss: 0.003128829412162304\n","Epoch 53, Step 21, Loss: 0.0027589439414441586\n","Epoch 53, Step 22, Loss: 0.002675860421732068\n","Epoch 53, Step 23, Loss: 0.002929362002760172\n","Epoch 53, Step 24, Loss: 0.003261104691773653\n","Epoch 53, Step 25, Loss: 0.003481793450191617\n","Epoch 53, Step 26, Loss: 0.0031776302494108677\n","Epoch 53, Step 27, Loss: 0.0026929080486297607\n","Epoch 53, Step 28, Loss: 0.002523303497582674\n","Epoch 53, Step 29, Loss: 0.00252724951133132\n","Epoch 53, Step 30, Loss: 0.0026123186107724905\n","Epoch 53, Step 31, Loss: 0.002726716920733452\n","Epoch 53, Step 32, Loss: 0.002754133427515626\n","Epoch 53, Step 33, Loss: 0.002759992377832532\n","Epoch 53, Step 34, Loss: 0.0026925434358417988\n","Epoch 53, Step 35, Loss: 0.0025008346419781446\n","Epoch 53, Step 36, Loss: 0.002487241756170988\n","Epoch 53, Step 37, Loss: 0.0026098168455064297\n","Epoch 53, Step 38, Loss: 0.0026141093112528324\n","Epoch 53, Step 39, Loss: 0.002609012881293893\n","Epoch 53, Step 40, Loss: 0.0025907710660248995\n","Epoch 53, Step 41, Loss: 0.0024334266781806946\n","Epoch 53, Step 42, Loss: 0.002415435155853629\n","Epoch 53, Step 43, Loss: 0.002537832362577319\n","Epoch 53, Step 44, Loss: 0.0026872651651501656\n","Epoch 53, Step 45, Loss: 0.0028451825492084026\n","Epoch 53, Step 46, Loss: 0.0026963960845023394\n","Epoch 53, Step 47, Loss: 0.002447804668918252\n","Epoch 53, Step 48, Loss: 0.002388788154348731\n","Epoch 53, Step 49, Loss: 0.0026172769721597433\n","Epoch 53, Step 50, Loss: 0.002716927556321025\n","Epoch 53, Step 51, Loss: 0.002690802561119199\n","Epoch 53, Step 52, Loss: 0.0027050150092691183\n","Epoch 53, Step 53, Loss: 0.0023561683483421803\n","Epoch 53, Step 54, Loss: 0.0023001111112535\n","Epoch 53, Step 55, Loss: 0.002471850486472249\n","Epoch 53, Step 56, Loss: 0.003077460452914238\n","Epoch 53, Step 57, Loss: 0.0027661616913974285\n","Epoch 53, Step 58, Loss: 0.002547535113990307\n","Epoch 53, Step 59, Loss: 0.002480838680639863\n","Epoch 53, Step 60, Loss: 0.002572081284597516\n","Train Metric MRRs: 0.649237194422393\n","Train Metric MAPs: 0.3533603393157634\n","Validation Metric MRRs: 0.6212725952840675\n","Validation Metric MAPs: 0.24529741392388157\n","Epoch 54, Step 1, Loss: 0.0026300372555851936\n","Epoch 54, Step 2, Loss: 0.0026993476785719395\n","Epoch 54, Step 3, Loss: 0.002745690057054162\n","Epoch 54, Step 4, Loss: 0.0026224942412227392\n","Epoch 54, Step 5, Loss: 0.002579596359282732\n","Epoch 54, Step 6, Loss: 0.0024712004233151674\n","Epoch 54, Step 7, Loss: 0.0027180807664990425\n","Epoch 54, Step 8, Loss: 0.0028260606341063976\n","Epoch 54, Step 9, Loss: 0.0028896217700093985\n","Epoch 54, Step 10, Loss: 0.002969481749460101\n","Epoch 54, Step 11, Loss: 0.0024975829292088747\n","Epoch 54, Step 12, Loss: 0.0026553648058325052\n","Epoch 54, Step 13, Loss: 0.0030760958325117826\n","Epoch 54, Step 14, Loss: 0.003260360797867179\n","Epoch 54, Step 15, Loss: 0.003212756710126996\n","Epoch 54, Step 16, Loss: 0.0029502571560442448\n","Epoch 54, Step 17, Loss: 0.0026450289878994226\n","Epoch 54, Step 18, Loss: 0.002957461401820183\n","Epoch 54, Step 19, Loss: 0.0031921800691634417\n","Epoch 54, Step 20, Loss: 0.003564057406038046\n","Epoch 54, Step 21, Loss: 0.0037751076743006706\n","Epoch 54, Step 22, Loss: 0.0032138326205313206\n","Epoch 54, Step 23, Loss: 0.002852759091183543\n","Epoch 54, Step 24, Loss: 0.002552983583882451\n","Epoch 54, Step 25, Loss: 0.0027366422582417727\n","Epoch 54, Step 26, Loss: 0.003054706146940589\n","Epoch 54, Step 27, Loss: 0.0031700616236776114\n","Epoch 54, Step 28, Loss: 0.0029931580647826195\n","Epoch 54, Step 29, Loss: 0.0028674849309027195\n","Epoch 54, Step 30, Loss: 0.0026715234853327274\n","Epoch 54, Step 31, Loss: 0.0025133618619292974\n","Epoch 54, Step 32, Loss: 0.00246331375092268\n","Epoch 54, Step 33, Loss: 0.002559836022555828\n","Epoch 54, Step 34, Loss: 0.002883294830098748\n","Epoch 54, Step 35, Loss: 0.0028772319201380014\n","Epoch 54, Step 36, Loss: 0.0026299578603357077\n","Epoch 54, Step 37, Loss: 0.0025135441683232784\n","Epoch 54, Step 38, Loss: 0.0024226754903793335\n","Epoch 54, Step 39, Loss: 0.002474091947078705\n","Epoch 54, Step 40, Loss: 0.002591419965028763\n","Epoch 54, Step 41, Loss: 0.0026958417147397995\n","Epoch 54, Step 42, Loss: 0.002720427932217717\n","Epoch 54, Step 43, Loss: 0.00266863196156919\n","Epoch 54, Step 44, Loss: 0.002442125231027603\n","Epoch 54, Step 45, Loss: 0.002377738244831562\n","Epoch 54, Step 46, Loss: 0.0023718744050711393\n","Epoch 54, Step 47, Loss: 0.0025322677101939917\n","Epoch 54, Step 48, Loss: 0.0024932140950113535\n","Epoch 54, Step 49, Loss: 0.002498909132555127\n","Epoch 54, Step 50, Loss: 0.002410657936707139\n","Epoch 54, Step 51, Loss: 0.0023017555940896273\n","Epoch 54, Step 52, Loss: 0.0023895842023193836\n","Epoch 54, Step 53, Loss: 0.002373564289882779\n","Epoch 54, Step 54, Loss: 0.002325063571333885\n","Epoch 54, Step 55, Loss: 0.0023458527866750956\n","Epoch 54, Step 56, Loss: 0.0023902750108391047\n","Epoch 54, Step 57, Loss: 0.002320030704140663\n","Epoch 54, Step 58, Loss: 0.00239262985996902\n","Epoch 54, Step 59, Loss: 0.0023643369786441326\n","Epoch 54, Step 60, Loss: 0.002476026304066181\n","Train Metric MRRs: 0.6496769377958664\n","Train Metric MAPs: 0.34276097378053894\n","Validation Metric MRRs: 0.6255206377895043\n","Validation Metric MAPs: 0.285845235513965\n","Epoch 55, Step 1, Loss: 0.0025099581107497215\n","Epoch 55, Step 2, Loss: 0.0024510419461876154\n","Epoch 55, Step 3, Loss: 0.0023708571679890156\n","Epoch 55, Step 4, Loss: 0.0024016364477574825\n","Epoch 55, Step 5, Loss: 0.002417326206341386\n","Epoch 55, Step 6, Loss: 0.0023186129983514547\n","Epoch 55, Step 7, Loss: 0.002307626884430647\n","Epoch 55, Step 8, Loss: 0.0023837785702198744\n","Epoch 55, Step 9, Loss: 0.002284043002873659\n","Epoch 55, Step 10, Loss: 0.002349763410165906\n","Epoch 55, Step 11, Loss: 0.002418320160359144\n","Epoch 55, Step 12, Loss: 0.002350635128095746\n","Epoch 55, Step 13, Loss: 0.0022846858482807875\n","Epoch 55, Step 14, Loss: 0.0022057760506868362\n","Epoch 55, Step 15, Loss: 0.00225778273306787\n","Epoch 55, Step 16, Loss: 0.0024097731802612543\n","Epoch 55, Step 17, Loss: 0.002465971512719989\n","Epoch 55, Step 18, Loss: 0.0028722486458718777\n","Epoch 55, Step 19, Loss: 0.0027071209624409676\n","Epoch 55, Step 20, Loss: 0.002641078317537904\n","Epoch 55, Step 21, Loss: 0.002687628846615553\n","Epoch 55, Step 22, Loss: 0.002761461539193988\n","Epoch 55, Step 23, Loss: 0.0028669252060353756\n","Epoch 55, Step 24, Loss: 0.00268542836420238\n","Epoch 55, Step 25, Loss: 0.0024900054559111595\n","Epoch 55, Step 26, Loss: 0.00248834234662354\n","Epoch 55, Step 27, Loss: 0.002486606827005744\n","Epoch 55, Step 28, Loss: 0.0024948432110249996\n","Epoch 55, Step 29, Loss: 0.0026367411483079195\n","Epoch 55, Step 30, Loss: 0.002622968517243862\n","Epoch 55, Step 31, Loss: 0.002530172932893038\n","Epoch 55, Step 32, Loss: 0.0023660222068428993\n","Epoch 55, Step 33, Loss: 0.002308417810127139\n","Epoch 55, Step 34, Loss: 0.0023565671872347593\n","Epoch 55, Step 35, Loss: 0.002429617801681161\n","Epoch 55, Step 36, Loss: 0.0024187874514609575\n","Epoch 55, Step 37, Loss: 0.0025115387979894876\n","Epoch 55, Step 38, Loss: 0.0023797827307134867\n","Epoch 55, Step 39, Loss: 0.002250486286357045\n","Epoch 55, Step 40, Loss: 0.0022363478783518076\n","Epoch 55, Step 41, Loss: 0.0022651972249150276\n","Epoch 55, Step 42, Loss: 0.002414405345916748\n","Epoch 55, Step 43, Loss: 0.0026065001729875803\n","Epoch 55, Step 44, Loss: 0.0024509953800588846\n","Epoch 55, Step 45, Loss: 0.0022870826069265604\n","Epoch 55, Step 46, Loss: 0.00219611800275743\n","Epoch 55, Step 47, Loss: 0.0022459635511040688\n","Epoch 55, Step 48, Loss: 0.002226867014542222\n","Epoch 55, Step 49, Loss: 0.0023389689158648252\n","Epoch 55, Step 50, Loss: 0.0024228408001363277\n","Epoch 55, Step 51, Loss: 0.00224770768545568\n","Epoch 55, Step 52, Loss: 0.0022444857750087976\n","Epoch 55, Step 53, Loss: 0.0021580562461167574\n","Epoch 55, Step 54, Loss: 0.002144266851246357\n","Epoch 55, Step 55, Loss: 0.0022487530950456858\n","Epoch 55, Step 56, Loss: 0.0023430627770721912\n","Epoch 55, Step 57, Loss: 0.0022958458866924047\n","Epoch 55, Step 58, Loss: 0.002328487578779459\n","Epoch 55, Step 59, Loss: 0.002279346575960517\n","Epoch 55, Step 60, Loss: 0.002371704438701272\n","Train Metric MRRs: 0.6655370176065445\n","Train Metric MAPs: 0.34794148758778704\n","Validation Metric MRRs: 0.6458333457357871\n","Validation Metric MAPs: 0.2899103890182484\n","Epoch 56, Step 1, Loss: 0.002321770414710045\n","Epoch 56, Step 2, Loss: 0.0023956538643687963\n","Epoch 56, Step 3, Loss: 0.002395270625129342\n","Epoch 56, Step 4, Loss: 0.0024369556922465563\n","Epoch 56, Step 5, Loss: 0.002397587988525629\n","Epoch 56, Step 6, Loss: 0.0022564218379557133\n","Epoch 56, Step 7, Loss: 0.0022087774705141783\n","Epoch 56, Step 8, Loss: 0.0022624307312071323\n","Epoch 56, Step 9, Loss: 0.002251612488180399\n","Epoch 56, Step 10, Loss: 0.0023611600045114756\n","Epoch 56, Step 11, Loss: 0.0022847254294902086\n","Epoch 56, Step 12, Loss: 0.002185687655583024\n","Epoch 56, Step 13, Loss: 0.002198002999648452\n","Epoch 56, Step 14, Loss: 0.002112871501594782\n","Epoch 56, Step 15, Loss: 0.00213811919093132\n","Epoch 56, Step 16, Loss: 0.002234804444015026\n","Epoch 56, Step 17, Loss: 0.0022559838835150003\n","Epoch 56, Step 18, Loss: 0.0026163842994719744\n","Epoch 56, Step 19, Loss: 0.002553879050537944\n","Epoch 56, Step 20, Loss: 0.002508259378373623\n","Epoch 56, Step 21, Loss: 0.0024624012876302004\n","Epoch 56, Step 22, Loss: 0.0023428190033882856\n","Epoch 56, Step 23, Loss: 0.002361638704314828\n","Epoch 56, Step 24, Loss: 0.002353586256504059\n","Epoch 56, Step 25, Loss: 0.0023141757119446993\n","Epoch 56, Step 26, Loss: 0.0023032198660075665\n","Epoch 56, Step 27, Loss: 0.0022355695255100727\n","Epoch 56, Step 28, Loss: 0.0021751064341515303\n","Epoch 56, Step 29, Loss: 0.0022334009408950806\n","Epoch 56, Step 30, Loss: 0.002295113867148757\n","Epoch 56, Step 31, Loss: 0.0023246053606271744\n","Epoch 56, Step 32, Loss: 0.002234832150861621\n","Epoch 56, Step 33, Loss: 0.0022162143141031265\n","Epoch 56, Step 34, Loss: 0.002176071284338832\n","Epoch 56, Step 35, Loss: 0.0021570706740021706\n","Epoch 56, Step 36, Loss: 0.0021575649734586477\n","Epoch 56, Step 37, Loss: 0.002259062835946679\n","Epoch 56, Step 38, Loss: 0.002316768979653716\n","Epoch 56, Step 39, Loss: 0.002225454431027174\n","Epoch 56, Step 40, Loss: 0.002186090685427189\n","Epoch 56, Step 41, Loss: 0.002105928026139736\n","Epoch 56, Step 42, Loss: 0.0021792668849229813\n","Epoch 56, Step 43, Loss: 0.002308422001078725\n","Epoch 56, Step 44, Loss: 0.0022814807016402483\n","Epoch 56, Step 45, Loss: 0.0021954020485281944\n","Epoch 56, Step 46, Loss: 0.002155836671590805\n","Epoch 56, Step 47, Loss: 0.002102054888382554\n","Epoch 56, Step 48, Loss: 0.0020571169443428516\n","Epoch 56, Step 49, Loss: 0.002129563130438328\n","Epoch 56, Step 50, Loss: 0.0022707500029355288\n","Epoch 56, Step 51, Loss: 0.002171025611460209\n","Epoch 56, Step 52, Loss: 0.002215077867731452\n","Epoch 56, Step 53, Loss: 0.0020874403417110443\n","Epoch 56, Step 54, Loss: 0.0020328655373305082\n","Epoch 56, Step 55, Loss: 0.002075230935588479\n","Epoch 56, Step 56, Loss: 0.002195431850850582\n","Epoch 56, Step 57, Loss: 0.002191153122112155\n","Epoch 56, Step 58, Loss: 0.0022614849731326103\n","Epoch 56, Step 59, Loss: 0.0022047769743949175\n","Epoch 56, Step 60, Loss: 0.002298958832398057\n","Train Metric MRRs: 0.675693196684586\n","Train Metric MAPs: 0.35680214328450327\n","Validation Metric MRRs: 0.6523482377157868\n","Validation Metric MAPs: 0.28827033080454584\n","Epoch 57, Step 1, Loss: 0.002242695540189743\n","Epoch 57, Step 2, Loss: 0.0022196276113390923\n","Epoch 57, Step 3, Loss: 0.0022541102953255177\n","Epoch 57, Step 4, Loss: 0.002346990630030632\n","Epoch 57, Step 5, Loss: 0.0023901916574686766\n","Epoch 57, Step 6, Loss: 0.0022480871994048357\n","Epoch 57, Step 7, Loss: 0.0021441448479890823\n","Epoch 57, Step 8, Loss: 0.0021513125393539667\n","Epoch 57, Step 9, Loss: 0.002184083452448249\n","Epoch 57, Step 10, Loss: 0.002269989810883999\n","Epoch 57, Step 11, Loss: 0.002218825276941061\n","Epoch 57, Step 12, Loss: 0.002183679025620222\n","Epoch 57, Step 13, Loss: 0.0021801027469336987\n","Epoch 57, Step 14, Loss: 0.0020737401209771633\n","Epoch 57, Step 15, Loss: 0.002055374439805746\n","Epoch 57, Step 16, Loss: 0.00216413545422256\n","Epoch 57, Step 17, Loss: 0.0022277594543993473\n","Epoch 57, Step 18, Loss: 0.0025840927846729755\n","Epoch 57, Step 19, Loss: 0.002529288874939084\n","Epoch 57, Step 20, Loss: 0.002467652317136526\n","Epoch 57, Step 21, Loss: 0.0024286783300340176\n","Epoch 57, Step 22, Loss: 0.0023058424703776836\n","Epoch 57, Step 23, Loss: 0.002311167772859335\n","Epoch 57, Step 24, Loss: 0.00228075310587883\n","Epoch 57, Step 25, Loss: 0.002254648134112358\n","Epoch 57, Step 26, Loss: 0.0022732035722583532\n","Epoch 57, Step 27, Loss: 0.0021939114667475224\n","Epoch 57, Step 28, Loss: 0.002090322319418192\n","Epoch 57, Step 29, Loss: 0.0021282117813825607\n","Epoch 57, Step 30, Loss: 0.0021992039401084185\n","Epoch 57, Step 31, Loss: 0.0022055290173739195\n","Epoch 57, Step 32, Loss: 0.00213028397411108\n","Epoch 57, Step 33, Loss: 0.002131992718204856\n","Epoch 57, Step 34, Loss: 0.00213593035005033\n","Epoch 57, Step 35, Loss: 0.002081242622807622\n","Epoch 57, Step 36, Loss: 0.002086044056341052\n","Epoch 57, Step 37, Loss: 0.00213792035356164\n","Epoch 57, Step 38, Loss: 0.002171560190618038\n","Epoch 57, Step 39, Loss: 0.002162494696676731\n","Epoch 57, Step 40, Loss: 0.0021495732944458723\n","Epoch 57, Step 41, Loss: 0.0020567269530147314\n","Epoch 57, Step 42, Loss: 0.0020822188816964626\n","Epoch 57, Step 43, Loss: 0.0022167169954627752\n","Epoch 57, Step 44, Loss: 0.0021468752529472113\n","Epoch 57, Step 45, Loss: 0.002107770647853613\n","Epoch 57, Step 46, Loss: 0.002114200731739402\n","Epoch 57, Step 47, Loss: 0.002081106184050441\n","Epoch 57, Step 48, Loss: 0.0020220992155373096\n","Epoch 57, Step 49, Loss: 0.00202959799207747\n","Epoch 57, Step 50, Loss: 0.0021691927686333656\n","Epoch 57, Step 51, Loss: 0.0021253922022879124\n","Epoch 57, Step 52, Loss: 0.0021920057479292154\n","Epoch 57, Step 53, Loss: 0.002060278318822384\n","Epoch 57, Step 54, Loss: 0.0020040490198880434\n","Epoch 57, Step 55, Loss: 0.002006576396524906\n","Epoch 57, Step 56, Loss: 0.002129186410456896\n","Epoch 57, Step 57, Loss: 0.0021579719614237547\n","Epoch 57, Step 58, Loss: 0.002204030519351363\n","Epoch 57, Step 59, Loss: 0.0021683669183403254\n","Epoch 57, Step 60, Loss: 0.002258164808154106\n","Train Metric MRRs: 0.681454087170893\n","Train Metric MAPs: 0.3587865633554325\n","Validation Metric MRRs: 0.6558146657528471\n","Validation Metric MAPs: 0.2801754169873407\n","Epoch 58, Step 1, Loss: 0.002238697838038206\n","Epoch 58, Step 2, Loss: 0.0021827425807714462\n","Epoch 58, Step 3, Loss: 0.0021763716358691454\n","Epoch 58, Step 4, Loss: 0.002263109665364027\n","Epoch 58, Step 5, Loss: 0.0023353202268481255\n","Epoch 58, Step 6, Loss: 0.0022560949437320232\n","Epoch 58, Step 7, Loss: 0.002118405420333147\n","Epoch 58, Step 8, Loss: 0.0021047545596957207\n","Epoch 58, Step 9, Loss: 0.002121547469869256\n","Epoch 58, Step 10, Loss: 0.002237171633169055\n","Epoch 58, Step 11, Loss: 0.0021931747905910015\n","Epoch 58, Step 12, Loss: 0.002204142976552248\n","Epoch 58, Step 13, Loss: 0.002181882271543145\n","Epoch 58, Step 14, Loss: 0.0020609917119145393\n","Epoch 58, Step 15, Loss: 0.0020302284974604845\n","Epoch 58, Step 16, Loss: 0.0021153660491108894\n","Epoch 58, Step 17, Loss: 0.002211954677477479\n","Epoch 58, Step 18, Loss: 0.0025687129236757755\n","Epoch 58, Step 19, Loss: 0.002537790220230818\n","Epoch 58, Step 20, Loss: 0.0024796640500426292\n","Epoch 58, Step 21, Loss: 0.0023683293256908655\n","Epoch 58, Step 22, Loss: 0.0022615958005189896\n","Epoch 58, Step 23, Loss: 0.0022652405314147472\n","Epoch 58, Step 24, Loss: 0.0022689788602292538\n","Epoch 58, Step 25, Loss: 0.0022055094595998526\n","Epoch 58, Step 26, Loss: 0.002201751107349992\n","Epoch 58, Step 27, Loss: 0.0021580797620117664\n","Epoch 58, Step 28, Loss: 0.0020794805604964495\n","Epoch 58, Step 29, Loss: 0.002109068213030696\n","Epoch 58, Step 30, Loss: 0.0021496338304132223\n","Epoch 58, Step 31, Loss: 0.002159942639991641\n","Epoch 58, Step 32, Loss: 0.0020760134793817997\n","Epoch 58, Step 33, Loss: 0.002080947859212756\n","Epoch 58, Step 34, Loss: 0.002090540947392583\n","Epoch 58, Step 35, Loss: 0.002029979368671775\n","Epoch 58, Step 36, Loss: 0.0020460416562855244\n","Epoch 58, Step 37, Loss: 0.0020968206226825714\n","Epoch 58, Step 38, Loss: 0.0021206773817539215\n","Epoch 58, Step 39, Loss: 0.002096435520797968\n","Epoch 58, Step 40, Loss: 0.002071116818115115\n","Epoch 58, Step 41, Loss: 0.0020305525977164507\n","Epoch 58, Step 42, Loss: 0.0020602247677743435\n","Epoch 58, Step 43, Loss: 0.002124115824699402\n","Epoch 58, Step 44, Loss: 0.002090881345793605\n","Epoch 58, Step 45, Loss: 0.0020782803185284138\n","Epoch 58, Step 46, Loss: 0.0021140449680387974\n","Epoch 58, Step 47, Loss: 0.002083666855469346\n","Epoch 58, Step 48, Loss: 0.0020026371348649263\n","Epoch 58, Step 49, Loss: 0.0020084576681256294\n","Epoch 58, Step 50, Loss: 0.0021434312220662832\n","Epoch 58, Step 51, Loss: 0.002104350598528981\n","Epoch 58, Step 52, Loss: 0.0021840871777385473\n","Epoch 58, Step 53, Loss: 0.002074374118819833\n","Epoch 58, Step 54, Loss: 0.001995587721467018\n","Epoch 58, Step 55, Loss: 0.001989801647141576\n","Epoch 58, Step 56, Loss: 0.0021009938791394234\n","Epoch 58, Step 57, Loss: 0.0021579198073595762\n","Epoch 58, Step 58, Loss: 0.002212243154644966\n","Epoch 58, Step 59, Loss: 0.0022011559922248125\n","Epoch 58, Step 60, Loss: 0.0022787803318351507\n","Train Metric MRRs: 0.6859482283167718\n","Train Metric MAPs: 0.3609951933704123\n","Validation Metric MRRs: 0.6609277709462468\n","Validation Metric MAPs: 0.27721552700899144\n","Epoch 59, Step 1, Loss: 0.0021913377568125725\n","Epoch 59, Step 2, Loss: 0.002149259904399514\n","Epoch 59, Step 3, Loss: 0.0021479972638189793\n","Epoch 59, Step 4, Loss: 0.0023023830726742744\n","Epoch 59, Step 5, Loss: 0.002342969411984086\n","Epoch 59, Step 6, Loss: 0.002222389215603471\n","Epoch 59, Step 7, Loss: 0.002110558794811368\n","Epoch 59, Step 8, Loss: 0.0021029063500463963\n","Epoch 59, Step 9, Loss: 0.00208122911863029\n","Epoch 59, Step 10, Loss: 0.002203645184636116\n","Epoch 59, Step 11, Loss: 0.002164144767448306\n","Epoch 59, Step 12, Loss: 0.0022760373540222645\n","Epoch 59, Step 13, Loss: 0.0021595973521471024\n","Epoch 59, Step 14, Loss: 0.002067493973299861\n","Epoch 59, Step 15, Loss: 0.00203366344794631\n","Epoch 59, Step 16, Loss: 0.0021308038849383593\n","Epoch 59, Step 17, Loss: 0.0021793118212372065\n","Epoch 59, Step 18, Loss: 0.0025651913601905107\n","Epoch 59, Step 19, Loss: 0.0025361559819430113\n","Epoch 59, Step 20, Loss: 0.0024634329602122307\n","Epoch 59, Step 21, Loss: 0.002414675196632743\n","Epoch 59, Step 22, Loss: 0.0022882064804434776\n","Epoch 59, Step 23, Loss: 0.0022640007082372904\n","Epoch 59, Step 24, Loss: 0.0022457612212747335\n","Epoch 59, Step 25, Loss: 0.0022178045473992825\n","Epoch 59, Step 26, Loss: 0.0022079634945839643\n","Epoch 59, Step 27, Loss: 0.002150908811017871\n","Epoch 59, Step 28, Loss: 0.002072042552754283\n","Epoch 59, Step 29, Loss: 0.0021225621458142996\n","Epoch 59, Step 30, Loss: 0.0021264120005071163\n","Epoch 59, Step 31, Loss: 0.0021419296972453594\n","Epoch 59, Step 32, Loss: 0.0020637521520256996\n","Epoch 59, Step 33, Loss: 0.0020884412806481123\n","Epoch 59, Step 34, Loss: 0.0020899008959531784\n","Epoch 59, Step 35, Loss: 0.002038694452494383\n","Epoch 59, Step 36, Loss: 0.0020587770268321037\n","Epoch 59, Step 37, Loss: 0.0020634126849472523\n","Epoch 59, Step 38, Loss: 0.0020866021513938904\n","Epoch 59, Step 39, Loss: 0.002082723192870617\n","Epoch 59, Step 40, Loss: 0.0020601176656782627\n","Epoch 59, Step 41, Loss: 0.0020100015681236982\n","Epoch 59, Step 42, Loss: 0.0020559183321893215\n","Epoch 59, Step 43, Loss: 0.0021054870449006557\n","Epoch 59, Step 44, Loss: 0.0020541308913379908\n","Epoch 59, Step 45, Loss: 0.002032042946666479\n","Epoch 59, Step 46, Loss: 0.002058375393971801\n","Epoch 59, Step 47, Loss: 0.0020375214517116547\n","Epoch 59, Step 48, Loss: 0.001996963517740369\n","Epoch 59, Step 49, Loss: 0.0019924731459468603\n","Epoch 59, Step 50, Loss: 0.0020515795331448317\n","Epoch 59, Step 51, Loss: 0.002039929386228323\n","Epoch 59, Step 52, Loss: 0.0021157176233828068\n","Epoch 59, Step 53, Loss: 0.0020259316079318523\n","Epoch 59, Step 54, Loss: 0.0020083726849406958\n","Epoch 59, Step 55, Loss: 0.0020019286312162876\n","Epoch 59, Step 56, Loss: 0.0020848691929131746\n","Epoch 59, Step 57, Loss: 0.0021086407359689474\n","Epoch 59, Step 58, Loss: 0.0021953359246253967\n","Epoch 59, Step 59, Loss: 0.002252117497846484\n","Epoch 59, Step 60, Loss: 0.0023136227391660213\n","Train Metric MRRs: 0.6868783552590835\n","Train Metric MAPs: 0.36135976671310244\n","Validation Metric MRRs: 0.6604864514931791\n","Validation Metric MAPs: 0.26609532730607344\n","Epoch 60, Step 1, Loss: 0.002218365902081132\n","Epoch 60, Step 2, Loss: 0.0021311636082828045\n","Epoch 60, Step 3, Loss: 0.002117736963555217\n","Epoch 60, Step 4, Loss: 0.0022891387343406677\n","Epoch 60, Step 5, Loss: 0.0023856880143284798\n","Epoch 60, Step 6, Loss: 0.002280219690874219\n","Epoch 60, Step 7, Loss: 0.0021358926314860582\n","Epoch 60, Step 8, Loss: 0.0020752265118062496\n","Epoch 60, Step 9, Loss: 0.002067667432129383\n","Epoch 60, Step 10, Loss: 0.0022146066185086966\n","Epoch 60, Step 11, Loss: 0.0021897556725889444\n","Epoch 60, Step 12, Loss: 0.0022830520756542683\n","Epoch 60, Step 13, Loss: 0.002176351146772504\n","Epoch 60, Step 14, Loss: 0.0020848941057920456\n","Epoch 60, Step 15, Loss: 0.0020371251739561558\n","Epoch 60, Step 16, Loss: 0.0020984590519219637\n","Epoch 60, Step 17, Loss: 0.002186757978051901\n","Epoch 60, Step 18, Loss: 0.002540274988859892\n","Epoch 60, Step 19, Loss: 0.002508714096620679\n","Epoch 60, Step 20, Loss: 0.0024702383670955896\n","Epoch 60, Step 21, Loss: 0.0024242117069661617\n","Epoch 60, Step 22, Loss: 0.0022828744258731604\n","Epoch 60, Step 23, Loss: 0.0022726859897375107\n","Epoch 60, Step 24, Loss: 0.0022927785757929087\n","Epoch 60, Step 25, Loss: 0.0022482636850327253\n","Epoch 60, Step 26, Loss: 0.0022064950317144394\n","Epoch 60, Step 27, Loss: 0.0021462072618305683\n","Epoch 60, Step 28, Loss: 0.0020752784330397844\n","Epoch 60, Step 29, Loss: 0.0021197237074375153\n","Epoch 60, Step 30, Loss: 0.0021115124691277742\n","Epoch 60, Step 31, Loss: 0.002118634060025215\n","Epoch 60, Step 32, Loss: 0.0020557295065373182\n","Epoch 60, Step 33, Loss: 0.0020671142265200615\n","Epoch 60, Step 34, Loss: 0.002087643602862954\n","Epoch 60, Step 35, Loss: 0.002033885335549712\n","Epoch 60, Step 36, Loss: 0.002038980834186077\n","Epoch 60, Step 37, Loss: 0.0020709082018584013\n","Epoch 60, Step 38, Loss: 0.0020807981491088867\n","Epoch 60, Step 39, Loss: 0.002049484523013234\n","Epoch 60, Step 40, Loss: 0.002030705101788044\n","Epoch 60, Step 41, Loss: 0.0019748504273593426\n","Epoch 60, Step 42, Loss: 0.002031817100942135\n","Epoch 60, Step 43, Loss: 0.002086406573653221\n","Epoch 60, Step 44, Loss: 0.0020187310874462128\n","Epoch 60, Step 45, Loss: 0.0019983595702797174\n","Epoch 60, Step 46, Loss: 0.002027746057137847\n","Epoch 60, Step 47, Loss: 0.0020245607011020184\n","Epoch 60, Step 48, Loss: 0.0019537699408829212\n","Epoch 60, Step 49, Loss: 0.0019879071041941643\n","Epoch 60, Step 50, Loss: 0.002059885999187827\n","Epoch 60, Step 51, Loss: 0.001998016145080328\n","Epoch 60, Step 52, Loss: 0.0020836505573242903\n","Epoch 60, Step 53, Loss: 0.0020249050576239824\n","Epoch 60, Step 54, Loss: 0.0019729379564523697\n","Epoch 60, Step 55, Loss: 0.0019772150553762913\n","Epoch 60, Step 56, Loss: 0.0020541928242892027\n","Epoch 60, Step 57, Loss: 0.0020783564541488886\n","Epoch 60, Step 58, Loss: 0.0021037382539361715\n","Epoch 60, Step 59, Loss: 0.0021385764703154564\n","Epoch 60, Step 60, Loss: 0.002291664946824312\n","Train Metric MRRs: 0.688847226638071\n","Train Metric MAPs: 0.36010711581078103\n","Validation Metric MRRs: 0.6647776437814595\n","Validation Metric MAPs: 0.2730096586377232\n","Epoch 61, Step 1, Loss: 0.002191567327827215\n","Epoch 61, Step 2, Loss: 0.0021526638884097338\n","Epoch 61, Step 3, Loss: 0.0020901616662740707\n","Epoch 61, Step 4, Loss: 0.002223221119493246\n","Epoch 61, Step 5, Loss: 0.002304843859747052\n","Epoch 61, Step 6, Loss: 0.0022718373220413923\n","Epoch 61, Step 7, Loss: 0.0021495181135833263\n","Epoch 61, Step 8, Loss: 0.0020920701790601015\n","Epoch 61, Step 9, Loss: 0.002048248890787363\n","Epoch 61, Step 10, Loss: 0.002137567847967148\n","Epoch 61, Step 11, Loss: 0.002140497090294957\n","Epoch 61, Step 12, Loss: 0.0022078172769397497\n","Epoch 61, Step 13, Loss: 0.002224333817139268\n","Epoch 61, Step 14, Loss: 0.0021317286882549524\n","Epoch 61, Step 15, Loss: 0.0020764905493706465\n","Epoch 61, Step 16, Loss: 0.002091720700263977\n","Epoch 61, Step 17, Loss: 0.0021579477470368147\n","Epoch 61, Step 18, Loss: 0.002553377766162157\n","Epoch 61, Step 19, Loss: 0.00254541146568954\n","Epoch 61, Step 20, Loss: 0.0024908098857849836\n","Epoch 61, Step 21, Loss: 0.002431727247312665\n","Epoch 61, Step 22, Loss: 0.0022601892706006765\n","Epoch 61, Step 23, Loss: 0.0022487782407552004\n","Epoch 61, Step 24, Loss: 0.002248803386464715\n","Epoch 61, Step 25, Loss: 0.0022205174900591373\n","Epoch 61, Step 26, Loss: 0.002210191683843732\n","Epoch 61, Step 27, Loss: 0.0021444852463901043\n","Epoch 61, Step 28, Loss: 0.0020663938485085964\n","Epoch 61, Step 29, Loss: 0.002119721146300435\n","Epoch 61, Step 30, Loss: 0.0021323799155652523\n","Epoch 61, Step 31, Loss: 0.002107044216245413\n","Epoch 61, Step 32, Loss: 0.0020668627694249153\n","Epoch 61, Step 33, Loss: 0.002049337374046445\n","Epoch 61, Step 34, Loss: 0.0020832812879234552\n","Epoch 61, Step 35, Loss: 0.0020416812039911747\n","Epoch 61, Step 36, Loss: 0.0020157115068286657\n","Epoch 61, Step 37, Loss: 0.0020533783826977015\n","Epoch 61, Step 38, Loss: 0.0020670746453106403\n","Epoch 61, Step 39, Loss: 0.0020074141211807728\n","Epoch 61, Step 40, Loss: 0.0019831836689263582\n","Epoch 61, Step 41, Loss: 0.001966272247955203\n","Epoch 61, Step 42, Loss: 0.002028196817263961\n","Epoch 61, Step 43, Loss: 0.002083243103697896\n","Epoch 61, Step 44, Loss: 0.0020056148059666157\n","Epoch 61, Step 45, Loss: 0.0019930340349674225\n","Epoch 61, Step 46, Loss: 0.0020209543872624636\n","Epoch 61, Step 47, Loss: 0.0020012978930026293\n","Epoch 61, Step 48, Loss: 0.0019259571563452482\n","Epoch 61, Step 49, Loss: 0.0019640829414129257\n","Epoch 61, Step 50, Loss: 0.0020359731279313564\n","Epoch 61, Step 51, Loss: 0.0019680156838148832\n","Epoch 61, Step 52, Loss: 0.0020694683771580458\n","Epoch 61, Step 53, Loss: 0.0019855049904435873\n","Epoch 61, Step 54, Loss: 0.0019363175379112363\n","Epoch 61, Step 55, Loss: 0.0019641756080091\n","Epoch 61, Step 56, Loss: 0.0020615144167095423\n","Epoch 61, Step 57, Loss: 0.0020697263535112143\n","Epoch 61, Step 58, Loss: 0.0020659652072936296\n","Epoch 61, Step 59, Loss: 0.0021008108742535114\n","Epoch 61, Step 60, Loss: 0.0022600090596824884\n","Train Metric MRRs: 0.6903367704845442\n","Train Metric MAPs: 0.3583659343273714\n","Validation Metric MRRs: 0.6661022309686716\n","Validation Metric MAPs: 0.279959872819522\n","Epoch 62, Step 1, Loss: 0.0021470417268574238\n","Epoch 62, Step 2, Loss: 0.0021476789843291044\n","Epoch 62, Step 3, Loss: 0.0020627642516046762\n","Epoch 62, Step 4, Loss: 0.002141504315659404\n","Epoch 62, Step 5, Loss: 0.0022641075775027275\n","Epoch 62, Step 6, Loss: 0.0022329192142933607\n","Epoch 62, Step 7, Loss: 0.0021710617002099752\n","Epoch 62, Step 8, Loss: 0.002102071885019541\n","Epoch 62, Step 9, Loss: 0.0020210216753184795\n","Epoch 62, Step 10, Loss: 0.0021229994017630816\n","Epoch 62, Step 11, Loss: 0.0020912985783070326\n","Epoch 62, Step 12, Loss: 0.0021681725047528744\n","Epoch 62, Step 13, Loss: 0.0021634618751704693\n","Epoch 62, Step 14, Loss: 0.0021179926116019487\n","Epoch 62, Step 15, Loss: 0.0021002660505473614\n","Epoch 62, Step 16, Loss: 0.0020990746561437845\n","Epoch 62, Step 17, Loss: 0.0021517593413591385\n","Epoch 62, Step 18, Loss: 0.0025174212642014027\n","Epoch 62, Step 19, Loss: 0.00249007111415267\n","Epoch 62, Step 20, Loss: 0.0024937812704592943\n","Epoch 62, Step 21, Loss: 0.0024894243106245995\n","Epoch 62, Step 22, Loss: 0.0022915422450751066\n","Epoch 62, Step 23, Loss: 0.0022414468694478273\n","Epoch 62, Step 24, Loss: 0.0022139698266983032\n","Epoch 62, Step 25, Loss: 0.002203530864790082\n","Epoch 62, Step 26, Loss: 0.0022242667619138956\n","Epoch 62, Step 27, Loss: 0.0021356293000280857\n","Epoch 62, Step 28, Loss: 0.0020553788635879755\n","Epoch 62, Step 29, Loss: 0.0021140703465789557\n","Epoch 62, Step 30, Loss: 0.0021010483615100384\n","Epoch 62, Step 31, Loss: 0.002088178414851427\n","Epoch 62, Step 32, Loss: 0.002048926893621683\n","Epoch 62, Step 33, Loss: 0.002015302889049053\n","Epoch 62, Step 34, Loss: 0.002053620060905814\n","Epoch 62, Step 35, Loss: 0.0019999139476567507\n","Epoch 62, Step 36, Loss: 0.0020185639150440693\n","Epoch 62, Step 37, Loss: 0.0020415731705725193\n","Epoch 62, Step 38, Loss: 0.002064038999378681\n","Epoch 62, Step 39, Loss: 0.0020043933764100075\n","Epoch 62, Step 40, Loss: 0.0019626040011644363\n","Epoch 62, Step 41, Loss: 0.0019396839197725058\n","Epoch 62, Step 42, Loss: 0.0019968163687735796\n","Epoch 62, Step 43, Loss: 0.002045007422566414\n","Epoch 62, Step 44, Loss: 0.001979457912966609\n","Epoch 62, Step 45, Loss: 0.001966143259778619\n","Epoch 62, Step 46, Loss: 0.00199806853197515\n","Epoch 62, Step 47, Loss: 0.0020069472957402468\n","Epoch 62, Step 48, Loss: 0.0019370234804227948\n","Epoch 62, Step 49, Loss: 0.0019518994959071279\n","Epoch 62, Step 50, Loss: 0.0020059016533195972\n","Epoch 62, Step 51, Loss: 0.0019419469172134995\n","Epoch 62, Step 52, Loss: 0.0020501127000898123\n","Epoch 62, Step 53, Loss: 0.0019755871035158634\n","Epoch 62, Step 54, Loss: 0.001943264389410615\n","Epoch 62, Step 55, Loss: 0.0019489802652969956\n","Epoch 62, Step 56, Loss: 0.0020129005424678326\n","Epoch 62, Step 57, Loss: 0.0020368825644254684\n","Epoch 62, Step 58, Loss: 0.002031757729128003\n","Epoch 62, Step 59, Loss: 0.0020694632548838854\n","Epoch 62, Step 60, Loss: 0.002219746820628643\n","Train Metric MRRs: 0.6918868237389126\n","Train Metric MAPs: 0.3590477192282966\n","Validation Metric MRRs: 0.6657287434230815\n","Validation Metric MAPs: 0.2854894227637345\n","Epoch 63, Step 1, Loss: 0.002102013910189271\n","Epoch 63, Step 2, Loss: 0.0021191807463765144\n","Epoch 63, Step 3, Loss: 0.002045466098934412\n","Epoch 63, Step 4, Loss: 0.002117054769769311\n","Epoch 63, Step 5, Loss: 0.0022014472633600235\n","Epoch 63, Step 6, Loss: 0.0021917696576565504\n","Epoch 63, Step 7, Loss: 0.0021411182824522257\n","Epoch 63, Step 8, Loss: 0.002095435746014118\n","Epoch 63, Step 9, Loss: 0.0020041451789438725\n","Epoch 63, Step 10, Loss: 0.002082021674141288\n","Epoch 63, Step 11, Loss: 0.0020307276863604784\n","Epoch 63, Step 12, Loss: 0.0020748537499457598\n","Epoch 63, Step 13, Loss: 0.002136682393029332\n","Epoch 63, Step 14, Loss: 0.002110452391207218\n","Epoch 63, Step 15, Loss: 0.002112387679517269\n","Epoch 63, Step 16, Loss: 0.002107198815792799\n","Epoch 63, Step 17, Loss: 0.0021374663338065147\n","Epoch 63, Step 18, Loss: 0.0024991265963763\n","Epoch 63, Step 19, Loss: 0.0024650106206536293\n","Epoch 63, Step 20, Loss: 0.0024777960497885942\n","Epoch 63, Step 21, Loss: 0.002484574681147933\n","Epoch 63, Step 22, Loss: 0.0022947790566831827\n","Epoch 63, Step 23, Loss: 0.0022575499024242163\n","Epoch 63, Step 24, Loss: 0.0022121567744761705\n","Epoch 63, Step 25, Loss: 0.0021709343418478966\n","Epoch 63, Step 26, Loss: 0.0021871542558073997\n","Epoch 63, Step 27, Loss: 0.002126446459442377\n","Epoch 63, Step 28, Loss: 0.0020420211367309093\n","Epoch 63, Step 29, Loss: 0.0021148796658962965\n","Epoch 63, Step 30, Loss: 0.0021060442086309195\n","Epoch 63, Step 31, Loss: 0.0020705503411591053\n","Epoch 63, Step 32, Loss: 0.002034116303548217\n","Epoch 63, Step 33, Loss: 0.0020145720336586237\n","Epoch 63, Step 34, Loss: 0.0020437543280422688\n","Epoch 63, Step 35, Loss: 0.0019913536962121725\n","Epoch 63, Step 36, Loss: 0.001998558407649398\n","Epoch 63, Step 37, Loss: 0.002034790115430951\n","Epoch 63, Step 38, Loss: 0.002043516142293811\n","Epoch 63, Step 39, Loss: 0.0019758478738367558\n","Epoch 63, Step 40, Loss: 0.0019559324719011784\n","Epoch 63, Step 41, Loss: 0.0019239246612414718\n","Epoch 63, Step 42, Loss: 0.0019917048048228025\n","Epoch 63, Step 43, Loss: 0.0020469094160944223\n","Epoch 63, Step 44, Loss: 0.001982626738026738\n","Epoch 63, Step 45, Loss: 0.0019484865479171276\n","Epoch 63, Step 46, Loss: 0.001968086464330554\n","Epoch 63, Step 47, Loss: 0.0019746222533285618\n","Epoch 63, Step 48, Loss: 0.0019286269089207053\n","Epoch 63, Step 49, Loss: 0.0019435907015576959\n","Epoch 63, Step 50, Loss: 0.0020096199586987495\n","Epoch 63, Step 51, Loss: 0.0019306569593027234\n","Epoch 63, Step 52, Loss: 0.0020276799332350492\n","Epoch 63, Step 53, Loss: 0.001989919925108552\n","Epoch 63, Step 54, Loss: 0.001921332092024386\n","Epoch 63, Step 55, Loss: 0.0019532563164830208\n","Epoch 63, Step 56, Loss: 0.0019925625529140234\n","Epoch 63, Step 57, Loss: 0.002049216302111745\n","Epoch 63, Step 58, Loss: 0.002020009560510516\n","Epoch 63, Step 59, Loss: 0.0020330664701759815\n","Epoch 63, Step 60, Loss: 0.0022042084019631147\n","Train Metric MRRs: 0.6939324304290767\n","Train Metric MAPs: 0.35643870553127543\n","Validation Metric MRRs: 0.6644184251133903\n","Validation Metric MAPs: 0.28583994725516926\n","Epoch 64, Step 1, Loss: 0.0020903649274259806\n","Epoch 64, Step 2, Loss: 0.0021163378842175007\n","Epoch 64, Step 3, Loss: 0.0020428034476935863\n","Epoch 64, Step 4, Loss: 0.0020857853814959526\n","Epoch 64, Step 5, Loss: 0.002177515998482704\n","Epoch 64, Step 6, Loss: 0.0021625207737088203\n","Epoch 64, Step 7, Loss: 0.0021405015140771866\n","Epoch 64, Step 8, Loss: 0.0020939898677170277\n","Epoch 64, Step 9, Loss: 0.0020159876439720392\n","Epoch 64, Step 10, Loss: 0.002091877395287156\n","Epoch 64, Step 11, Loss: 0.0020278969313949347\n","Epoch 64, Step 12, Loss: 0.0020674096886068583\n","Epoch 64, Step 13, Loss: 0.002080971375107765\n","Epoch 64, Step 14, Loss: 0.002059860387817025\n","Epoch 64, Step 15, Loss: 0.0020787965040653944\n","Epoch 64, Step 16, Loss: 0.0021127464715391397\n","Epoch 64, Step 17, Loss: 0.002140233526006341\n","Epoch 64, Step 18, Loss: 0.0025109287817031145\n","Epoch 64, Step 19, Loss: 0.002445537829771638\n","Epoch 64, Step 20, Loss: 0.0024185676593333483\n","Epoch 64, Step 21, Loss: 0.0024368828162550926\n","Epoch 64, Step 22, Loss: 0.0023192090447992086\n","Epoch 64, Step 23, Loss: 0.002303265267983079\n","Epoch 64, Step 24, Loss: 0.0022447749506682158\n","Epoch 64, Step 25, Loss: 0.002160069067031145\n","Epoch 64, Step 26, Loss: 0.002159779192879796\n","Epoch 64, Step 27, Loss: 0.002115944167599082\n","Epoch 64, Step 28, Loss: 0.0020335123408585787\n","Epoch 64, Step 29, Loss: 0.002127123763784766\n","Epoch 64, Step 30, Loss: 0.0021319109946489334\n","Epoch 64, Step 31, Loss: 0.002093351213261485\n","Epoch 64, Step 32, Loss: 0.0020341509953141212\n","Epoch 64, Step 33, Loss: 0.002030463656410575\n","Epoch 64, Step 34, Loss: 0.002060122787952423\n","Epoch 64, Step 35, Loss: 0.0019968790002167225\n","Epoch 64, Step 36, Loss: 0.0019942440558224916\n","Epoch 64, Step 37, Loss: 0.0020291153341531754\n","Epoch 64, Step 38, Loss: 0.002014564350247383\n","Epoch 64, Step 39, Loss: 0.001988057279959321\n","Epoch 64, Step 40, Loss: 0.0019757712725549936\n","Epoch 64, Step 41, Loss: 0.0019152110908180475\n","Epoch 64, Step 42, Loss: 0.001985993003472686\n","Epoch 64, Step 43, Loss: 0.002045206492766738\n","Epoch 64, Step 44, Loss: 0.0019814849365502596\n","Epoch 64, Step 45, Loss: 0.001941345282830298\n","Epoch 64, Step 46, Loss: 0.001975821563974023\n","Epoch 64, Step 47, Loss: 0.001981311710551381\n","Epoch 64, Step 48, Loss: 0.0019213443156331778\n","Epoch 64, Step 49, Loss: 0.0019353090319782495\n","Epoch 64, Step 50, Loss: 0.00199431786313653\n","Epoch 64, Step 51, Loss: 0.0019231521291658282\n","Epoch 64, Step 52, Loss: 0.0020232752431184053\n","Epoch 64, Step 53, Loss: 0.0019468312384560704\n","Epoch 64, Step 54, Loss: 0.0019032950513064861\n","Epoch 64, Step 55, Loss: 0.0019416920840740204\n","Epoch 64, Step 56, Loss: 0.001975594088435173\n","Epoch 64, Step 57, Loss: 0.0020198840647935867\n","Epoch 64, Step 58, Loss: 0.0020012077875435352\n","Epoch 64, Step 59, Loss: 0.0020052872132509947\n","Epoch 64, Step 60, Loss: 0.002141351345926523\n","Train Metric MRRs: 0.6950760639564146\n","Train Metric MAPs: 0.3550337209784179\n","Validation Metric MRRs: 0.6673793316871766\n","Validation Metric MAPs: 0.2835451824422478\n","Epoch 65, Step 1, Loss: 0.002057393081486225\n","Epoch 65, Step 2, Loss: 0.0021334325429052114\n","Epoch 65, Step 3, Loss: 0.0020506628789007664\n","Epoch 65, Step 4, Loss: 0.0020600096322596073\n","Epoch 65, Step 5, Loss: 0.002119783777743578\n","Epoch 65, Step 6, Loss: 0.0021007258910685778\n","Epoch 65, Step 7, Loss: 0.0021233605220913887\n","Epoch 65, Step 8, Loss: 0.0020898489747196436\n","Epoch 65, Step 9, Loss: 0.0020107736345380545\n","Epoch 65, Step 10, Loss: 0.0020804593805223703\n","Epoch 65, Step 11, Loss: 0.0020151620265096426\n","Epoch 65, Step 12, Loss: 0.0020213329698890448\n","Epoch 65, Step 13, Loss: 0.0020534370560199022\n","Epoch 65, Step 14, Loss: 0.0020713971462100744\n","Epoch 65, Step 15, Loss: 0.002105587860569358\n","Epoch 65, Step 16, Loss: 0.0021301056258380413\n","Epoch 65, Step 17, Loss: 0.0021653196308761835\n","Epoch 65, Step 18, Loss: 0.002502930350601673\n","Epoch 65, Step 19, Loss: 0.0024446838069707155\n","Epoch 65, Step 20, Loss: 0.0024747559800744057\n","Epoch 65, Step 21, Loss: 0.0024630685802549124\n","Epoch 65, Step 22, Loss: 0.002308748196810484\n","Epoch 65, Step 23, Loss: 0.002256021834909916\n","Epoch 65, Step 24, Loss: 0.0022049034014344215\n","Epoch 65, Step 25, Loss: 0.0021678581833839417\n","Epoch 65, Step 26, Loss: 0.002196172485128045\n","Epoch 65, Step 27, Loss: 0.002115398645401001\n","Epoch 65, Step 28, Loss: 0.0020348476245999336\n","Epoch 65, Step 29, Loss: 0.0020990094635635614\n","Epoch 65, Step 30, Loss: 0.0021103029139339924\n","Epoch 65, Step 31, Loss: 0.0021107292268425226\n","Epoch 65, Step 32, Loss: 0.0021052286028862\n","Epoch 65, Step 33, Loss: 0.0020436018239706755\n","Epoch 65, Step 34, Loss: 0.002071562223136425\n","Epoch 65, Step 35, Loss: 0.0020315973088145256\n","Epoch 65, Step 36, Loss: 0.0020279609598219395\n","Epoch 65, Step 37, Loss: 0.0020493390038609505\n","Epoch 65, Step 38, Loss: 0.0020711245015263557\n","Epoch 65, Step 39, Loss: 0.0020687677897512913\n","Epoch 65, Step 40, Loss: 0.0020130877383053303\n","Epoch 65, Step 41, Loss: 0.001960993045940995\n","Epoch 65, Step 42, Loss: 0.002050559502094984\n","Epoch 65, Step 43, Loss: 0.0020980234257876873\n","Epoch 65, Step 44, Loss: 0.0020022287499159575\n","Epoch 65, Step 45, Loss: 0.001962758367881179\n","Epoch 65, Step 46, Loss: 0.0019975395407527685\n","Epoch 65, Step 47, Loss: 0.0020344271324574947\n","Epoch 65, Step 48, Loss: 0.0019555464386940002\n","Epoch 65, Step 49, Loss: 0.0019619350787252188\n","Epoch 65, Step 50, Loss: 0.002046521520242095\n","Epoch 65, Step 51, Loss: 0.0019541652873158455\n","Epoch 65, Step 52, Loss: 0.0020316080190241337\n","Epoch 65, Step 53, Loss: 0.0019505416275933385\n","Epoch 65, Step 54, Loss: 0.0019324321765452623\n","Epoch 65, Step 55, Loss: 0.0019370713271200657\n","Epoch 65, Step 56, Loss: 0.0019753328524529934\n","Epoch 65, Step 57, Loss: 0.0020317360758781433\n","Epoch 65, Step 58, Loss: 0.0019885264337062836\n","Epoch 65, Step 59, Loss: 0.0019976356998085976\n","Epoch 65, Step 60, Loss: 0.0021533819381147623\n","Train Metric MRRs: 0.6951691645156454\n","Train Metric MAPs: 0.3515401286699169\n","Validation Metric MRRs: 0.6648561632264471\n","Validation Metric MAPs: 0.28255206104516395\n","Epoch 66, Step 1, Loss: 0.00206211325712502\n","Epoch 66, Step 2, Loss: 0.0021088621579110622\n","Epoch 66, Step 3, Loss: 0.0020737710874527693\n","Epoch 66, Step 4, Loss: 0.002059845719486475\n","Epoch 66, Step 5, Loss: 0.0021219784393906593\n","Epoch 66, Step 6, Loss: 0.0020518596284091473\n","Epoch 66, Step 7, Loss: 0.002039535902440548\n","Epoch 66, Step 8, Loss: 0.0020317514427006245\n","Epoch 66, Step 9, Loss: 0.0020233767572790384\n","Epoch 66, Step 10, Loss: 0.0021229658741503954\n","Epoch 66, Step 11, Loss: 0.0020321691408753395\n","Epoch 66, Step 12, Loss: 0.0020153054501861334\n","Epoch 66, Step 13, Loss: 0.0020386590622365475\n","Epoch 66, Step 14, Loss: 0.0020356103777885437\n","Epoch 66, Step 15, Loss: 0.002131267450749874\n","Epoch 66, Step 16, Loss: 0.002165778772905469\n","Epoch 66, Step 17, Loss: 0.0022148401476442814\n","Epoch 66, Step 18, Loss: 0.00250567146576941\n","Epoch 66, Step 19, Loss: 0.0024673331063240767\n","Epoch 66, Step 20, Loss: 0.002435623900964856\n","Epoch 66, Step 21, Loss: 0.0025733637157827616\n","Epoch 66, Step 22, Loss: 0.0024449070915579796\n","Epoch 66, Step 23, Loss: 0.002312775468453765\n","Epoch 66, Step 24, Loss: 0.002205850323662162\n","Epoch 66, Step 25, Loss: 0.002171229338273406\n","Epoch 66, Step 26, Loss: 0.002284347079694271\n","Epoch 66, Step 27, Loss: 0.002204945543780923\n","Epoch 66, Step 28, Loss: 0.002077633747830987\n","Epoch 66, Step 29, Loss: 0.0020644855685532093\n","Epoch 66, Step 30, Loss: 0.0020849246066063643\n","Epoch 66, Step 31, Loss: 0.0021337794605642557\n","Epoch 66, Step 32, Loss: 0.0020874813199043274\n","Epoch 66, Step 33, Loss: 0.0021006327588111162\n","Epoch 66, Step 34, Loss: 0.0020823762752115726\n","Epoch 66, Step 35, Loss: 0.0020664213225245476\n","Epoch 66, Step 36, Loss: 0.0020787185057997704\n","Epoch 66, Step 37, Loss: 0.001989342737942934\n","Epoch 66, Step 38, Loss: 0.0019924177322536707\n","Epoch 66, Step 39, Loss: 0.001999414060264826\n","Epoch 66, Step 40, Loss: 0.001988684292882681\n","Epoch 66, Step 41, Loss: 0.001974988728761673\n","Epoch 66, Step 42, Loss: 0.002022989559918642\n","Epoch 66, Step 43, Loss: 0.0020642359741032124\n","Epoch 66, Step 44, Loss: 0.0020231925882399082\n","Epoch 66, Step 45, Loss: 0.002024527173489332\n","Epoch 66, Step 46, Loss: 0.002016171347349882\n","Epoch 66, Step 47, Loss: 0.0019866591319441795\n","Epoch 66, Step 48, Loss: 0.001994113205000758\n","Epoch 66, Step 49, Loss: 0.00196378817781806\n","Epoch 66, Step 50, Loss: 0.002012582030147314\n","Epoch 66, Step 51, Loss: 0.0019337791018188\n","Epoch 66, Step 52, Loss: 0.0020413461606949568\n","Epoch 66, Step 53, Loss: 0.0019557862542569637\n","Epoch 66, Step 54, Loss: 0.0019100948702543974\n","Epoch 66, Step 55, Loss: 0.0019176002824679017\n","Epoch 66, Step 56, Loss: 0.0019546577241271734\n","Epoch 66, Step 57, Loss: 0.0020026187412440777\n","Epoch 66, Step 58, Loss: 0.0020024285186082125\n","Epoch 66, Step 59, Loss: 0.001991701079532504\n","Epoch 66, Step 60, Loss: 0.0021368868183344603\n","Train Metric MRRs: 0.6956147955580894\n","Train Metric MAPs: 0.34845575769255266\n","Validation Metric MRRs: 0.6639281369631911\n","Validation Metric MAPs: 0.2883885763660616\n","Epoch 67, Step 1, Loss: 0.002047996735200286\n","Epoch 67, Step 2, Loss: 0.002102541271597147\n","Epoch 67, Step 3, Loss: 0.0020291886758059263\n","Epoch 67, Step 4, Loss: 0.0020397144835442305\n","Epoch 67, Step 5, Loss: 0.0021047291811555624\n","Epoch 67, Step 6, Loss: 0.002063976600766182\n","Epoch 67, Step 7, Loss: 0.0021363096311688423\n","Epoch 67, Step 8, Loss: 0.0019913155119866133\n","Epoch 67, Step 9, Loss: 0.0020137031096965075\n","Epoch 67, Step 10, Loss: 0.002098997123539448\n","Epoch 67, Step 11, Loss: 0.0020657775457948446\n","Epoch 67, Step 12, Loss: 0.002105951076373458\n","Epoch 67, Step 13, Loss: 0.002230302896350622\n","Epoch 67, Step 14, Loss: 0.0020269404631108046\n","Epoch 67, Step 15, Loss: 0.002088043140247464\n","Epoch 67, Step 16, Loss: 0.0022162918467074633\n","Epoch 67, Step 17, Loss: 0.002262476133182645\n","Epoch 67, Step 18, Loss: 0.002718615811318159\n","Epoch 67, Step 19, Loss: 0.0025896187871694565\n","Epoch 67, Step 20, Loss: 0.0025049676187336445\n","Epoch 67, Step 21, Loss: 0.0025965587701648474\n","Epoch 67, Step 22, Loss: 0.002648402936756611\n","Epoch 67, Step 23, Loss: 0.0026008603163063526\n","Epoch 67, Step 24, Loss: 0.0024111568927764893\n","Epoch 67, Step 25, Loss: 0.0023017742205411196\n","Epoch 67, Step 26, Loss: 0.0024565677158534527\n","Epoch 67, Step 27, Loss: 0.0023786285892128944\n","Epoch 67, Step 28, Loss: 0.0023570943158119917\n","Epoch 67, Step 29, Loss: 0.002368181012570858\n","Epoch 67, Step 30, Loss: 0.0021535689011216164\n","Epoch 67, Step 31, Loss: 0.002221354516223073\n","Epoch 67, Step 32, Loss: 0.0022498243488371372\n","Epoch 67, Step 33, Loss: 0.002413518261164427\n","Epoch 67, Step 34, Loss: 0.00226631760597229\n","Epoch 67, Step 35, Loss: 0.002132571302354336\n","Epoch 67, Step 36, Loss: 0.00213821092620492\n","Epoch 67, Step 37, Loss: 0.0022433712147176266\n","Epoch 67, Step 38, Loss: 0.002289844211190939\n","Epoch 67, Step 39, Loss: 0.0021485534962266684\n","Epoch 67, Step 40, Loss: 0.0021579714957624674\n","Epoch 67, Step 41, Loss: 0.0020927961450070143\n","Epoch 67, Step 42, Loss: 0.0022028747480362654\n","Epoch 67, Step 43, Loss: 0.002278253436088562\n","Epoch 67, Step 44, Loss: 0.0021103282924741507\n","Epoch 67, Step 45, Loss: 0.0021264534443616867\n","Epoch 67, Step 46, Loss: 0.00215632445178926\n","Epoch 67, Step 47, Loss: 0.0021352185867726803\n","Epoch 67, Step 48, Loss: 0.002134784124791622\n","Epoch 67, Step 49, Loss: 0.0022118734195828438\n","Epoch 67, Step 50, Loss: 0.0021650807466357946\n","Epoch 67, Step 51, Loss: 0.0021018071565777063\n","Epoch 67, Step 52, Loss: 0.0021936551202088594\n","Epoch 67, Step 53, Loss: 0.0021219805348664522\n","Epoch 67, Step 54, Loss: 0.0022111141588538885\n","Epoch 67, Step 55, Loss: 0.002187728649005294\n","Epoch 67, Step 56, Loss: 0.0021917300764471292\n","Epoch 67, Step 57, Loss: 0.0022312034852802753\n","Epoch 67, Step 58, Loss: 0.0022243226412683725\n","Epoch 67, Step 59, Loss: 0.002147829392924905\n","Epoch 67, Step 60, Loss: 0.002272565383464098\n","Train Metric MRRs: 0.6869983838130439\n","Train Metric MAPs: 0.3257436121466052\n","Validation Metric MRRs: 0.6443159754801382\n","Validation Metric MAPs: 0.2635072985758298\n","Epoch 68, Step 1, Loss: 0.0021819686517119408\n","Epoch 68, Step 2, Loss: 0.0023427170235663652\n","Epoch 68, Step 3, Loss: 0.002267115516588092\n","Epoch 68, Step 4, Loss: 0.002216956578195095\n","Epoch 68, Step 5, Loss: 0.002242123708128929\n","Epoch 68, Step 6, Loss: 0.0022184208501130342\n","Epoch 68, Step 7, Loss: 0.0022066354285925627\n","Epoch 68, Step 8, Loss: 0.0021173087880015373\n","Epoch 68, Step 9, Loss: 0.0021571828983724117\n","Epoch 68, Step 10, Loss: 0.0021566920913755894\n","Epoch 68, Step 11, Loss: 0.002130198758095503\n","Epoch 68, Step 12, Loss: 0.002132499124854803\n","Epoch 68, Step 13, Loss: 0.0024615165311843157\n","Epoch 68, Step 14, Loss: 0.0020133559592068195\n","Epoch 68, Step 15, Loss: 0.002057332545518875\n","Epoch 68, Step 16, Loss: 0.0022176015190780163\n","Epoch 68, Step 17, Loss: 0.002262467984110117\n","Epoch 68, Step 18, Loss: 0.0027319209184497595\n","Epoch 68, Step 19, Loss: 0.0025038605090230703\n","Epoch 68, Step 20, Loss: 0.0024665638338774443\n","Epoch 68, Step 21, Loss: 0.0025293882936239243\n","Epoch 68, Step 22, Loss: 0.0024453294463455677\n","Epoch 68, Step 23, Loss: 0.002448830520734191\n","Epoch 68, Step 24, Loss: 0.002312804339453578\n","Epoch 68, Step 25, Loss: 0.0021942071616649628\n","Epoch 68, Step 26, Loss: 0.002221182221546769\n","Epoch 68, Step 27, Loss: 0.002255141269415617\n","Epoch 68, Step 28, Loss: 0.0022581734228879213\n","Epoch 68, Step 29, Loss: 0.0023477612994611263\n","Epoch 68, Step 30, Loss: 0.002282871399074793\n","Epoch 68, Step 31, Loss: 0.0021483188029378653\n","Epoch 68, Step 32, Loss: 0.002168458653613925\n","Epoch 68, Step 33, Loss: 0.0021529709920287132\n","Epoch 68, Step 34, Loss: 0.0023343467619270086\n","Epoch 68, Step 35, Loss: 0.0022339727729558945\n","Epoch 68, Step 36, Loss: 0.002136310562491417\n","Epoch 68, Step 37, Loss: 0.0021280653309077024\n","Epoch 68, Step 38, Loss: 0.0021154636051505804\n","Epoch 68, Step 39, Loss: 0.0021392765920609236\n","Epoch 68, Step 40, Loss: 0.0020793252624571323\n","Epoch 68, Step 41, Loss: 0.0020660627633333206\n","Epoch 68, Step 42, Loss: 0.002095151226967573\n","Epoch 68, Step 43, Loss: 0.002190954517573118\n","Epoch 68, Step 44, Loss: 0.002150550950318575\n","Epoch 68, Step 45, Loss: 0.002197320805862546\n","Epoch 68, Step 46, Loss: 0.0020164332818239927\n","Epoch 68, Step 47, Loss: 0.0020546463783830404\n","Epoch 68, Step 48, Loss: 0.0020373293664306402\n","Epoch 68, Step 49, Loss: 0.0021185497753322124\n","Epoch 68, Step 50, Loss: 0.0021068525966256857\n","Epoch 68, Step 51, Loss: 0.002067340537905693\n","Epoch 68, Step 52, Loss: 0.0020403671078383923\n","Epoch 68, Step 53, Loss: 0.002010862110182643\n","Epoch 68, Step 54, Loss: 0.002025226829573512\n","Epoch 68, Step 55, Loss: 0.0020318771712481976\n","Epoch 68, Step 56, Loss: 0.002117140917107463\n","Epoch 68, Step 57, Loss: 0.0021663156803697348\n","Epoch 68, Step 58, Loss: 0.002146176528185606\n","Epoch 68, Step 59, Loss: 0.0021486212499439716\n","Epoch 68, Step 60, Loss: 0.0021890858188271523\n","Train Metric MRRs: 0.690402656681719\n","Train Metric MAPs: 0.3248006324140304\n","Validation Metric MRRs: 0.6560935791460683\n","Validation Metric MAPs: 0.26438289960120903\n","Epoch 69, Step 1, Loss: 0.002131827874109149\n","Epoch 69, Step 2, Loss: 0.0022379010915756226\n","Epoch 69, Step 3, Loss: 0.0021334721241146326\n","Epoch 69, Step 4, Loss: 0.0021366667933762074\n","Epoch 69, Step 5, Loss: 0.002174053806811571\n","Epoch 69, Step 6, Loss: 0.0021159632597118616\n","Epoch 69, Step 7, Loss: 0.0020997438114136457\n","Epoch 69, Step 8, Loss: 0.002080820733681321\n","Epoch 69, Step 9, Loss: 0.0020998341497033834\n","Epoch 69, Step 10, Loss: 0.002160717034712434\n","Epoch 69, Step 11, Loss: 0.0020954411011189222\n","Epoch 69, Step 12, Loss: 0.002082572551444173\n","Epoch 69, Step 13, Loss: 0.0021441204007714987\n","Epoch 69, Step 14, Loss: 0.0020096427761018276\n","Epoch 69, Step 15, Loss: 0.00201382115483284\n","Epoch 69, Step 16, Loss: 0.002124519320204854\n","Epoch 69, Step 17, Loss: 0.0022539610508829355\n","Epoch 69, Step 18, Loss: 0.0026676871348172426\n","Epoch 69, Step 19, Loss: 0.002533016260713339\n","Epoch 69, Step 20, Loss: 0.0024143923074007034\n","Epoch 69, Step 21, Loss: 0.0024086262565106153\n","Epoch 69, Step 22, Loss: 0.002429758198559284\n","Epoch 69, Step 23, Loss: 0.0024583344347774982\n","Epoch 69, Step 24, Loss: 0.0023138143587857485\n","Epoch 69, Step 25, Loss: 0.0021537612192332745\n","Epoch 69, Step 26, Loss: 0.002157903276383877\n","Epoch 69, Step 27, Loss: 0.00214348454028368\n","Epoch 69, Step 28, Loss: 0.002146705985069275\n","Epoch 69, Step 29, Loss: 0.002233104780316353\n","Epoch 69, Step 30, Loss: 0.0022120694629848003\n","Epoch 69, Step 31, Loss: 0.0021776589564979076\n","Epoch 69, Step 32, Loss: 0.0020747606176882982\n","Epoch 69, Step 33, Loss: 0.002095934469252825\n","Epoch 69, Step 34, Loss: 0.002152477391064167\n","Epoch 69, Step 35, Loss: 0.002173039596527815\n","Epoch 69, Step 36, Loss: 0.0021190561819821596\n","Epoch 69, Step 37, Loss: 0.002078534569591284\n","Epoch 69, Step 38, Loss: 0.0020631705410778522\n","Epoch 69, Step 39, Loss: 0.0020457832142710686\n","Epoch 69, Step 40, Loss: 0.0021598157472908497\n","Epoch 69, Step 41, Loss: 0.0020986697636544704\n","Epoch 69, Step 42, Loss: 0.0021124526392668486\n","Epoch 69, Step 43, Loss: 0.002063619438558817\n","Epoch 69, Step 44, Loss: 0.00203257380053401\n","Epoch 69, Step 45, Loss: 0.0021412742789834738\n","Epoch 69, Step 46, Loss: 0.0021134342532604933\n","Epoch 69, Step 47, Loss: 0.002029517199844122\n","Epoch 69, Step 48, Loss: 0.0019332565134391189\n","Epoch 69, Step 49, Loss: 0.002003184985369444\n","Epoch 69, Step 50, Loss: 0.0021365077700465918\n","Epoch 69, Step 51, Loss: 0.0021717872004956007\n","Epoch 69, Step 52, Loss: 0.0021142675541341305\n","Epoch 69, Step 53, Loss: 0.001962822861969471\n","Epoch 69, Step 54, Loss: 0.0019435940776020288\n","Epoch 69, Step 55, Loss: 0.0020802784711122513\n","Epoch 69, Step 56, Loss: 0.002127391519024968\n","Epoch 69, Step 57, Loss: 0.002198637230321765\n","Epoch 69, Step 58, Loss: 0.002067698398604989\n","Epoch 69, Step 59, Loss: 0.0021399925462901592\n","Epoch 69, Step 60, Loss: 0.002243167720735073\n","Train Metric MRRs: 0.6934178941175841\n","Train Metric MAPs: 0.3398724398472608\n","Validation Metric MRRs: 0.6627569582112323\n","Validation Metric MAPs: 0.28805976506507236\n","Epoch 70, Step 1, Loss: 0.0021756018977612257\n","Epoch 70, Step 2, Loss: 0.002180875511839986\n","Epoch 70, Step 3, Loss: 0.002060272265225649\n","Epoch 70, Step 4, Loss: 0.0020867346320301294\n","Epoch 70, Step 5, Loss: 0.0021964588668197393\n","Epoch 70, Step 6, Loss: 0.002208260353654623\n","Epoch 70, Step 7, Loss: 0.002175157656893134\n","Epoch 70, Step 8, Loss: 0.0020535141229629517\n","Epoch 70, Step 9, Loss: 0.002041708678007126\n","Epoch 70, Step 10, Loss: 0.0021035175304859877\n","Epoch 70, Step 11, Loss: 0.0020806947723031044\n","Epoch 70, Step 12, Loss: 0.0021047242917120457\n","Epoch 70, Step 13, Loss: 0.002026479924097657\n","Epoch 70, Step 14, Loss: 0.0019416093127802014\n","Epoch 70, Step 15, Loss: 0.0019830656237900257\n","Epoch 70, Step 16, Loss: 0.0021123848855495453\n","Epoch 70, Step 17, Loss: 0.0021085336338728666\n","Epoch 70, Step 18, Loss: 0.0025694319047033787\n","Epoch 70, Step 19, Loss: 0.002591579919680953\n","Epoch 70, Step 20, Loss: 0.0024699007626622915\n","Epoch 70, Step 21, Loss: 0.002368638524785638\n","Epoch 70, Step 22, Loss: 0.002170925261452794\n","Epoch 70, Step 23, Loss: 0.002303800079971552\n","Epoch 70, Step 24, Loss: 0.002448499435558915\n","Epoch 70, Step 25, Loss: 0.002381281228736043\n","Epoch 70, Step 26, Loss: 0.002257890533655882\n","Epoch 70, Step 27, Loss: 0.0020803590305149555\n","Epoch 70, Step 28, Loss: 0.002078891498968005\n","Epoch 70, Step 29, Loss: 0.0023367279209196568\n","Epoch 70, Step 30, Loss: 0.002451176755130291\n","Epoch 70, Step 31, Loss: 0.0023532609920948744\n","Epoch 70, Step 32, Loss: 0.0020639593712985516\n","Epoch 70, Step 33, Loss: 0.00200841692276299\n","Epoch 70, Step 34, Loss: 0.0020970834884792566\n","Epoch 70, Step 35, Loss: 0.002226780168712139\n","Epoch 70, Step 36, Loss: 0.0022643457632511854\n","Epoch 70, Step 37, Loss: 0.0022412447724491358\n","Epoch 70, Step 38, Loss: 0.0020608187187463045\n","Epoch 70, Step 39, Loss: 0.001985144102945924\n","Epoch 70, Step 40, Loss: 0.0020373219158500433\n","Epoch 70, Step 41, Loss: 0.00217142840847373\n","Epoch 70, Step 42, Loss: 0.0023287234362214804\n","Epoch 70, Step 43, Loss: 0.002252408768981695\n","Epoch 70, Step 44, Loss: 0.0020478605292737484\n","Epoch 70, Step 45, Loss: 0.002125368919223547\n","Epoch 70, Step 46, Loss: 0.0022532662842422724\n","Epoch 70, Step 47, Loss: 0.0023137140087783337\n","Epoch 70, Step 48, Loss: 0.0022115411702543497\n","Epoch 70, Step 49, Loss: 0.002054407959803939\n","Epoch 70, Step 50, Loss: 0.002076690550893545\n","Epoch 70, Step 51, Loss: 0.0021833505015820265\n","Epoch 70, Step 52, Loss: 0.002347835572436452\n","Epoch 70, Step 53, Loss: 0.0022800387814641\n","Epoch 70, Step 54, Loss: 0.002152487402781844\n","Epoch 70, Step 55, Loss: 0.0019256158266216516\n","Epoch 70, Step 56, Loss: 0.0020820973441004753\n","Epoch 70, Step 57, Loss: 0.0024279027711600065\n","Epoch 70, Step 58, Loss: 0.0024563635233789682\n","Epoch 70, Step 59, Loss: 0.0022618586663156748\n","Epoch 70, Step 60, Loss: 0.002204151591286063\n","Train Metric MRRs: 0.6952586985981544\n","Train Metric MAPs: 0.3538555816868995\n","Validation Metric MRRs: 0.6598941015823374\n","Validation Metric MAPs: 0.28169325864538214\n","Epoch 71, Step 1, Loss: 0.002106437226757407\n","Epoch 71, Step 2, Loss: 0.002323146676644683\n","Epoch 71, Step 3, Loss: 0.0024361663963645697\n","Epoch 71, Step 4, Loss: 0.002442345954477787\n","Epoch 71, Step 5, Loss: 0.002297948580235243\n","Epoch 71, Step 6, Loss: 0.0021281323861330748\n","Epoch 71, Step 7, Loss: 0.002288894262164831\n","Epoch 71, Step 8, Loss: 0.002496070694178343\n","Epoch 71, Step 9, Loss: 0.0024636259768158197\n","Epoch 71, Step 10, Loss: 0.002293010475113988\n","Epoch 71, Step 11, Loss: 0.002163571771234274\n","Epoch 71, Step 12, Loss: 0.0022104140371084213\n","Epoch 71, Step 13, Loss: 0.0022919951006770134\n","Epoch 71, Step 14, Loss: 0.0022213307674974203\n","Epoch 71, Step 15, Loss: 0.002092641545459628\n","Epoch 71, Step 16, Loss: 0.002117655472829938\n","Epoch 71, Step 17, Loss: 0.0021701499354094267\n","Epoch 71, Step 18, Loss: 0.0026576691307127476\n","Epoch 71, Step 19, Loss: 0.0024495364632457495\n","Epoch 71, Step 20, Loss: 0.002385887783020735\n","Epoch 71, Step 21, Loss: 0.002365359105169773\n","Epoch 71, Step 22, Loss: 0.0022667478770017624\n","Epoch 71, Step 23, Loss: 0.002257794141769409\n","Epoch 71, Step 24, Loss: 0.0022706270683556795\n","Epoch 71, Step 25, Loss: 0.002149390522390604\n","Epoch 71, Step 26, Loss: 0.0021995266433805227\n","Epoch 71, Step 27, Loss: 0.002214679727330804\n","Epoch 71, Step 28, Loss: 0.0021356039214879274\n","Epoch 71, Step 29, Loss: 0.0020641922019422054\n","Epoch 71, Step 30, Loss: 0.002096676966175437\n","Epoch 71, Step 31, Loss: 0.002238156972452998\n","Epoch 71, Step 32, Loss: 0.0024096653796732426\n","Epoch 71, Step 33, Loss: 0.0023478283546864986\n","Epoch 71, Step 34, Loss: 0.002170770661905408\n","Epoch 71, Step 35, Loss: 0.0019809743389487267\n","Epoch 71, Step 36, Loss: 0.0020947225857526064\n","Epoch 71, Step 37, Loss: 0.0024665219243615866\n","Epoch 71, Step 38, Loss: 0.0025127180851995945\n","Epoch 71, Step 39, Loss: 0.0023854223545640707\n","Epoch 71, Step 40, Loss: 0.002091110683977604\n","Epoch 71, Step 41, Loss: 0.0019960328936576843\n","Epoch 71, Step 42, Loss: 0.0023221485316753387\n","Epoch 71, Step 43, Loss: 0.0026418159250169992\n","Epoch 71, Step 44, Loss: 0.0025021573528647423\n","Epoch 71, Step 45, Loss: 0.0022577261552214622\n","Epoch 71, Step 46, Loss: 0.0020582617726176977\n","Epoch 71, Step 47, Loss: 0.002086329273879528\n","Epoch 71, Step 48, Loss: 0.0023493897169828415\n","Epoch 71, Step 49, Loss: 0.0024992150720208883\n","Epoch 71, Step 50, Loss: 0.0025820306036621332\n","Epoch 71, Step 51, Loss: 0.002270314609631896\n","Epoch 71, Step 52, Loss: 0.00217276974581182\n","Epoch 71, Step 53, Loss: 0.002336546778678894\n","Epoch 71, Step 54, Loss: 0.0024306525010615587\n","Epoch 71, Step 55, Loss: 0.0024564829654991627\n","Epoch 71, Step 56, Loss: 0.002355067292228341\n","Epoch 71, Step 57, Loss: 0.0022011587861925364\n","Epoch 71, Step 58, Loss: 0.0021155658178031445\n","Epoch 71, Step 59, Loss: 0.0023065023124217987\n","Epoch 71, Step 60, Loss: 0.0025472925044596195\n","Train Metric MRRs: 0.6905988730830396\n","Train Metric MAPs: 0.35965731580288934\n","Validation Metric MRRs: 0.6340845949877592\n","Validation Metric MAPs: 0.22955532450206695\n","Epoch 72, Step 1, Loss: 0.002678058808669448\n","Epoch 72, Step 2, Loss: 0.002424671081826091\n","Epoch 72, Step 3, Loss: 0.002127599436789751\n","Epoch 72, Step 4, Loss: 0.0022812657989561558\n","Epoch 72, Step 5, Loss: 0.002479215618222952\n","Epoch 72, Step 6, Loss: 0.002628543647006154\n","Epoch 72, Step 7, Loss: 0.0024344820994883776\n","Epoch 72, Step 8, Loss: 0.0023103179410099983\n","Epoch 72, Step 9, Loss: 0.00213331775739789\n","Epoch 72, Step 10, Loss: 0.0023243052419275045\n","Epoch 72, Step 11, Loss: 0.002445211401209235\n","Epoch 72, Step 12, Loss: 0.002621891675516963\n","Epoch 72, Step 13, Loss: 0.00235381000675261\n","Epoch 72, Step 14, Loss: 0.002039728220552206\n","Epoch 72, Step 15, Loss: 0.0021941366139799356\n","Epoch 72, Step 16, Loss: 0.002463599666953087\n","Epoch 72, Step 17, Loss: 0.002531566424295306\n","Epoch 72, Step 18, Loss: 0.0029162641149014235\n","Epoch 72, Step 19, Loss: 0.0026088289450854063\n","Epoch 72, Step 20, Loss: 0.002345295622944832\n","Epoch 72, Step 21, Loss: 0.0025635852944105864\n","Epoch 72, Step 22, Loss: 0.0026553559582680464\n","Epoch 72, Step 23, Loss: 0.0027516677509993315\n","Epoch 72, Step 24, Loss: 0.002566913841292262\n","Epoch 72, Step 25, Loss: 0.0022885985672473907\n","Epoch 72, Step 26, Loss: 0.0022252416238188744\n","Epoch 72, Step 27, Loss: 0.0022163670510053635\n","Epoch 72, Step 28, Loss: 0.0022867603693157434\n","Epoch 72, Step 29, Loss: 0.002227542456239462\n","Epoch 72, Step 30, Loss: 0.0021107876673340797\n","Epoch 72, Step 31, Loss: 0.0021205854136496782\n","Epoch 72, Step 32, Loss: 0.0021270830184221268\n","Epoch 72, Step 33, Loss: 0.0020861730445176363\n","Epoch 72, Step 34, Loss: 0.0020529800094664097\n","Epoch 72, Step 35, Loss: 0.0020808344706892967\n","Epoch 72, Step 36, Loss: 0.0021092777606099844\n","Epoch 72, Step 37, Loss: 0.002097678603604436\n","Epoch 72, Step 38, Loss: 0.002043014159426093\n","Epoch 72, Step 39, Loss: 0.002073934767395258\n","Epoch 72, Step 40, Loss: 0.002203403040766716\n","Epoch 72, Step 41, Loss: 0.002115199575200677\n","Epoch 72, Step 42, Loss: 0.0020184244494885206\n","Epoch 72, Step 43, Loss: 0.0020570699125528336\n","Epoch 72, Step 44, Loss: 0.002123500220477581\n","Epoch 72, Step 45, Loss: 0.00228842138312757\n","Epoch 72, Step 46, Loss: 0.0021783451084047556\n","Epoch 72, Step 47, Loss: 0.0020827760454267263\n","Epoch 72, Step 48, Loss: 0.0019650058820843697\n","Epoch 72, Step 49, Loss: 0.0020162155851721764\n","Epoch 72, Step 50, Loss: 0.002180734183639288\n","Epoch 72, Step 51, Loss: 0.0022601892706006765\n","Epoch 72, Step 52, Loss: 0.00241822749376297\n","Epoch 72, Step 53, Loss: 0.002121926983818412\n","Epoch 72, Step 54, Loss: 0.002057260600849986\n","Epoch 72, Step 55, Loss: 0.002084758598357439\n","Epoch 72, Step 56, Loss: 0.0022852940019220114\n","Epoch 72, Step 57, Loss: 0.0023871385492384434\n","Epoch 72, Step 58, Loss: 0.0023057153448462486\n","Epoch 72, Step 59, Loss: 0.0021012381184846163\n","Epoch 72, Step 60, Loss: 0.002180247101932764\n","Train Metric MRRs: 0.6879940175651599\n","Train Metric MAPs: 0.3766059688591721\n","Validation Metric MRRs: 0.6508449688089302\n","Validation Metric MAPs: 0.21073224934633\n","Epoch 73, Step 1, Loss: 0.0021418079268187284\n","Epoch 73, Step 2, Loss: 0.002285685623064637\n","Epoch 73, Step 3, Loss: 0.0022739616688340902\n","Epoch 73, Step 4, Loss: 0.0022360750008374453\n","Epoch 73, Step 5, Loss: 0.0022245971485972404\n","Epoch 73, Step 6, Loss: 0.0020917109213769436\n","Epoch 73, Step 7, Loss: 0.00215229787863791\n","Epoch 73, Step 8, Loss: 0.0023391370195895433\n","Epoch 73, Step 9, Loss: 0.0023421107325702906\n","Epoch 73, Step 10, Loss: 0.0023330373223870993\n","Epoch 73, Step 11, Loss: 0.0021362854167819023\n","Epoch 73, Step 12, Loss: 0.0020591060165315866\n","Epoch 73, Step 13, Loss: 0.0021828741300851107\n","Epoch 73, Step 14, Loss: 0.002335598459467292\n","Epoch 73, Step 15, Loss: 0.002135877963155508\n","Epoch 73, Step 16, Loss: 0.002073122188448906\n","Epoch 73, Step 17, Loss: 0.002145208651199937\n","Epoch 73, Step 18, Loss: 0.002675791038200259\n","Epoch 73, Step 19, Loss: 0.00278303399682045\n","Epoch 73, Step 20, Loss: 0.002723575569689274\n","Epoch 73, Step 21, Loss: 0.002505808835849166\n","Epoch 73, Step 22, Loss: 0.0022486334200948477\n","Epoch 73, Step 23, Loss: 0.002309120027348399\n","Epoch 73, Step 24, Loss: 0.002565256552770734\n","Epoch 73, Step 25, Loss: 0.0026128997560590506\n","Epoch 73, Step 26, Loss: 0.0025532105937600136\n","Epoch 73, Step 27, Loss: 0.002273536054417491\n","Epoch 73, Step 28, Loss: 0.0020233788527548313\n","Epoch 73, Step 29, Loss: 0.002219146117568016\n","Epoch 73, Step 30, Loss: 0.0023902838584035635\n","Epoch 73, Step 31, Loss: 0.00249081221409142\n","Epoch 73, Step 32, Loss: 0.0023230069782584906\n","Epoch 73, Step 33, Loss: 0.0021256692707538605\n","Epoch 73, Step 34, Loss: 0.002063326770439744\n","Epoch 73, Step 35, Loss: 0.00210200110450387\n","Epoch 73, Step 36, Loss: 0.0021748491562902927\n","Epoch 73, Step 37, Loss: 0.0022386936470866203\n","Epoch 73, Step 38, Loss: 0.0021095052361488342\n","Epoch 73, Step 39, Loss: 0.0020253676921129227\n","Epoch 73, Step 40, Loss: 0.0019524836679920554\n","Epoch 73, Step 41, Loss: 0.0019526943797245622\n","Epoch 73, Step 42, Loss: 0.002017300808802247\n","Epoch 73, Step 43, Loss: 0.0021555337589234114\n","Epoch 73, Step 44, Loss: 0.0020904734265059233\n","Epoch 73, Step 45, Loss: 0.0020433429162949324\n","Epoch 73, Step 46, Loss: 0.0020284028723835945\n","Epoch 73, Step 47, Loss: 0.0019753228407353163\n","Epoch 73, Step 48, Loss: 0.0019944726955145597\n","Epoch 73, Step 49, Loss: 0.0019580007065087557\n","Epoch 73, Step 50, Loss: 0.00207529473118484\n","Epoch 73, Step 51, Loss: 0.002026521833613515\n","Epoch 73, Step 52, Loss: 0.0020621269941329956\n","Epoch 73, Step 53, Loss: 0.0019377451390028\n","Epoch 73, Step 54, Loss: 0.0020010345615446568\n","Epoch 73, Step 55, Loss: 0.001975669991225004\n","Epoch 73, Step 56, Loss: 0.002059978898614645\n","Epoch 73, Step 57, Loss: 0.0021186633966863155\n","Epoch 73, Step 58, Loss: 0.002057652920484543\n","Epoch 73, Step 59, Loss: 0.0019879366736859083\n","Epoch 73, Step 60, Loss: 0.002131636720150709\n","Train Metric MRRs: 0.6939485059108486\n","Train Metric MAPs: 0.36608712612902117\n","Validation Metric MRRs: 0.6591250320554369\n","Validation Metric MAPs: 0.25903628910537413\n","Epoch 74, Step 1, Loss: 0.002107221633195877\n","Epoch 74, Step 2, Loss: 0.0020909165032207966\n","Epoch 74, Step 3, Loss: 0.002151738852262497\n","Epoch 74, Step 4, Loss: 0.002142835408449173\n","Epoch 74, Step 5, Loss: 0.0022076391614973545\n","Epoch 74, Step 6, Loss: 0.0021044507157057524\n","Epoch 74, Step 7, Loss: 0.002047295682132244\n","Epoch 74, Step 8, Loss: 0.002147930208593607\n","Epoch 74, Step 9, Loss: 0.0020748102106153965\n","Epoch 74, Step 10, Loss: 0.0021861172281205654\n","Epoch 74, Step 11, Loss: 0.0020823520608246326\n","Epoch 74, Step 12, Loss: 0.002022478962317109\n","Epoch 74, Step 13, Loss: 0.001975961495190859\n","Epoch 74, Step 14, Loss: 0.001940256915986538\n","Epoch 74, Step 15, Loss: 0.001953556202352047\n","Epoch 74, Step 16, Loss: 0.002120233839377761\n","Epoch 74, Step 17, Loss: 0.002132783178240061\n","Epoch 74, Step 18, Loss: 0.0024669328704476357\n","Epoch 74, Step 19, Loss: 0.0024315917398780584\n","Epoch 74, Step 20, Loss: 0.002342044375836849\n","Epoch 74, Step 21, Loss: 0.0024048779159784317\n","Epoch 74, Step 22, Loss: 0.002267035422846675\n","Epoch 74, Step 23, Loss: 0.0022212329786270857\n","Epoch 74, Step 24, Loss: 0.0021926364861428738\n","Epoch 74, Step 25, Loss: 0.0021697389893233776\n","Epoch 74, Step 26, Loss: 0.002245248993858695\n","Epoch 74, Step 27, Loss: 0.002244518604129553\n","Epoch 74, Step 28, Loss: 0.002089242683723569\n","Epoch 74, Step 29, Loss: 0.002016431652009487\n","Epoch 74, Step 30, Loss: 0.002082361839711666\n","Epoch 74, Step 31, Loss: 0.002184175420552492\n","Epoch 74, Step 32, Loss: 0.0022867198567837477\n","Epoch 74, Step 33, Loss: 0.00226845545694232\n","Epoch 74, Step 34, Loss: 0.002179217990487814\n","Epoch 74, Step 35, Loss: 0.0019740492571145296\n","Epoch 74, Step 36, Loss: 0.0019773824606090784\n","Epoch 74, Step 37, Loss: 0.002173229120671749\n","Epoch 74, Step 38, Loss: 0.002269079675897956\n","Epoch 74, Step 39, Loss: 0.0022577967029064894\n","Epoch 74, Step 40, Loss: 0.0020474432967603207\n","Epoch 74, Step 41, Loss: 0.0019404195481911302\n","Epoch 74, Step 42, Loss: 0.001988576492294669\n","Epoch 74, Step 43, Loss: 0.0021700719371438026\n","Epoch 74, Step 44, Loss: 0.002209010999649763\n","Epoch 74, Step 45, Loss: 0.0021961485035717487\n","Epoch 74, Step 46, Loss: 0.0020166817121207714\n","Epoch 74, Step 47, Loss: 0.001942771952599287\n","Epoch 74, Step 48, Loss: 0.0019405242055654526\n","Epoch 74, Step 49, Loss: 0.001995633589103818\n","Epoch 74, Step 50, Loss: 0.002246350049972534\n","Epoch 74, Step 51, Loss: 0.00214804126881063\n","Epoch 74, Step 52, Loss: 0.0021861984860152006\n","Epoch 74, Step 53, Loss: 0.0019418566953390837\n","Epoch 74, Step 54, Loss: 0.0019148836145177484\n","Epoch 74, Step 55, Loss: 0.0019818528089672327\n","Epoch 74, Step 56, Loss: 0.0021003216970711946\n","Epoch 74, Step 57, Loss: 0.002189876977354288\n","Epoch 74, Step 58, Loss: 0.002072014380246401\n","Epoch 74, Step 59, Loss: 0.0019706706516444683\n","Epoch 74, Step 60, Loss: 0.0021048944909125566\n","Train Metric MRRs: 0.6966492778305445\n","Train Metric MAPs: 0.3722918198958934\n","Validation Metric MRRs: 0.6620829562818665\n","Validation Metric MAPs: 0.26084576804306664\n","Early stopping triggered!\n"]}],"source":["trainer.resume_training(config['num_epochs'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"brUW-S9GWkh3","executionInfo":{"status":"ok","timestamp":1693390948876,"user_tz":-60,"elapsed":141983,"user":{"displayName":"Quandary zhang","userId":"14333747180371872055"}},"outputId":"0d0958ca-914f-43aa-d39f-e020a4871a14"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Metric MRRs: 0.6673801458103733\n","Validation Metric MAPs: 0.28354526721121115\n","Test Metric MRRs: 0.6601163697022185\n","Test Metric MAPs: 0.2465421311087633\n"]},{"output_type":"execute_result","data":{"text/plain":["(0.6601163697022185, 0.2465421311087633)"]},"metadata":{},"execution_count":6}],"source":["trainer.restore_best_checkpoint()\n","trainer.validate('Validation')\n","trainer.validate('Test')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1pBEj0pxNgq6XrBwUPufSzhud3nzdxhxq","timestamp":1691741843673}],"gpuType":"A100","authorship_tag":"ABX9TyMF5K2FDffnQmte9Vz+DK2E"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}